{
  "paragraphs": [
    {
      "title": "Collaborative Filtering:  Matrix Factorization using Alternating Least Squares (ALS)",
      "text": "%md ![Alternating Least Squares - Matrix Factorization](https://s3.amazonaws.com/fluxcapacitor.com/img/collaborative-filtering-with-als-matrix-factorization.png)",
      "dateUpdated": "Apr 21, 2016 8:40:48 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091989_1924127606",
      "id": "20160317-011131_2058308868",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"https://s3.amazonaws.com/fluxcapacitor.com/img/collaborative-filtering-with-als-matrix-factorization.png\" alt\u003d\"Alternating Least Squares - Matrix Factorization\" /\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:40:48 AM",
      "dateFinished": "Apr 21, 2016 8:40:49 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get Reference Data for Enrichment",
      "text": "val itemsDF \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"file:/root/pipeline/datasets/movielens/ml-latest/movies.csv\")\n  .toDF(\"itemId\", \"title\", \"tags\")\n  .as(\"items\")\n\nz.show(itemsDF.select($\"itemId\", $\"title\", $\"tags\"))",
      "dateUpdated": "Apr 21, 2016 8:40:48 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 175.0,
          "optionOpen": false,
          "keys": [],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091989_1924127606",
      "id": "20160317-011131_550251797",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n\tat com.databricks.spark.csv.CsvRelation.firstLine$lzycompute(CsvRelation.scala:267)\n\tat com.databricks.spark.csv.CsvRelation.firstLine(CsvRelation.scala:263)\n\tat com.databricks.spark.csv.CsvRelation.inferSchema(CsvRelation.scala:240)\n\tat com.databricks.spark.csv.CsvRelation.\u003cinit\u003e(CsvRelation.scala:73)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:162)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:44)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:158)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:30)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:37)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:39)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:41)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:43)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:45)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:47)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:49)\n\tat \u003cinit\u003e(\u003cconsole\u003e:51)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:55)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:40:48 AM",
      "dateFinished": "Apr 21, 2016 8:44:22 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get Ratings",
      "text": "val itemRatingsDF \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"file:/root/pipeline/datasets/movielens/ml-latest/ratings.csv\")\n  .toDF(\"userId\", \"itemId\", \"rating\", \"timestamp\")\n  .as(\"itemRatings\")\n  \nz.show(itemsDF.select($\"itemId\", $\"title\", $\"tags\"))",
      "dateUpdated": "Apr 21, 2016 8:40:48 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091990_1925281853",
      "id": "20160317-011131_922270486",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:82)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkContext(SparkInterpreter.java:339)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkContext(SparkInterpreter.java:145)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:465)\norg.apache.zeppelin.interpreter.ClassloaderInterpreter.open(ClassloaderInterpreter.java:74)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:68)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:92)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\norg.apache.zeppelin.scheduler.Job.run(Job.java:169)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:82)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkContext(SparkInterpreter.java:339)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkContext(SparkInterpreter.java:145)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:465)\norg.apache.zeppelin.interpreter.ClassloaderInterpreter.open(ClassloaderInterpreter.java:74)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:68)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:92)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\norg.apache.zeppelin.scheduler.Job.run(Job.java:169)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745)\n         \n\tat org.apache.spark.SparkContext.org$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:106)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2086)\n\tat org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.textFile$default$2(SparkContext.scala:830)\n\tat com.databricks.spark.csv.util.TextFile$.withCharset(TextFile.scala:30)\n\tat com.databricks.spark.csv.DefaultSource$$anonfun$createRelation$1.apply(DefaultSource.scala:146)\n\tat com.databricks.spark.csv.DefaultSource$$anonfun$createRelation$1.apply(DefaultSource.scala:146)\n\tat com.databricks.spark.csv.CsvRelation.firstLine$lzycompute(CsvRelation.scala:265)\n\tat com.databricks.spark.csv.CsvRelation.firstLine(CsvRelation.scala:263)\n\tat com.databricks.spark.csv.CsvRelation.inferSchema(CsvRelation.scala:240)\n\tat com.databricks.spark.csv.CsvRelation.\u003cinit\u003e(CsvRelation.scala:73)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:162)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:44)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:158)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:30)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:37)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:39)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:41)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:43)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:45)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:47)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:49)\n\tat \u003cinit\u003e(\u003cconsole\u003e:51)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:55)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:40:53 AM",
      "dateFinished": "Apr 21, 2016 8:44:22 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train The ALS Model",
      "text": "import org.apache.spark.ml.recommendation.ALS\n\nval rank \u003d 10 // this is k\nval maxIterations \u003d 5\nval convergenceThreshold \u003d 0.01\n\nval als \u003d new ALS()\n  .setRank(rank)\n  .setRegParam(convergenceThreshold)\n  .setUserCol(\"userId\")\n  .setItemCol(\"itemId\")\n  .setRatingCol(\"rating\")\n\nval model \u003d als.fit(itemRatingsDF)",
      "dateUpdated": "Apr 21, 2016 8:40:49 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091991_1924897104",
      "id": "20160317-011131_1402155952",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.recommendation.ALS\nrank: Int \u003d 10\nmaxIterations: Int \u003d 5\nconvergenceThreshold: Double \u003d 0.01\nals: org.apache.spark.ml.recommendation.ALS \u003d als_436657809b35\n\u003cconsole\u003e:35: error: not found: value itemRatingsDF\n       val model \u003d als.fit(itemRatingsDF)\n                           ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:44:22 AM",
      "dateFinished": "Apr 21, 2016 8:44:23 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show User Factors Matrix",
      "text": "import org.apache.spark.mllib.linalg.Vectors\n\nval userFactorsDF \u003d model.userFactors.map(row \u003d\u003e \n  (row.getInt(0),  Vectors.dense(row.getSeq[Float](1).toArray.map(_.toDouble)))\n).toDF(\"userId\", \"userFactors\")\n .sort($\"userId\" asc)\n .as(\"userFactors\")\n\nz.show(userFactorsDF)",
      "dateUpdated": "Apr 21, 2016 8:40:49 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091991_1924897104",
      "id": "20160317-011131_1398047659",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.linalg.Vectors\n\u003cconsole\u003e:30: error: not found: value model\n       val userFactorsDF \u003d model.userFactors.map(row \u003d\u003e \n                           ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:44:23 AM",
      "dateFinished": "Apr 21, 2016 8:44:23 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show ItemFactors Matrix",
      "text": "import org.apache.spark.mllib.linalg.Vectors\n\nval itemFactorsDF \u003d model.itemFactors.map(row \u003d\u003e\n  (row.getInt(0),  Vectors.dense(row.getSeq[Float](1).toArray.map(_.toDouble)))\n).toDF(\"itemId\", \"itemFactors\")\n .sort($\"itemId\" asc)\n .as(\"itemFactors\")\n\nz.show(itemFactorsDF)",
      "dateUpdated": "Apr 21, 2016 8:40:49 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 314.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "itemFactors",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "itemFactors",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091992_1922973360",
      "id": "20160317-011131_305248337",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.linalg.Vectors\n\u003cconsole\u003e:32: error: not found: value model\n       val itemFactorsDF \u003d model.itemFactors.map(row \u003d\u003e\n                           ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:44:23 AM",
      "dateFinished": "Apr 21, 2016 8:44:24 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Generate Personalized Recommendations For All Users",
      "text": "model.setPredictionCol(\"confidence\")\nval recommendationsDF \u003d model.transform(itemRatingsDF.select($\"userId\", $\"itemId\"))\n  .toDF(\"userId\", \"recommendedItemId\", \"confidence\")\nz.show(recommendationsDF.select($\"userId\", $\"recommendedItemId\", $\"confidence\"))",
      "dateUpdated": "Apr 21, 2016 8:40:50 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 284.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "recommendedItemId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091994_1923742857",
      "id": "20160317-011131_1642238218",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value model\n              model.setPredictionCol(\"confidence\")\n              ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:44:24 AM",
      "dateFinished": "Apr 21, 2016 8:44:24 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Enrich the recommendations",
      "text": "val enrichedRecommendationsDF \u003d \n   recommendationsDF.join(itemsDF, $\"items.itemId\" \u003d\u003d\u003d $\"recommendedItemId\")\n   .select($\"userId\", $\"recommendedItemId\", $\"title\", $\"tags\", $\"confidence\", $\"tags\")\n   .sort($\"userId\", $\"recommendedItemId\", $\"confidence\" desc)\n   \nz.show(enrichedRecommendationsDF.select($\"userId\", $\"recommendedItemId\", $\"title\", $\"confidence\"))",
      "dateUpdated": "Apr 21, 2016 8:40:51 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "recommendedItemId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "recommendedItemId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461096949529_1856645982",
      "id": "20160419-201549_521403480",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value recommendationsDF\n          recommendationsDF.join(itemsDF, $\"items.itemId\" \u003d\u003d\u003d $\"recommendedItemId\")\n          ^\n"
      },
      "dateCreated": "Apr 19, 2016 8:15:49 PM",
      "dateStarted": "Apr 21, 2016 8:44:24 AM",
      "dateFinished": "Apr 21, 2016 8:44:24 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Predict given userId and ItemId - Compare to Model Above",
      "text": "import org.jblas.DoubleMatrix\n\n// User 12663\nval givenUserId \u003d 1\nval givenUserFactors \u003d userFactorsDF.select($\"userId\", $\"userFactors\")\n  .where($\"userId\" \u003d\u003d\u003d givenUserId)\n  .map(row \u003d\u003e row.getSeq[Double](1))\n  .collect()\n\n// Item 318 // Shawshank Redemption\nval givenItemId \u003d 318 \nval givenItemFactors \u003d itemFactorsDF.select($\"itemId\", $\"itemFactors\")\n  .where($\"itemId\" \u003d\u003d\u003d givenItemId)\n  .map(row \u003d\u003e row.getSeq[Double](1))\n  .collect()\n  \n// Create JBlas DoubleMatrix\nval userFactors \u003d new DoubleMatrix(givenUserFactors)\nval itemFactors \u003d new DoubleMatrix(givenItemFactors)\n\n// Take dot product of the User x Item vectors\n// This should equal the confidence value in the offline-generated matrix\nval confidence \u003d userFactors.dot(itemFactors)",
      "dateUpdated": "Apr 21, 2016 8:40:51 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177091994_1923742857",
      "id": "20160317-011131_732311605",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.jblas.DoubleMatrix\ngivenUserId: Int \u003d 1\n\u003cconsole\u003e:34: error: not found: value userFactorsDF\n         val givenUserFactors \u003d userFactorsDF.select($\"userId\", $\"userFactors\")\n                                ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:11:31 AM",
      "dateStarted": "Apr 21, 2016 8:44:24 AM",
      "dateFinished": "Apr 21, 2016 8:44:24 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Enrich The Item Factors For Display Purposes",
      "text": "val enrichedItemFactorsDF \u003d itemFactorsDF\n  .join(itemsDF, $\"items.itemId\" \u003d\u003d\u003d $\"itemFactors.itemId\")\n  .select($\"items.itemId\", $\"title\", $\"tags\", $\"itemFactors\")\n  .sort($\"items.itemId\")\n  .as(\"enrichedItemFactors\")\n\nz.show(enrichedItemFactorsDF)",
      "dateUpdated": "Apr 21, 2016 8:40:51 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177140375_1901320152",
      "id": "20160317-011220_449708626",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value itemFactorsDF\n         val enrichedItemFactorsDF \u003d itemFactorsDF\n                                     ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:12:20 AM",
      "dateStarted": "Apr 21, 2016 8:44:24 AM",
      "dateFinished": "Apr 21, 2016 8:44:25 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train K-Means Cluster Model",
      "text": "import org.apache.spark.ml.clustering.KMeans\n\nval kmeans \u003d new KMeans()\n  .setK(5)\n  .setMaxIter(10)\n  .setFeaturesCol(\"itemFactors\")\n  .setPredictionCol(\"clusterId\")\n\nval kmeansModel \u003d kmeans.fit(enrichedItemFactorsDF)",
      "dateUpdated": "Apr 21, 2016 8:40:52 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177166619_2068331823",
      "id": "20160317-011246_903180471",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.clustering.KMeans\nkmeans: org.apache.spark.ml.clustering.KMeans \u003d kmeans_782a862696a7\n\u003cconsole\u003e:36: error: not found: value enrichedItemFactorsDF\n       val kmeansModel \u003d kmeans.fit(enrichedItemFactorsDF)\n                                    ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:12:46 AM",
      "dateStarted": "Apr 21, 2016 8:44:25 AM",
      "dateFinished": "Apr 21, 2016 8:44:25 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Cluster Centers",
      "text": "val clusterCenters \u003d kmeansModel.clusterCenters.mkString(\"\\n\")",
      "dateUpdated": "Apr 21, 2016 8:40:52 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177191726_-123735903",
      "id": "20160317-011311_819499675",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:33: error: not found: value kmeansModel\n         val clusterCenters \u003d kmeansModel.clusterCenters.mkString(\"\\n\")\n                              ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:13:11 AM",
      "dateStarted": "Apr 21, 2016 8:44:25 AM",
      "dateFinished": "Apr 21, 2016 8:44:25 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Evaluate K-Means Cluster Model Using Within Set Sum Of Squared Errors (WSSSE)",
      "text": "val WSSSE \u003d kmeansModel.computeCost(enrichedItemFactorsDF)\n\nprintln(\"Within Set Sum of Squared Errors \u003d \" + WSSSE)",
      "dateUpdated": "Apr 21, 2016 8:40:52 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177218519_-155639413",
      "id": "20160317-011338_940922176",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:33: error: not found: value kmeansModel\n         val WSSSE \u003d kmeansModel.computeCost(enrichedItemFactorsDF)\n                     ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:13:38 AM",
      "dateStarted": "Apr 21, 2016 8:44:25 AM",
      "dateFinished": "Apr 21, 2016 8:44:25 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Analyze the clusters",
      "text": "val enrichedClusteredItems \u003d kmeansModel.transform(enrichedItemFactorsDF)\n  .select($\"clusterId\", $\"itemId\", $\"title\", $\"itemFactors\", $\"tags\")\n  .sort($\"clusterId\", $\"itemId\")\n  \nz.show(enrichedClusteredItems)",
      "dateUpdated": "Apr 21, 2016 8:40:53 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "itemId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "itemId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177239457_-1211359746",
      "id": "20160317-011359_777998034",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:33: error: not found: value kmeansModel\n         val enrichedClusteredItems \u003d kmeansModel.transform(enrichedItemFactorsDF)\n                                      ^\n"
      },
      "dateCreated": "Mar 17, 2016 1:13:59 AM",
      "dateStarted": "Apr 21, 2016 8:44:25 AM",
      "dateFinished": "Apr 21, 2016 8:44:25 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Apr 21, 2016 8:40:53 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458177375610_-1064618818",
      "id": "20160317-011615_1706205729",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Mar 17, 2016 1:16:15 AM",
      "dateStarted": "Apr 21, 2016 8:44:25 AM",
      "dateFinished": "Apr 21, 2016 8:44:25 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Recommendations/02: Item-to-Item Collab Filtering + Clustering Recs (ALS + K-Means)",
  "id": "2BDAF991E",
  "angularObjects": {
    "2BCMAZYZ5": [],
    "2BDF8WQWY": [],
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}