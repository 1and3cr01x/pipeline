{
  "paragraphs": [
    {
      "text": "%md ![SVD](http://advancedspark.com/img/svd.png)",
      "dateUpdated": "Apr 14, 2016 9:11:07 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460655575003_-109981452",
      "id": "20160414-173935_976143776",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"http://advancedspark.com/img/svd.png\" alt\u003d\"SVD\" /\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 14, 2016 5:39:35 PM",
      "dateStarted": "Apr 14, 2016 9:11:07 PM",
      "dateFinished": "Apr 14, 2016 9:11:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ![User-to-Item Graph-based Recs](http://advancedspark.com/img/user-to-item-graph-based-recs.png)",
      "dateUpdated": "Apr 14, 2016 9:11:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460659284258_-782542034",
      "id": "20160414-184124_1255047583",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"http://advancedspark.com/img/user-to-item-graph-based-recs.png\" alt\u003d\"User-to-Item Graph-based Recs\" /\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 14, 2016 6:41:24 PM",
      "dateStarted": "Apr 14, 2016 9:11:07 PM",
      "dateFinished": "Apr 14, 2016 9:11:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get Edges",
      "text": "//val itemRatingsDF \u003d sqlContext.read\n//  .format(\"com.databricks.spark.csv\")\n//  .option(\"header\", \"true\") \n//  .option(\"inferSchema\", \"true\") \n//  .load(\"file:/root/pipeline/datasets/movielens/ml-latest/ratings.csv\")\n//  .toDF(\"userId\", \"itemId\", \"rating\", \"timestamp\")\n//  .select($\"userId\", $\"itemId\", $\"rating\")\n\nval itemRatingsDF \u003d sqlContext.read\n  .format(\"com.databricks.spark.csv\")\n  .option(\"header\", \"false\") \n  .option(\"inferSchema\", \"true\") \n  .load(\"/root/pipeline/datasets/graph/user-to-item-graph-based-recs.csv\")\n  .toDF(\"userId\", \"itemId\")\n  .select($\"userId\", $\"itemId\", lit(1.0) as \"rating\")\n\nz.show(itemRatingsDF)",
      "dateUpdated": "Apr 14, 2016 9:33:05 PM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 294.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "itemId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "itemId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460655575003_-109981452",
      "id": "20160414-173935_647383275",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "userId\titemId\trating\n10001\t90001\t1.0\n10001\t90002\t1.0\n10001\t90003\t1.0\n10001\t90004\t1.0\n10001\t90005\t1.0\n10002\t90004\t1.0\n10002\t90006\t1.0\n10003\t90001\t1.0\n10003\t90003\t1.0\n10003\t90006\t1.0\n"
      },
      "dateCreated": "Apr 14, 2016 5:39:35 PM",
      "dateStarted": "Apr 14, 2016 9:33:05 PM",
      "dateFinished": "Apr 14, 2016 9:33:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create Graph from edges",
      "text": "import org.apache.spark.graphx.Edge \nimport org.apache.spark.graphx.Graph\nimport org.apache.spark.graphx.PartitionStrategy\n\nval connectionEdgesRDD \u003d itemRatingsDF.map(row \u003d\u003e {\n  Edge(row(0).toString.toLong, row(1).toString.toLong, 1.0)\n//  Edge(row(0).toString.toLong, row(1).toString.toLong, row(2).toString.toDouble)\n})\n\nval graph \u003d Graph.fromEdges(connectionEdgesRDD, 0L).partitionBy(PartitionStrategy.RandomVertexCut)",
      "dateUpdated": "Apr 14, 2016 9:29:24 PM",
      "config": {
        "colWidth": 9.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 294.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460655575003_-109981452",
      "id": "20160414-173935_1104547073",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx.Edge\nimport org.apache.spark.graphx.Graph\nimport org.apache.spark.graphx.PartitionStrategy\nconnectionEdgesRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] \u003d MapPartitionsRDD[1776] at map at \u003cconsole\u003e:63\ngraph: org.apache.spark.graphx.Graph[Long,Double] \u003d org.apache.spark.graphx.impl.GraphImpl@3a4e8183\n"
      },
      "dateCreated": "Apr 14, 2016 5:39:35 PM",
      "dateStarted": "Apr 14, 2016 9:23:13 PM",
      "dateFinished": "Apr 14, 2016 9:23:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Generate Graph-based Recommendations using SVD++",
      "text": "//////////////////////////////////////\n// MUST CHANGE maxRating\n//////////////////////////////////////\n\nimport org.apache.spark.graphx.lib.SVDPlusPlus\nimport org.apache.spark.graphx.lib.SVDPlusPlus._\n\nval rank \u003d 2 // number of latent variables (ie. k)\nval maxIterations \u003d 10 // max iters for convergance\nval minRating \u003d 0 // 0 stars\nval maxRating \u003d 1 // 5 stars\nval gamma1 \u003d 0.007 // rate of bias change between iterations\nval gamma2 \u003d 0.007 // rate of latent variable vectors can change between iters\nval gamma6 \u003d 0.005 // bias dampening factor (keeps them small)\nval gamma7 \u003d 0.015 // degree to which latent variable vectors can interact\n\nval conf \u003d new Conf(rank, maxIterations, minRating, maxRating, gamma1, gamma2, gamma6, gamma7)\n\nvar (modelGraph, mean) \u003d SVDPlusPlus.run(graph.edges, conf)",
      "dateUpdated": "Apr 14, 2016 9:37:49 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460655575003_-109981452",
      "id": "20160414-173935_715476232",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx.lib.SVDPlusPlus\nimport org.apache.spark.graphx.lib.SVDPlusPlus._\nrank: Int \u003d 2\nmaxIterations: Int \u003d 10\nminRating: Int \u003d 0\nmaxRating: Int \u003d 1\ngamma1: Double \u003d 0.007\ngamma2: Double \u003d 0.007\ngamma6: Double \u003d 0.005\ngamma7: Double \u003d 0.015\nconf: org.apache.spark.graphx.lib.SVDPlusPlus.Conf \u003d org.apache.spark.graphx.lib.SVDPlusPlus$Conf@3c8fd9bb\norg.apache.spark.SparkException: Job 161 cancelled part of cancelled job group zeppelin-20160414-173935_715476232\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat org.apache.spark.graphx.impl.EdgeRDDImpl.count(EdgeRDDImpl.scala:89)\n\tat org.apache.spark.graphx.lib.SVDPlusPlus$.org$apache$spark$graphx$lib$SVDPlusPlus$$materialize(SVDPlusPlus.scala:215)\n\tat org.apache.spark.graphx.lib.SVDPlusPlus$$anonfun$run$1.apply$mcVI$sp(SVDPlusPlus.scala:178)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)\n\tat org.apache.spark.graphx.lib.SVDPlusPlus$.run(SVDPlusPlus.scala:130)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:99)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:106)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:110)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:112)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:114)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:116)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:118)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:120)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:122)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:124)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:126)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:128)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:130)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:132)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:134)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:136)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:138)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:140)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:142)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:144)\n\tat \u003cinit\u003e(\u003cconsole\u003e:146)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:150)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Apr 14, 2016 5:39:35 PM",
      "dateStarted": "Apr 14, 2016 9:35:14 PM",
      "dateFinished": "Apr 14, 2016 9:37:28 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.jblas.DoubleMatrix\n\n//////////////////////////////////\n// MUST CHANGE mean\n//////////////////////////////////\n\nval theMean \u003d 0.5\nval theMean \u003d mean\n\nval userId \u003d 10001\nval itemId \u003d 90006\n\nval userBias \u003d modelGraph.vertices.filter(_._1 \u003d\u003d userId).collect()(0)._2._3\nval itemBias \u003d modelGraph.vertices.filter(_._1 \u003d\u003d itemId).collect()(0)._2._3\n\nval userLatentFactors \u003d modelGraph.vertices.filter(_._1 \u003d\u003d userId).collect()(0)._2._2\nval itemLatentFactors \u003d modelGraph.vertices.filter(_._1 \u003d\u003d itemId).collect()(0)._2._1\n\n// Create JBlas DoubleMatrix from arrays\nval userLatentFactorVector \u003d new DoubleMatrix(userLatentFactors)\nval itemLatentFactorVector \u003d new DoubleMatrix(itemLatentFactors)\n\n// Take dot product of the User x Item vectors\n// This should equal the confidence value in the offline-generated matrix\nval confidence \u003d userLatentFactorVector.dot(itemLatentFactorVector) + theMean + userBias + itemBias",
      "dateUpdated": "Apr 14, 2016 9:36:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460663816878_-1544311516",
      "id": "20160414-195656_964830363",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.jblas.DoubleMatrix\nuserId: Int \u003d 10001\nitemId: Int \u003d 90006\nuserBias: Double \u003d 0.0\nitemBias: Double \u003d 0.0\nuserLatentFactors: Array[Double] \u003d Array(1.9115655504946123, 2.1684612679736626)\nitemLatentFactors: Array[Double] \u003d Array(0.7877834374251195, 0.47657585254874824)\nuserLatentFactorVector: org.jblas.DoubleMatrix \u003d [1.911566; 2.168461]\nitemLatentFactorVector: org.jblas.DoubleMatrix \u003d [0.787783; 0.476576]\nconfidence: Double \u003d 3.0393359577355743\n"
      },
      "dateCreated": "Apr 14, 2016 7:56:56 PM",
      "dateStarted": "Apr 14, 2016 9:37:28 PM",
      "dateFinished": "Apr 14, 2016 9:12:07 PM",
      "status": "RUNNING",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Graph Vertices returned from SVD++",
      "text": "%md ![SVD++ Return Value](http://advancedspark.com/img/svdplusplus-return-value.png)",
      "dateUpdated": "Apr 14, 2016 9:11:07 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460655575003_-109981452",
      "id": "20160414-173935_1046518591",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"http://advancedspark.com/img/svdplusplus-return-value.png\" alt\u003d\"SVD++ Return Value\" /\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Apr 14, 2016 5:39:35 PM",
      "dateStarted": "Apr 14, 2016 9:11:07 PM",
      "dateFinished": "Apr 14, 2016 9:11:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/////////////////////////////////////////\n// This is here for comparison purposes\n////////////////////////////////////////\n\nimport org.apache.spark.ml.recommendation.ALS\n\nval rank \u003d 2 // this is k\nval maxIterations \u003d 10\nval convergenceThreshold \u003d 0.01\n\nval als \u003d new ALS()\n  .setRank(rank)\n  .setRegParam(convergenceThreshold)\n  .setUserCol(\"userId\")\n  .setItemCol(\"itemId\")\n  .setRatingCol(\"rating\")\n\nval model \u003d als.fit(itemRatingsDF)\n\nval userFactors \u003d model.userFactors.collect()\nval itemFactors \u003d model.itemFactors.collect()\n\nmodel.setPredictionCol(\"confidence\")\n\nval recommendationsDF \u003d model.transform(itemRatingsDF.select($\"userId\", $\"itemId\"))\n  .toDF(\"userId\", \"recommendedItemId\", \"confidence\")\nz.show(recommendationsDF.sort($\"userId\", $\"recommendedItemId\", $\"confidence\" desc))\n",
      "dateUpdated": "Apr 14, 2016 9:36:39 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460660615961_721295800",
      "id": "20160414-190335_922495284",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Apr 14, 2016 7:03:35 PM",
      "dateStarted": "Apr 14, 2016 9:37:28 PM",
      "dateFinished": "Apr 14, 2016 9:11:24 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460669799913_1473207280",
      "id": "20160414-213639_1098156008",
      "dateCreated": "Apr 14, 2016 9:36:39 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Recommendations/03: User-to-Item Graph-based Recommendations (SVD++)",
  "id": "2BJGMHZND",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}