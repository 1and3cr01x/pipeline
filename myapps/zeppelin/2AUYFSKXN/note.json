{
  "paragraphs": [
    {
      "title": "Collaborative Filtering:  Matrix Factorization using Alternating Least Squares (ALS)",
      "text": "%md ![Alternating Least Squares - Matrix Factorization](http://advancedspark.com/img/collaborative-filtering-with-als-matrix-factorization.png)",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": true,
        "editorMode": "ace/mode/markdown",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435978153894_1534941045",
      "id": "20150704-024913_884517592",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"http://advancedspark.com/img/collaborative-filtering-with-als-matrix-factorization.png\" alt\u003d\"Alternating Least Squares - Matrix Factorization\" /\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 4, 2015 2:49:13 AM",
      "dateStarted": "Apr 20, 2016 8:13:16 AM",
      "dateFinished": "Apr 20, 2016 8:13:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get Reference Data for Enrichment",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")\n  .withColumnRenamed(\"id\", \"itemId\")\n  .as(\"items\")\n\nz.show(itemsDF.select($\"itemId\", $\"title\", $\"img\", $\"tags\"))",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 175.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456864386968_-1684206029",
      "id": "20160301-203306_1764877860",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job 0 cancelled part of cancelled job group zeppelin-20160301-203306_1764877860\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1127)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.infer(InferSchema.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$4.apply(JSONRelation.scala:114)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation$$anonfun$4.apply(JSONRelation.scala:109)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema$lzycompute(JSONRelation.scala:109)\n\tat org.apache.spark.sql.execution.datasources.json.JSONRelation.dataSchema(JSONRelation.scala:108)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:636)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:635)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.\u003cinit\u003e(LogicalRelation.scala:37)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:28)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:35)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:37)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:39)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:41)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:43)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:45)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:47)\n\tat \u003cinit\u003e(\u003cconsole\u003e:49)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:53)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Mar 1, 2016 8:33:06 PM",
      "dateStarted": "Apr 20, 2016 8:13:16 AM",
      "dateFinished": "Apr 20, 2016 8:14:14 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get Live Ratings from Cassandra",
      "text": "val cassandraConfig \u003d Map(\"keyspace\" -\u003e \"advancedspark\", \"table\" -\u003e \"item_ratings\")\n\nval itemRatingsDF \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\")\n  .options(cassandraConfig)\n  .load()\n  .toDF(\"userId\", \"itemId\", \"rating\", \"timestamp\")\n  .as(\"itemRatings\")",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456864425764_-1117794352",
      "id": "20160301-203345_1180596367",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "cassandraConfig: scala.collection.immutable.Map[String,String] \u003d Map(keyspace -\u003e advancedspark, table -\u003e item_ratings)\nitemRatingsDF: org.apache.spark.sql.DataFrame \u003d [userId: int, itemId: int, rating: int, timestamp: bigint]\n"
      },
      "dateCreated": "Mar 1, 2016 8:33:45 PM",
      "dateStarted": "Apr 20, 2016 8:13:20 AM",
      "dateFinished": "Apr 20, 2016 8:14:14 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train The ALS Model",
      "text": "import org.apache.spark.ml.recommendation.ALS\n\nval rank \u003d 10 // this is k\nval maxIterations \u003d 20\nval convergenceThreshold \u003d 0.01\n\nval als \u003d new ALS()\n  .setRank(rank)\n  .setRegParam(convergenceThreshold)\n  .setUserCol(\"userId\")\n  .setItemCol(\"itemId\")\n  .setRatingCol(\"rating\")\n\nval model \u003d als.fit(itemRatingsDF)",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435978256373_-160526409",
      "id": "20150704-025056_169923529",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.recommendation.ALS\nrank: Int \u003d 10\nmaxIterations: Int \u003d 20\nconvergenceThreshold: Double \u003d 0.01\nals: org.apache.spark.ml.recommendation.ALS \u003d als_c376c8334fdd\n\u003cconsole\u003e:35: error: not found: value itemRatingsDF\n       val model \u003d als.fit(itemRatingsDF)\n                           ^\n"
      },
      "dateCreated": "Jul 4, 2015 2:50:56 AM",
      "dateStarted": "Apr 20, 2016 8:14:14 AM",
      "dateFinished": "Apr 20, 2016 8:14:34 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Generate Recommendations for all Users",
      "text": "model.setPredictionCol(\"confidence\")\n\nval recommendationsDF \u003d model.transform(itemRatingsDF.select($\"userId\", $\"itemId\"))\n\nval enrichedRecommendationsDF \u003d \n   recommendationsDF.join(itemsDF, $\"itemId\" \u003d\u003d\u003d $\"id\")\n   .select($\"userId\", $\"itemId\", $\"title\", $\"description\", $\"tags\", $\"img\", $\"confidence\")\n   .sort($\"userId\", $\"itemId\", $\"confidence\" desc)\n   \nz.show(enrichedRecommendationsDF.select($\"userId\", $\"itemId\", $\"title\", $\"confidence\"))",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100304691_419185220",
      "id": "20160419-211144_1773438539",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:29: error: not found: value model\n              model.setPredictionCol(\"confidence\")\n              ^\n"
      },
      "dateCreated": "Apr 19, 2016 9:11:44 PM",
      "dateStarted": "Apr 20, 2016 8:10:32 AM",
      "dateFinished": "Apr 20, 2016 8:10:33 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Store full recommendations to disk",
      "text": "enrichedRecommendationsDF\n .select($\"userId\", $\"itemId\", $\"title\", $\"description\", $\"tags\", $\"img\", $\"confidence\")\n .sort($\"userId\", $\"itemId\", $\"confidence\" desc)\n .coalesce(1)\n .write.mode(\"overwrite\")\n .json(s\"\"\"/root/pipeline/work/serving/recommendations/als/full-recs/\"\"\")",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100402332_-1279142397",
      "id": "20160419-211322_1021941780",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:29: error: not found: value enrichedRecommendationsDF\nenrichedRecommendationsDF\n^\n"
      },
      "dateCreated": "Apr 19, 2016 9:13:22 PM",
      "dateStarted": "Apr 20, 2016 8:10:33 AM",
      "dateFinished": "Apr 20, 2016 8:10:33 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Store full recommendations in elasticSearch",
      "text": "import org.elasticsearch.spark.sql._ \nimport org.apache.spark.sql.SaveMode\n\nval esConfig \u003d Map(\"pushdown\" -\u003e \"true\", \"es.nodes\" -\u003e \"127.0.0.1\", \"es.port\" -\u003e \"9200\")\nenrichedRecommendationsDF.write.format(\"org.elasticsearch.spark.sql\").mode(SaveMode.Overwrite).options(esConfig)\n  .save(\"advancedspark/personalized-als\")",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100478491_-968756780",
      "id": "20160419-211438_174483447",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.elasticsearch.spark.sql._\nimport org.apache.spark.sql.SaveMode\nesConfig: scala.collection.immutable.Map[String,String] \u003d Map(pushdown -\u003e true, es.nodes -\u003e 127.0.0.1, es.port -\u003e 9200)\n\u003cconsole\u003e:35: error: not found: value enrichedRecommendationsDF\n              enrichedRecommendationsDF.write.format(\"org.elasticsearch.spark.sql\").mode(SaveMode.Overwrite).options(esConfig)\n              ^\n"
      },
      "dateCreated": "Apr 19, 2016 9:14:38 PM",
      "dateStarted": "Apr 20, 2016 8:10:34 AM",
      "dateFinished": "Apr 20, 2016 8:10:34 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md # PUT THIS IN A SEPARATE NOTEBOOK!",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100352255_809598085",
      "id": "20160419-211232_1591507464",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003ePUT THIS IN A SEPARATE NOTEBOOK!\u003c/h1\u003e\n"
      },
      "dateCreated": "Apr 19, 2016 9:12:32 PM",
      "dateStarted": "Apr 20, 2016 8:13:17 AM",
      "dateFinished": "Apr 20, 2016 8:13:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show User Factors Matrix",
      "text": "val userFactorsDF \u003d model.userFactors.map(row \u003d\u003e \n  (row.getInt(0),  Vectors.dense(row.getSeq[Float](1).toArray.map(_.toDouble)))\n).toDF(\"userId\", \"userFactors\")\n .sort($\"userId\" asc)\n .as(\"userFactors\")\n\nz.show(userFactorsDF)",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "userFactors",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "userFactors",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456873568687_1047705598",
      "id": "20160301-230608_2073461478",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value model\n         val userFactorsDF \u003d model.userFactors.map(row \u003d\u003e \n                             ^\n"
      },
      "dateCreated": "Mar 1, 2016 11:06:08 PM",
      "dateStarted": "Apr 20, 2016 8:10:34 AM",
      "dateFinished": "Apr 20, 2016 8:10:34 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show ItemFactors Matrix",
      "text": "val itemFactorsDF \u003d model.itemFactors.map(row \u003d\u003e\n  (row.getInt(0),  Vectors.dense(row.getSeq[Float](1).toArray.map(_.toDouble)))\n).toDF(\"itemId\", \"itemFactors\")\n .sort($\"itemId\" asc)\n .as(\"itemFactors\")\n\nz.show(itemFactorsDF)",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 314.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456865217092_774550692",
      "id": "20160301-204657_541485869",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value model\n         val itemFactorsDF \u003d model.itemFactors.map(row \u003d\u003e\n                             ^\n"
      },
      "dateCreated": "Mar 1, 2016 8:46:57 PM",
      "dateStarted": "Apr 20, 2016 8:10:35 AM",
      "dateFinished": "Apr 20, 2016 8:10:35 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Enriched the Item Factors",
      "text": "val enrichedItemFactorsDF \u003d itemFactorsDF\n  .join(itemsDF, $\"items.itemId\" \u003d\u003d\u003d $\"itemFactors.itemId\")\n  .select($\"items.itemId\", $\"title\", $\"tags\", $\"itemFactors\")\n  .sort($\"items.itemId\")\n  .as(\"enrichedItemFactors\")\n\nz.show(enrichedItemFactorsDF)",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461098265813_1981370856",
      "id": "20160419-203745_842155819",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value itemFactorsDF\n         val enrichedItemFactorsDF \u003d itemFactorsDF\n                                     ^\n"
      },
      "dateCreated": "Apr 19, 2016 8:37:45 PM",
      "dateStarted": "Apr 20, 2016 8:10:35 AM",
      "dateFinished": "Apr 20, 2016 8:10:35 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write the user and item factor vectors to disk",
      "text": "//userFactorsDF.select($\"userId\", $\"userFactors\")\n// .sort($\"userId\" asc)\n// .coalesce(1)\n// .write.mode(\"overwrite\")\n// .json(s\"\"\"/root/pipeline/work/serving/recommendations/als/user-factors\"\"\")\n\n//enrichedItemFactorsDF.select($\"itemId\", $\"title\", $\"itemFactors\", $\"description\", $\"tags\", $\"img\")\n// .sort($\"itemId\" asc)\n// .coalesce(1)\n// .write.mode(\"overwrite\")\n// .json(s\"\"\"/root/pipeline/work/serving/recommendations/als/item-factors\"\"\")",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460923887330_-466946993",
      "id": "20160417-201127_1997357021",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Apr 17, 2016 8:11:27 PM",
      "dateStarted": "Apr 20, 2016 8:10:36 AM",
      "dateFinished": "Apr 20, 2016 8:10:37 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write the User and item factor vectors to elasticSearch",
      "text": "val esConfig \u003d Map(\"pushdown\" -\u003e \"true\", \"es.nodes\" -\u003e \"127.0.0.1\", \"es.port\" -\u003e \"9200\")\n\nuserFactorsDF.select($\"userId\", $\"userFactors\")\n .sort($\"userId\" asc)\n .write.format(\"org.elasticsearch.spark.sql\")\n .mode(\"overwrite\")\n .options(esConfig)\n .save(\"advancedspark/user-factors-als\")\n \nenrichedItemFactorsDF.select($\"itemId\", $\"title\", $\"itemFactors\", $\"description\", $\"tags\", $\"img\")\n .sort($\"itemId\" asc)\n .write.format(\"org.elasticsearch.spark.sql\")\n .mode(\"overwrite\")\n .options(esConfig)\n .save(\"advancedspark/item-factors-als\")",
      "dateUpdated": "Apr 20, 2016 8:13:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460932159717_2026735022",
      "id": "20160417-222919_647801278",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "esConfig: scala.collection.immutable.Map[String,String] \u003d Map(pushdown -\u003e true, es.nodes -\u003e 127.0.0.1, es.port -\u003e 9200)\n\u003cconsole\u003e:36: error: not found: value userFactorsDF\n              userFactorsDF.select($\"userId\", $\"userFactors\")\n              ^\n"
      },
      "dateCreated": "Apr 17, 2016 10:29:19 PM",
      "dateStarted": "Apr 20, 2016 8:10:37 AM",
      "dateFinished": "Apr 20, 2016 8:10:37 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md # TODO MOVE THIS TO A SEPARATE NOTEBOOK!!\n",
      "dateUpdated": "Apr 20, 2016 8:13:17 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100072773_584320764",
      "id": "20160419-210752_1637087894",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eTODO MOVE THIS TO A SEPARATE NOTEBOOK!!\u003c/h1\u003e\n"
      },
      "dateCreated": "Apr 19, 2016 9:07:52 PM",
      "dateStarted": "Apr 20, 2016 8:13:17 AM",
      "dateFinished": "Apr 20, 2016 8:13:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Confidence Given userId and ItemId",
      "text": "////////////////////////////////////////////\n// TODO:  Must Update Item ID and User ID //\n////////////////////////////////////////////\n\nimport org.jblas.DoubleMatrix\n\n// Create JBlas DoubleMatrix from features\n\n// User 12663\nval givenUserId \u003d 12663\nval givenUserFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n  .where($\"userId\" \u003d\u003d\u003d givenUserId)\n  .collect()(0)\n\n// Item 7\nval givenItemId \u003d 7\nval givenItemFactors \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n  .where($\"itemId\" \u003d\u003d\u003d givenItemId)\n  .collect()(0)\n  \nval userFactors \u003d new DoubleMatrix(givenUserFactors)\nval itemFactors \u003d new DoubleMatrix(givenItemFactors)\n\n// Take dot product of the User x Item vectors\n// This should equal the confidence value in the offline-generated matrix\nval confidence \u003d userFactors.dot(itemFactors)\n\n////////////////////////////////////////////\n// TODO:  Must Update Item ID and User ID //\n////////////////////////////////////////////",
      "dateUpdated": "Apr 20, 2016 8:13:17 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456954812726_1681075178",
      "id": "20160302-214012_540640872",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.jblas.DoubleMatrix\ngivenUserId: Int \u003d 12663\n\u003cconsole\u003e:35: error: not found: value model\n         val givenUserFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n                                ^\n"
      },
      "dateCreated": "Mar 2, 2016 9:40:12 PM",
      "dateStarted": "Apr 20, 2016 8:10:37 AM",
      "dateFinished": "Apr 20, 2016 8:10:38 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Top 5 Similar Items to a given item",
      "text": "import org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity \n\n// Given Item: 7 (Spark)\nval givenItemId \u003d 7\n\nval givenItemLatentFactors \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n  .where($\"itemId\" \u003d\u003d\u003d givenItemId)\n  .map(row \u003d\u003e (row.getSeq[Float](1).toArray.map(_.toDouble)))\n  .collect()\n\n// Convert Array[Double] to DoubleMatrix\nval givenItemLatentFactorVector \u003d new DoubleMatrix(givenItemLatentFactors)\n\n// Find Similar Items to the Given Item\nval similarItems \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n  .filter($\"itemId\" !\u003d\u003d givenItemId)\n  .map{ row \u003d\u003e\n     val itemId \u003d row.getInt(0)\n     val itemLatentFactor \u003d row.getSeq[Float](1).map(_.toDouble).toArray\n     val itemLatentFactorVector \u003d new DoubleMatrix(itemLatentFactor)\n     val similarity \u003d Similarity.cosineSimilarity(itemLatentFactorVector, givenItemLatentFactorVector)\n     (itemId, similarity)\n  }\n  \n// Sort and Return Top 5 Items by Similarity to Given Item\nval sortedSimilarItems \u003d similarItems.top(5)(Ordering.by[(Int, Double), Double] { case (id, similarity) \u003d\u003e similarity })\n\nval sortedSimilarItemsDF \u003d sqlContext.createDataFrame(sortedSimilarItems).toDF(\"similarItemId\", \"similarity\")\n\nval enrichedSortedSimilarItemsDF \u003d \n   sortedSimilarItemsDF.join(itemsDF, $\"similarItemId\" \u003d\u003d\u003d $\"id\")\n   .select($\"similarItemId\", $\"title\", $\"description\", $\"tags\", $\"img\", $\"similarity\")\n   .sort($\"similarity\" desc)\n   \nz.show(enrichedSortedSimilarItemsDF.select(lit(givenItemId).as(\"itemId\"), $\"similarItemId\", $\"title\", $\"similarity\"))",
      "dateUpdated": "Apr 20, 2016 8:13:17 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 182.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460866272937_387648933",
      "id": "20160417-041112_1866839334",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity\ngivenItemId: Int \u003d 7\n\u003cconsole\u003e:38: error: not found: value model\n       val givenItemLatentFactors \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n                                    ^\n"
      },
      "dateCreated": "Apr 17, 2016 4:11:12 AM",
      "dateStarted": "Apr 20, 2016 8:10:38 AM",
      "dateFinished": "Apr 20, 2016 8:10:38 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Top 5 Similar Users to a given user",
      "text": "/////////////////////////////////////\n// May Need to Update User ID      //\n/////////////////////////////////////\n\n// User 12663\nimport org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity \n\n// Given User: 12663\nval givenUserId \u003d 12663\nval givenUserLatentFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n  .where($\"userId\" \u003d\u003d\u003d givenUserId)\n  .map(row \u003d\u003e (row.getSeq[Float](1).toArray.map(_.toDouble)))\n  .collect()(0)\n\n// Convert Array[Double] to DoubleMatrix\nval givenUserLatentFactorVector \u003d new DoubleMatrix(givenUserLatentFactors)\n\n// Find Similar Users to the Given User\nval similarUsers \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n  .filter($\"userId\" !\u003d\u003d givenUserId)\n  .map{ row \u003d\u003e\n     val userId \u003d row.getInt(0)\n     val userLatentFactor \u003d row.getSeq[Float](1).map(_.toDouble).toArray\n     val userLatentFactorVector \u003d new DoubleMatrix(userLatentFactor)\n     val similarity \u003d Similarity.cosineSimilarity(userLatentFactorVector, givenUserLatentFactorVector)\n     (userId, similarity)\n  }\n  \n// Sort and Return Top 5 Users by Similarity to Given Users\nval sortedSimilarUsers \u003d similarUsers.top(5)(Ordering.by[(Int, Double), Double] { \n  case (id, similarity) \u003d\u003e similarity \n}).mkString(\"\\n\")\n\n/////////////////////////////////\n// Must Update User ID         //\n/////////////////////////////////",
      "dateUpdated": "Apr 20, 2016 8:13:17 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460868286479_-213464598",
      "id": "20160417-044446_1052340818",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity\ngivenUserId: Int \u003d 12663\n\u003cconsole\u003e:40: error: not found: value model\n         val givenUserLatentFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n                                      ^\n"
      },
      "dateCreated": "Apr 17, 2016 4:44:46 AM",
      "dateStarted": "Apr 20, 2016 8:10:38 AM",
      "dateFinished": "Apr 20, 2016 8:10:39 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Apr 20, 2016 8:13:18 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460922825977_-689845777",
      "id": "20160417-195345_2022099383",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Apr 17, 2016 7:53:45 PM",
      "dateStarted": "Apr 20, 2016 8:10:39 AM",
      "dateFinished": "Apr 20, 2016 8:10:39 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Live Recs/03:  Generate User-to-Item Collaborative Filter Recs (ALS)",
  "id": "2AUYFSKXN",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}