{
  "paragraphs": [
    {
      "text": "val membersArray \u003d sqlContext.read\n  .format(\"com.databricks.spark.csv\")\n  .option(\"header\", \"false\") \n  .option(\"inferSchema\", \"true\") \n  .load(\"/\u003clist-of-whatever\u003e\")\n  .select($\"C0\", $\"C8\")\n  .toDF(\"member\", \"memberUrl\")\n  .map(row \u003d\u003e (row.getString(0), row.getString(1)))\n  .collect()",
      "dateUpdated": "Apr 23, 2016 12:47:29 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460513116915_1708594940",
      "id": "20160413-020516_229951153",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/\u003clist-of-whatever\u003e\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1307)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n\tat com.databricks.spark.csv.CsvRelation.firstLine$lzycompute(CsvRelation.scala:267)\n\tat com.databricks.spark.csv.CsvRelation.firstLine(CsvRelation.scala:263)\n\tat com.databricks.spark.csv.CsvRelation.inferSchema(CsvRelation.scala:240)\n\tat com.databricks.spark.csv.CsvRelation.\u003cinit\u003e(CsvRelation.scala:73)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:162)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:44)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:158)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:49)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:60)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:62)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:64)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:66)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:68)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:70)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:72)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:74)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:76)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:78)\n\tat \u003cinit\u003e(\u003cconsole\u003e:80)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:84)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Apr 13, 2016 2:05:16 AM",
      "dateStarted": "Apr 23, 2016 12:47:29 AM",
      "dateFinished": "Apr 23, 2016 12:47:29 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.SparkContext\nimport org.apache.spark.mllib.random.RandomRDDs._\nimport scala.util.Random\n\n// Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n// uniform distribution `N(0.0, 1.0)`, evenly distributed in 10 partitions.\nval u \u003d uniformRDD(sc, 1000000L, 2)\n//val uniformIds \u003d u.collect()\n\n// To transform the distribution in the generated RDD from `U(0.0, 1.0)` to `U(a, b)`, \n//  use uniformIds.map(v \u003d\u003e a + (b - a) * v)`.\nval v \u003d u.map(id \u003d\u003e (0 + 433 * id).toInt)\nval ids \u003d v.collect()\n\n// scala.util.Random.nextInt generates an Int between 0 inclusive, \u003cvalue\u003e exclusive\n//val winnersDF \u003d sc.parallelize((0 to 100).map(x \u003d\u003e membersArray(Random.nextInt(433)))).toDF(\"member\", \"memberUrl\")\n\n",
      "dateUpdated": "Apr 23, 2016 12:47:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 370.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460512761685_-1001113298",
      "id": "20160413-015921_2053145450",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.SparkContext\nimport org.apache.spark.mllib.random.RandomRDDs._\nimport scala.util.Random\nu: org.apache.spark.rdd.RDD[Double] \u003d RandomRDD[1084] at RDD at RandomRDD.scala:38\nv: org.apache.spark.rdd.RDD[Int] \u003d MapPartitionsRDD[1085] at map at \u003cconsole\u003e:58\nids: Array[Int] \u003d Array(268, 176, 401, 135, 292, 382, 383, 302, 412, 186, 16, 73, 274, 200, 19, 201, 233, 332, 99, 124, 153, 138, 224, 273, 330, 386, 430, 205, 153, 239, 201, 217, 402, 157, 102, 9, 314, 241, 431, 69, 91, 216, 12, 153, 63, 394, 399, 306, 328, 373, 323, 230, 361, 185, 122, 403, 416, 45, 137, 231, 212, 147, 198, 13, 213, 173, 11, 191, 59, 238, 366, 168, 291, 359, 155, 275, 89, 137, 84, 60, 164, 34, 108, 371, 376, 61, 420, 232, 72, 256, 317, 14, 415, 376, 11, 256, 155, 358, 428, 132, 13, 138, 250, 27, 325, 230, 142, 412, 237, 421, 186, 117, 87, 301, 99, 299, 380, 12, 413, 17, 419, 120, 234, 274, 412, 331, 354, 59, 223, 324, 269, 237, 331, 37, 240, 356, 92, 102, 124, 108, 249, 243, 128, 49, 30, 228, 320, 20, 62, 125, 63, 0, 8, 345, 18, 346, 184, 117, 185, 388, 57, 87, 397, 3..."
      },
      "dateCreated": "Apr 13, 2016 1:59:21 AM",
      "dateStarted": "Apr 23, 2016 12:47:31 AM",
      "dateFinished": "Apr 23, 2016 12:47:32 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Apr 21, 2016 1:11:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461200852751_-78747567",
      "id": "20160421-010732_1542544841",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Apr 21, 2016 1:07:32 AM",
      "dateStarted": "Apr 21, 2016 1:20:55 AM",
      "dateFinished": "Apr 21, 2016 1:20:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Statistics/01: Random Numbers",
  "id": "2BFQJSXNS",
  "angularObjects": {
    "2BCMAZYZ5": [],
    "2BDF8WQWY": [],
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {},
  "info": {}
}