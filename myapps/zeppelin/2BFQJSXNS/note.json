{
  "paragraphs": [
    {
      "text": "import org.apache.spark.SparkContext\nimport org.apache.spark.mllib.random.RandomRDDs._\nimport scala.util.Random\n\n\n\n// Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n// uniform distribution `N(0.0, 1.0)`, evenly distributed in 10 partitions.\nval u \u003d uniformRDD(sc, 1000000L, 2)\n//val uniformIds \u003d u.collect()\n\n// To transform the distribution in the generated RDD from `U(0.0, 1.0)` to `U(a, b)`, \n//  use uniformIds.map(v \u003d\u003e a + (b - a) * v)`.\nval v \u003d u.map(id \u003d\u003e (0 + 2433 * id).toInt)\nval ids \u003d v.collect()\n\n// scala.util.Random.nextInt generates an Int between 0 inclusive, \u003cvalue\u003e exclusive\nval randomId \u003d Random.nextInt(2433)",
      "dateUpdated": "Apr 21, 2016 1:20:57 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460512761685_-1001113298",
      "id": "20160413-015921_2053145450",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.SparkContext\nimport org.apache.spark.mllib.random.RandomRDDs._\nimport scala.util.Random\nu: org.apache.spark.rdd.RDD[Double] \u003d RandomRDD[77] at RDD at RandomRDD.scala:38\nv: org.apache.spark.rdd.RDD[Int] \u003d MapPartitionsRDD[78] at map at \u003cconsole\u003e:53\nids: Array[Int] \u003d Array(1481, 1781, 2129, 457, 1392, 1770, 464, 1311, 1182, 2023, 1524, 157, 617, 960, 1198, 573, 1933, 1069, 1860, 259, 890, 2296, 1722, 1355, 699, 1938, 1008, 160, 1631, 277, 1024, 1313, 1294, 1560, 1523, 1864, 623, 2120, 1579, 755, 1862, 2083, 1387, 1158, 1167, 1559, 547, 1909, 922, 797, 453, 1188, 2153, 1193, 1025, 818, 380, 2080, 1738, 1922, 1879, 2428, 1090, 297, 1340, 1704, 464, 2425, 37, 1622, 865, 1975, 2008, 1647, 1297, 2353, 731, 2240, 1893, 443, 1151, 626, 1435, 108, 1103, 3, 1791, 1550, 1370, 1380, 315, 2306, 2151, 1096, 736, 2332, 1142, 1200, 1326, 2031, 781, 1489, 71, 2078, 459, 482, 2341, 1070, 549, 487, 204, 1150, 2125, 1792, 1352, 62, 1405, 471, 934, 1369, 53, 793, 2089, 2078, 980, 1460, 350, 252, 1588, 1639, 1240, 1945, 1485, 362, 633, 933, 2210, 696, ...randomId: Int \u003d 2160\n"
      },
      "dateCreated": "Apr 13, 2016 1:59:21 AM",
      "dateStarted": "Apr 21, 2016 1:23:52 AM",
      "dateFinished": "Apr 21, 2016 1:23:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val membersArray \u003d sqlContext.read\n  .format(\"com.databricks.spark.csv\")\n  .option(\"header\", \"false\") \n  .option(\"inferSchema\", \"true\") \n  .load(\"\u003cput export.csv here\u003e\")\n  .select($\"C0\")\n  .toDF(\"email\")\n  .map(row \u003d\u003e row.getString(0))\n  .collect()",
      "dateUpdated": "Apr 21, 2016 1:11:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460513116915_1708594940",
      "id": "20160413-020516_229951153",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/root/pipeline/\u003cput export.csv here\u003e\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1307)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n\tat com.databricks.spark.csv.CsvRelation.firstLine$lzycompute(CsvRelation.scala:267)\n\tat com.databricks.spark.csv.CsvRelation.firstLine(CsvRelation.scala:263)\n\tat com.databricks.spark.csv.CsvRelation.inferSchema(CsvRelation.scala:240)\n\tat com.databricks.spark.csv.CsvRelation.\u003cinit\u003e(CsvRelation.scala:73)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:162)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:44)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:158)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:36)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:45)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:47)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:49)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:51)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:53)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:55)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:57)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:59)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:61)\n\tat \u003cinit\u003e(\u003cconsole\u003e:63)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:67)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Apr 13, 2016 2:05:16 AM",
      "dateStarted": "Apr 21, 2016 1:11:51 AM",
      "dateFinished": "Apr 21, 2016 1:20:55 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Apr 21, 2016 1:11:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461200852751_-78747567",
      "id": "20160421-010732_1542544841",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Apr 21, 2016 1:07:32 AM",
      "dateStarted": "Apr 21, 2016 1:20:55 AM",
      "dateFinished": "Apr 21, 2016 1:20:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Statistics/01: Random Numbers",
  "id": "2BFQJSXNS",
  "angularObjects": {
    "2BCMAZYZ5": [],
    "2BDF8WQWY": [],
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {},
  "info": {}
}