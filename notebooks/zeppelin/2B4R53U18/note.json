{
  "paragraphs": [
    {
      "text": "# Databricks notebook source exported at Tue, 27 Oct 2015 09:43:00 UTC\n# MAGIC %md\n# MAGIC # External modeling libraries\n# MAGIC ---\n# MAGIC How can we leverage our existing experience with modeling libraries like [scikit-learn](http://scikit-learn.org/stable/index.html)?  We\u0027ll explore three approaches that make use of existing libraries, but still benefit from the parallelism provided by Spark.\n# MAGIC \n# MAGIC These approaches are:\n# MAGIC  * Grid Search\n# MAGIC  * Cross Validation\n# MAGIC  * Sampling\n# MAGIC  \n# MAGIC We\u0027ll start by using scikit-learn on the driver and then we\u0027ll demonstrate the parallel techniques.\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Part 1: Use scikit-learn locally\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Load the data from `sklearn.datasets`, and create test and train sets.\n\n# COMMAND ----------\n\nimport numpy as np\nfrom sklearn import datasets\n\n# COMMAND ----------\n\n# Load the data\niris \u003d datasets.load_iris()\n\n# Generate test and train sets\nsize \u003d len(iris.target)\nindices \u003d np.random.permutation(size)\n\ncutoff \u003d int(size * .30)\n\ntestX \u003d iris.data[indices[0:cutoff],:]\ntrainX \u003d iris.data[indices[cutoff:],:]\ntestY \u003d iris.target[indices[0:cutoff]]\ntrainY \u003d iris.target[indices[cutoff:]]\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Build a nearest neighbors classifier using [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n\n# COMMAND ----------\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create a KNeighborsClassifier using the default settings\nknn \u003d KNeighborsClassifier()\nknn.fit(trainX, trainY)\n\npredictions \u003d knn.predict(testX)\n\n# Print out the accuracy of the classifier on the test set\nprint sum(predictions \u003d\u003d testY) / float(len(testY))\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Part 2: Grid Search\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Define a function `runNearestNeighbors` that takes in a parameter `k` and returns a tuple of (`k`, accuracy).  Note that we\u0027ll load the data from `sklearn.datasets`, and we\u0027ll create train and test splits using [sklearn.cross_validation.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html).\n\n# COMMAND ----------\n\nfrom sklearn.cross_validation import train_test_split\n\ndef runNearestNeighbors(k):\n    # Load dataset from sklearn.datasets\n    irisData \u003d datasets.load_iris()\n    \n    # Split into train and test using sklearn.cross_validation.train_test_split\n    yTrain, yTest, XTrain, XTest \u003d train_test_split(irisData.target, \n                                                    irisData.data)\n    \n    # Build the model\n    knn \u003d KNeighborsClassifier(n_neighbors\u003dk)\n    knn.fit(XTrain, yTrain)\n    \n    # Calculate predictions and accuracy\n    predictions \u003d knn.predict(XTest)\n    accuracy \u003d (predictions \u003d\u003d yTest).sum() / float(len(yTest))\n    \n    return (k, accuracy)   \n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Now we\u0027ll run a grid search for `k` from 1 to 10.\n\n# COMMAND ----------\n\nk \u003d sc.parallelize(xrange(1, 11))\nresults \u003d k.map(runNearestNeighbors)\nprint \u0027\\n\u0027.join(map(str, results.collect()))\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Let\u0027s transfer the data using a Broadcast instead of loading it at each executor.  You can create a [Broadcast](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.Broadcast) variable using [sc.broadcast()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.broadcast).\n\n# COMMAND ----------\n\n# Create the Broadcast variable\nirisBroadcast \u003d sc.broadcast(iris)\n\ndef runNearestNeighborsBroadcast(k):\n    # Using the data in the irisBroadcast variable split into train and test using\n    # sklearn.cross_validation.train_test_split\n    yTrain, yTest, XTrain, XTest \u003d train_test_split(irisBroadcast.value.target,\n                                                    irisBroadcast.value.data)\n    \n    # Build the model\n    knn \u003d KNeighborsClassifier(n_neighbors\u003dk)\n    knn.fit(XTrain, yTrain)\n    \n    # Calculate predictions and accuracy\n    predictions \u003d knn.predict(XTest)\n    accuracy \u003d (predictions \u003d\u003d yTest).sum() / float(len(yTest))\n    \n    return (k, accuracy)   \n  \n# Rerun grid search\nk \u003d sc.parallelize(xrange(1, 11))\nresults \u003d k.map(runNearestNeighborsBroadcast)\nprint \u0027\\n\u0027.join(map(str, results.collect()))\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ### Part 3: Cross Validation\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Now we\u0027ll use [sklearn.cross_validation.KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) to evaluate our model using 10-fold cross validation.  First, generate the 10 folds using `KFold`.\n\n# COMMAND ----------\n\nfrom sklearn.cross_validation import KFold\n\n# Create indicies for 10-fold cross validation\nkf \u003d KFold(size, n_folds\u003d10)\n\nfolds \u003d sc.parallelize(kf)\nprint folds.take(1)\n\n# COMMAND ----------\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\ndef preparePlot(xticks, yticks, figsize\u003d(10.5, 6), hideLabels\u003dFalse, gridColor\u003d\u0027#999999\u0027,\n                gridWidth\u003d1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax \u003d plt.subplots(figsize\u003dfigsize, facecolor\u003d\u0027white\u0027, edgecolor\u003d\u0027white\u0027)\n    ax.axes.tick_params(labelcolor\u003d\u0027#999999\u0027, labelsize\u003d\u002710\u0027)\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position(\u0027none\u0027)\n        axis.set_ticks(ticks)\n        axis.label.set_color(\u0027#999999\u0027)\n        if hideLabels: axis.set_ticklabels([])\n    plt.grid(color\u003dgridColor, linewidth\u003dgridWidth, linestyle\u003d\u0027-\u0027)\n    map(lambda position: ax.spines[position].set_visible(False), [\u0027bottom\u0027, \u0027top\u0027, \u0027left\u0027, \u0027right\u0027])\n    return fig, ax\n\n\nanArray \u003d np.zeros(150)\n\ndata \u003d []\nfor fold in folds.collect():\n  bIdx, rIdx \u003d fold\n  anArray[bIdx] \u003d 0\n  anArray[rIdx] \u003d 1\n  data.append(anArray.copy())\n\ndataValues \u003d np.vstack(data)\n\n# generate layout and plot\nfig, ax \u003d preparePlot(np.arange(-.5, 150, 15), np.arange(-.5, 10, 1), figsize\u003d(8,7), hideLabels\u003dTrue,\n                      gridColor\u003d\u0027#333333\u0027, gridWidth\u003d1.1)\nimage \u003d plt.imshow(dataValues,interpolation\u003d\u0027nearest\u0027, aspect\u003d\u0027auto\u0027, cmap\u003dcm.winter)\ndisplay(fig) \n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Create a function that runs nearest neighbors based on the fold information passed in.  Note that we\u0027ll have the function return an [np.array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) which provides us with additional functionality that we\u0027ll take advantage of in a couple steps.\n\n# COMMAND ----------\n\nimport numpy as np\n\ndef runNearestNeighborsWithFolds((trainIndex, testIndex)):\n    # Assign training and test sets from irisBroadcast using trainIndex and testIndex\n    XTrain \u003d irisBroadcast.value.data[trainIndex]\n    yTrain \u003d irisBroadcast.value.target[trainIndex]\n    XTest \u003d irisBroadcast.value.data[testIndex]\n    yTest \u003d irisBroadcast.value.target[testIndex]\n    \n   # Build the model\n    knn \u003d KNeighborsClassifier(n_neighbors\u003d5)\n    knn.fit(XTrain, yTrain)\n    \n    # Calculate predictions\n    predictions \u003d knn.predict(XTest)\n    \n    # Compute the number of correct predictions and total predictions\n    correct \u003d (predictions \u003d\u003d yTest).sum() \n    total \u003d len(testIndex)\n    \n    # Return an np.array of the number of correct predictions and total predictions\n    return np.array([correct, total])\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Computer nearest neighbors using each fold.\n\n# COMMAND ----------\n\n# Run nearest neighbors on each of the folds\nfoldResults \u003d folds.map(runNearestNeighborsWithFolds)\nprint \u0027correct / total\\n\u0027 + \u0027\\n\u0027.join(map(str, foldResults.collect()))\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Now aggregate the results from the folds to see overall accuracy\n\n# COMMAND ----------\n\n# Note that using .sum() on an RDD of numpy arrays sums by columns \ncorrect, total \u003d foldResults.sum()\nprint correct / float(total)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ### Part 4: Sampling\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC We might have a dataset that is too large where we can\u0027t use our external modeling library on the full data set.  In this case we might want to build several models on samples of the dataset.  We could either build the same model, using different parameters, or try completely different techniques to see what works best.\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC First we\u0027ll parallelize the iris dataset and distributed it across our cluster.  \n\n# COMMAND ----------\n\n# Split the iris dataset into 8 partitions\nirisData \u003d sc.parallelize(zip(iris.target, iris.data), 8)\nprint irisData.take(2), \u0027\\n\u0027\n\n# View the number of elements found in each of the eight partitions\nprint (irisData\n       .mapPartitions(lambda x: [len(list(x))])\n       .collect())\n\n# View the target (y) stored by partition\nprint \u0027\\n\u0027, irisData.keys().glom().collect()\n\n# COMMAND ----------\n\ndataValues \u003d np.vstack([np.array(x[:18]) for x in irisData.keys().glom().collect()])\n\n# generate layout and plot\nfig, ax \u003d preparePlot(np.arange(-.5, 18, 2), np.arange(-.5, 10, 1), figsize\u003d(8,7), hideLabels\u003dTrue,\n                      gridColor\u003d\u0027#555555\u0027, gridWidth\u003d1.1)\nimage \u003d plt.imshow(dataValues,interpolation\u003d\u0027nearest\u0027, aspect\u003d\u0027auto\u0027, cmap\u003dcm.Blues)\ndisplay(fig) \n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Since each of the partitions represents a dataset that we\u0027ll be using to run our local model, we have a problem.  The data is ordered, so our partitions are mostly homogenous with regard to our target variable.\n# MAGIC \n# MAGIC We\u0027ll repartition the data using `partitionBy` so that the data is randomly ordered across partitions.\n\n# COMMAND ----------\n\n# Randomly reorder the data across partitions\nrandomOrderData \u003d (irisData\n                   .map(lambda x: (np.random.randint(5), x))\n                   .partitionBy(5)\n                   .values())\n\n# Show the new groupings of target variables\nprint randomOrderData.keys().glom().collect()\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC Finally, we\u0027ll build a function that takes in the target and data from the `randomOrderData` RDD and returns the number of correct and total predictions (with regard to a test set).\n\n# COMMAND ----------\n\nprint randomOrderData.keys().mapPartitions(lambda x: [len(list(x))]).collect()\n\n# COMMAND ----------\n\ndata \u003d []\nfor x in randomOrderData.keys().glom().collect():\n  temp \u003d np.repeat(-1, 35)\n  pData \u003d x[:35]\n  temp[:len(pData)] \u003d pData\n  data.append(temp.copy())\n\ndataValues \u003d np.vstack(data)\n\nfig, ax \u003d preparePlot(np.arange(-.5, 35, 4), np.arange(-.5, 5, 1), figsize\u003d(8,7), hideLabels\u003dTrue,\n                      gridColor\u003d\u0027#555555\u0027, gridWidth\u003d1.1)\nimage \u003d plt.imshow(dataValues,interpolation\u003d\u0027nearest\u0027, aspect\u003d\u0027auto\u0027, cmap\u003dcm.Blues)\ndisplay(fig) \n\n# COMMAND ----------\n\n# Recall what randomOrderData contains\nprint randomOrderData.take(3)\n\n# COMMAND ----------\n\ndef runNearestNeighborsPartition(labelAndFeatures):\n    y, X \u003d zip(*labelAndFeatures)\n    yTrain, yTest, XTrain, XTest \u003d train_test_split(y, X)\n    \n    knn \u003d KNeighborsClassifier()\n    knn.fit(XTrain, yTrain)\n    \n    predictions \u003d knn.predict(XTest)\n    correct \u003d (predictions \u003d\u003d yTest).sum() \n    total \u003d len(yTest)\n    return [np.array([correct, total])]\n\nsampleResults \u003d randomOrderData.mapPartitions(runNearestNeighborsPartition)\n\nprint \u0027correct / total\\n\u0027 + \u0027\\n\u0027.join(map(str, sampleResults.collect()))\n\n# COMMAND ----------\n\n\n",
      "dateUpdated": "Oct 28, 2015 6:01:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446055241613_738735579",
      "id": "20151028-180041_753809394",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:1: error: illegal start of definition\n       # Databricks notebook source exported at Tue, 27 Oct 2015 09:43:00 UTC\n       ^\n"
      },
      "dateCreated": "Oct 28, 2015 6:00:41 PM",
      "dateStarted": "Oct 28, 2015 6:01:32 PM",
      "dateFinished": "Oct 28, 2015 6:01:32 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446055292069_-1080913825",
      "id": "20151028-180132_1353713564",
      "dateCreated": "Oct 28, 2015 6:01:32 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "SciKitLearn/01:  Nearest Neighbors with KFolds",
  "id": "2B4R53U18",
  "angularObjects": {
    "2AR33ZMZJ": [],
    "2AS9P7JSA": [],
    "2ARR8UZDJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}