{
  "paragraphs": [
    {
      "text": "%md # Start the Kafka Spark Streaming Job\n* ### This is optional.\n* ### Alternatively, use $PIPELINE_HOME/flux-start-kafka-streaming-likes.sh",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435352774943_904665808",
      "id": "20150626-210614_464619583",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eStart the Kafka Spark Streaming Job\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ch3\u003eThis is optional.\u003c/h3\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ch3\u003eAlternatively, use $PIPELINE_HOME/flux-start-kafka-streaming-likes.sh\u003c/h3\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jun 26, 2015 9:06:14 PM",
      "dateStarted": "Oct 6, 2015 11:18:18 PM",
      "dateFinished": "Oct 6, 2015 11:18:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\nz.reset()f\nz.addRepo(\"maven central\").url(\"search.maven.org\")\nz.load(\"com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M3\")\nz.load(\"org.elasticsearch:elasticsearch-spark_2.10:2.1.0\")\nz.load(\"com.databricks:spark-csv_2.10:1.2.0\")\nz.load(\"org.apache.spark:spark-streaming-kafka-assembly_2.10:1.5.1\")\nz.load(\"/root/zeppelin-0.6.0-spark-1.5.1-hadoop-2.6.0-fluxcapacitor/lib/mysql-connector-java.jar\")\nz.load(\"/root/pipeline/myapps/simpledatasource/target/scala-2.10/simpledatasource_2.10-1.0.jar\")",
      "dateUpdated": "Nov 30, 2015 7:07:21 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1438110093664_1508237074",
      "id": "20150728-190133_576357566",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Must be used before SparkInterpreter (%spark) initialized"
      },
      "dateCreated": "Jul 28, 2015 7:01:33 PM",
      "dateStarted": "Oct 15, 2015 4:27:49 AM",
      "dateFinished": "Oct 15, 2015 4:27:49 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.SparkConf\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.Row\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.Time\nimport sqlContext.implicits._\n\ncase class Like(fromUserId: Int, toUserId: Int, batchtime: Long)\n\nval sqlContext \u003d SQLContext.getOrCreate(sc)\n",
      "dateUpdated": "Oct 15, 2015 4:24:42 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1444882696183_576691761",
      "id": "20151015-041816_2129226927",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:49: error: object kafka is not a member of package org.apache.spark.streaming\n       import org.apache.spark.streaming.kafka.KafkaUtils\n                                         ^\n"
      },
      "dateCreated": "Oct 15, 2015 4:18:16 AM",
      "dateStarted": "Oct 15, 2015 4:24:42 AM",
      "dateFinished": "Oct 15, 2015 4:24:42 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Configure Kafka",
      "text": "val brokers \u003d \"localhost:9092\"\nval topics \u003d Set(\"likes\")\nval kafkaParams \u003d Map[String, String](\"metadata.broker.list\" -\u003e brokers)\nval cassandraConfig \u003d Map(\"keyspace\" -\u003e \"fluxcapacitor\", \"table\" -\u003e \"likes\")",
      "dateUpdated": "Oct 15, 2015 4:24:44 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435351067462_-362700882",
      "id": "20150626-203747_286831760",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "brokers: String \u003d localhost:9092\ntopics: scala.collection.immutable.Set[String] \u003d Set(likes)\nkafkaParams: scala.collection.immutable.Map[String,String] \u003d Map(metadata.broker.list -\u003e localhost:9092)\ncassandraConfig: scala.collection.immutable.Map[String,String] \u003d Map(keyspace -\u003e fluxcapacitor, table -\u003e likes)\n"
      },
      "dateCreated": "Jun 26, 2015 8:37:47 PM",
      "dateStarted": "Oct 15, 2015 4:24:44 AM",
      "dateFinished": "Oct 15, 2015 4:24:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create the Kafka Stream",
      "text": "val likesStream \u003d KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)\n\nlikesStream.foreachRDD {\n  (message: RDD[(String, String)], batchTime: Time) \u003d\u003e {\n    // convert each RDD from the batch into a DataFrame\n    val df \u003d message.map(_._2.split(\",\")).map(like \u003d\u003e Like(like(0).trim.toInt, like(1).trim.toInt, batchTime.milliseconds)).toDF(\"fromuserid\", \"touserid\", \"batchtime\")\n\n    // save the DataFrame to Cassandra\n    df.write.format(\"org.apache.spark.sql.cassandra\")\n      .mode(SaveMode.Append)\n      .options(cassandraConfig)\n      .save()\n  }\n}",
      "dateUpdated": "Oct 15, 2015 4:24:46 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435352502700_1656285081",
      "id": "20150626-210142_283675308",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:55: error: not found: value KafkaUtils\n       val likesStream \u003d KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)\n                         ^\n"
      },
      "dateCreated": "Jun 26, 2015 9:01:42 PM",
      "dateStarted": "Oct 15, 2015 4:24:46 AM",
      "dateFinished": "Oct 15, 2015 4:24:46 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Setup the Stream Data Transformations and Actions (Append to Cassandra Table)",
      "text": "import org.apache.spark.sql.SaveMode\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.Time\n\nlikesStream.foreachRDD{\n  (rdd: RDD[(String,String)], batchTime: Time) \u003d\u003e {\n      // convert each RDD from the batch into a DataFrame\n      val df \u003d rdd.toDF(\"from_user_id\", \"to_user_id\").select($\"from_user_id\", $\"to_user_id\")\n      \n      // add the batch time to the DataFrame\n      val dfWithBatchTime \u003d df.withColumn(\"batch_time\", lit(batchTime.milliseconds))\n      \n      // save the DataFrame to Cassandra\n      dfWithBatchTime.write.format(\"org.apache.spark.sql.cassandra\")\n        .mode(SaveMode.Append)\n        .options(cassandraConfig)\n        .save()\n  }\n}",
      "dateUpdated": "Oct 15, 2015 4:24:48 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435352637874_125200642",
      "id": "20150626-210357_701410267",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.SaveMode\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.Time\n\u003cconsole\u003e:57: error: not found: value likesStream\n              likesStream.foreachRDD{\n              ^\n"
      },
      "dateCreated": "Jun 26, 2015 9:03:57 PM",
      "dateStarted": "Oct 15, 2015 4:24:48 AM",
      "dateFinished": "Oct 15, 2015 4:24:48 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Start the Stream",
      "text": "ssc.start()",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435811421871_1604917400",
      "id": "20150702-043021_1434265086",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 2, 2015 4:30:21 AM",
      "dateStarted": "Oct 6, 2015 11:18:37 PM",
      "dateFinished": "Oct 6, 2015 11:18:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Uncomment To stop the stream",
      "text": "//ssc.stop(stopSparkContext\u003dfalse, stopGracefully\u003dfalse)",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435900392138_1989524513",
      "id": "20150703-051312_1037224009",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 3, 2015 5:13:12 AM",
      "dateStarted": "Oct 6, 2015 11:18:38 PM",
      "dateFinished": "Oct 6, 2015 11:18:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436047416200_1602967377",
      "id": "20150704-220336_100190094",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jul 4, 2015 10:03:36 PM",
      "dateStarted": "Oct 6, 2015 11:18:38 PM",
      "dateFinished": "Oct 6, 2015 11:18:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Streaming/01: Start Kafka Streaming Server",
  "id": "2ASK4UQ3M",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}
