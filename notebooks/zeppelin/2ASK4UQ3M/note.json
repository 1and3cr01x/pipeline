{
  "paragraphs": [
    {
      "text": "%md # Start the Kafka Spark Streaming Job",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435352774943_904665808",
      "id": "20150626-210614_464619583",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eStart the Kafka Spark Streaming Job\u003c/h1\u003e\n"
      },
      "dateCreated": "Jun 26, 2015 9:06:14 PM",
      "dateStarted": "Jul 4, 2015 9:59:58 PM",
      "dateFinished": "Jul 4, 2015 9:59:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\nz.addRepo(\"maven central\").url(\"search.maven.org\")\nz.load(\"com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M1\")\nz.load(\"org.apache.spark:spark-streaming-kafka-assembly_2.10:1.4.0\")",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1438110093664_1508237074",
      "id": "20150728-190133_576357566",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Must be used before SparkInterpreter (%spark) initialized"
      },
      "dateCreated": "Jul 28, 2015 7:01:33 PM",
      "dateStarted": "Sep 27, 2015 4:32:55 AM",
      "dateFinished": "Sep 27, 2015 4:32:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Configure Kafka",
      "text": "val brokers \u003d \"demo.fluxcapacitor.com:39092,demo.fluxcapacitor.com:39093\";\rval topicsSet \u003d Set(\"likes\");\rval kafkaParams \u003d Map[String, String](\"metadata.broker.list\" -\u003e brokers);",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435351067462_-362700882",
      "id": "20150626-203747_286831760",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "brokers: String \u003d demo.fluxcapacitor.com:39092,demo.fluxcapacitor.com:39093\ntopicsSet: scala.collection.immutable.Set[String] \u003d Set(likes)\nkafkaParams: scala.collection.immutable.Map[String,String] \u003d Map(metadata.broker.list -\u003e demo.fluxcapacitor.com:39092,demo.fluxcapacitor.com:39093)\n"
      },
      "dateCreated": "Jun 26, 2015 8:37:47 PM",
      "dateStarted": "Sep 27, 2015 3:57:28 AM",
      "dateFinished": "Sep 27, 2015 3:57:29 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create the Kafka Stream",
      "text": "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nimport kafka.serializer.StringDecoder\n\ndef create(): StreamingContext \u003d {\n  @transient val newSsc \u003d new StreamingContext(sc, Seconds(2))\n  println(s\"Creating new StreamingContext $newSsc\")\n\n  newSsc\n}\n\nval ssc \u003d StreamingContext.getActiveOrCreate(create)\n\nval likeStream \u003d KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)\n",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435352502700_1656285081",
      "id": "20150626-210142_283675308",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nimport kafka.serializer.StringDecoder\ncreate: ()org.apache.spark.streaming.StreamingContext\nCreating new StreamingContext org.apache.spark.streaming.StreamingContext@294bfafb\nssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@294bfafb\nlikeStream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] \u003d org.apache.spark.streaming.kafka.DirectKafkaInputDStream@6106a037\n"
      },
      "dateCreated": "Jun 26, 2015 9:01:42 PM",
      "dateStarted": "Jul 28, 2015 9:22:21 PM",
      "dateFinished": "Jul 28, 2015 9:22:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Setup the Stream Data Transformations and Actions (Append to Cassandra Table)",
      "text": "import org.apache.spark.sql.SaveMode\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.Time\n\nlikeStream.foreachRDD{\n  (rdd: RDD[(String,String)], batchTime: Time) \u003d\u003e {\n      // convert each RDD from the batch into a DataFrame\n      val df \u003d rdd.toDF(\"from_user_id\", \"to_user_id\").select($\"from_user_id\", $\"to_user_id\")\n      \n      // add the batch time to the DataFrame\n      val dfWithBatchTime \u003d df.withColumn(\"batch_time\", lit(batchTime.milliseconds))\n      \n      // save the DataFrame to Cassandra\n      dfWithBatchTime.write.format(\"org.apache.spark.sql.cassandra\")\n        .mode(SaveMode.Append)\n        .options(Map(\"keyspace\" -\u003e \"fluxcapacitor\", \"table\" -\u003e \"real_time_likes\"))\n        .save()\n  }\n}",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435352637874_125200642",
      "id": "20150626-210357_701410267",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.SaveMode\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.Time\n"
      },
      "dateCreated": "Jun 26, 2015 9:03:57 PM",
      "dateStarted": "Jul 28, 2015 9:22:24 PM",
      "dateFinished": "Jul 28, 2015 9:22:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Start the Stream",
      "text": "ssc.start()",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435811421871_1604917400",
      "id": "20150702-043021_1434265086",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 2, 2015 4:30:21 AM",
      "dateStarted": "Jul 28, 2015 9:22:29 PM",
      "dateFinished": "Jul 28, 2015 9:22:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Uncomment To stop the stream",
      "text": "//ssc.stop(stopSparkContext\u003dfalse, stopGracefully\u003dfalse)",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435900392138_1989524513",
      "id": "20150703-051312_1037224009",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 3, 2015 5:13:12 AM",
      "dateStarted": "Jul 4, 2015 8:25:27 AM",
      "dateFinished": "Jul 4, 2015 8:25:27 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436047416200_1602967377",
      "id": "20150704-220336_100190094",
      "dateCreated": "Jul 4, 2015 10:03:36 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "99. Kafka-Streaming-Server",
  "id": "2ASK4UQ3M",
  "angularObjects": {},
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}
