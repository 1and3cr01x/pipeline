{
  "paragraphs": [
    {
      "text": "// Databricks notebook source exported at Tue, 27 Oct 2015 09:39:31 UTC\n// MAGIC %md\n// MAGIC # Pipelines and Logistic Regression\n// MAGIC  \n// MAGIC In this lab we\u0027ll cover using transformers, estimators, evaluators, and pipelines.  We\u0027ll use transformers and estimators to prepare our data for use in a logistic regression model and will use pipelines to combine these steps together.  Finally, we\u0027ll evaluate our model\n// MAGIC  \n// MAGIC This lab also covers creating train and test datasets using `randomSplit`, visualizing a ROC curve, and generating both `ml` and `mllib` logistic regression models.\n// MAGIC  \n// MAGIC After completing this lab you should be comfortable using transformers, estimators, evaluators, and pipelines.\n\n// COMMAND ----------\n\nval baseDir \u003d \"/mnt/ml-amsterdam/\"\nval irisTwoFeatures \u003d sqlContext.read.parquet(baseDir + \"irisTwoFeatures.parquet\").cache\nirisTwoFeatures.take(2).foreach(println)\n\n// COMMAND ----------\n\ndisplay(irisTwoFeatures)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Prepare the data\n// MAGIC  \n// MAGIC To explore our data into more detail we\u0027re going to create two column where we pull out sepal length and sepal width.  These are the two features found in our `DenseVector`.\n// MAGIC  \n// MAGIC In order to do this you will write a `udf` that takes in two values.  The first will be the name of the column that we are operating on and the second is a literal for the index position.  Here are links to lit in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lit) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) APIs.\n// MAGIC  \n// MAGIC In order to call our function, we need to wrap the second value in `lit()` (e.g. `lit(1)` for the second element).  This is because our `udf` expects a `Column` and `lit` generates a `Column` where the literal is the value.\n\n// COMMAND ----------\n\n// ANSWER\nimport org.apache.spark.sql.functions.{udf, lit}\nimport org.apache.spark.mllib.linalg.Vector\n \nval getElement \u003d udf { (v: Vector, i: Int) \u003d\u003e v(i) }\n \nval irisSeparateFeatures \u003d (irisTwoFeatures\n                            .withColumn(\"sepalLength\", getElement($\"features\", lit(0)))\n                            .withColumn(\"sepalWidth\", getElement($\"features\", lit(1))))\ndisplay(irisSeparateFeatures)\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.linalg.Vector\n// TEST\nval firstRow \u003d irisSeparateFeatures.select(\"sepalWidth\", \"features\").map(r \u003d\u003e (r.getAs[Double](0), r.getAs[Vector](1))).first\nassert(firstRow._1 \u003d\u003d firstRow._2(1), \"incorrect definition for getElement\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What about using `Column`\u0027s `getItem` method?\n\n// COMMAND ----------\n\ndisplay(irisTwoFeatures.withColumn(\"sepalLength\", $\"features\".getItem(0)))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Unfortunately, it doesn\u0027t work for vectors, but it does work on arrays.\n\n// COMMAND ----------\n\nval arrayDF \u003d sqlContext.createDataFrame(Seq((Array(1,2,3), 0), (Array(3,4,5), 1))).toDF(\"anArray\", \"index\")\narrayDF.show\n \narrayDF.select($\"anArray\".getItem(0)).show()\narrayDF.select($\"anArray\"(1)).show()\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC ## Part 2\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, let\u0027s register our function and then call it directly from SQL.\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.linalg.Vector\n \nsqlContext.udf.register(\"getElement\", getElement.f.asInstanceOf[Function2[Vector, Int, Double]])\nirisTwoFeatures.registerTempTable(\"irisTwo\")\n\n// COMMAND ----------\n\n// MAGIC %sql\n// MAGIC select getElement(features, 0) as sepalLength from irisTwo\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### EDA and feature engineering\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s see what range our values have and view their means an standard deviations.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Our features both take on values from -1.0 to 1.0, but have different means and standard deviations.  How could we standardize our data to have zero mean and unit standard deviations.  For this task we\u0027ll use the `ml` estimator `StandardScaler`.  Feature transformers (which are sometimes estimators) can be found in [pyspark.ml.feature](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature) for Python or [org.apache.spark.ml.feature](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.package) for Scala.\n// MAGIC  \n// MAGIC Also, remember that the [ML Guide](http://spark.apache.org/docs/latest/ml-features.html#standardscaler) is a good place to find additional information.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.StandardScaler\n \nval standardScaler \u003d new StandardScaler()\n  .setInputCol(\"features\")\n  .setOutputCol(\"standardized\")\n  .setWithMean(true)\n \nprintln(standardScaler.explainParams() + \"\\n\\n\")\n\n// COMMAND ----------\n\nval irisStandardizedLength \u003d standardScaler\n  .fit(irisSeparateFeatures)\n  .transform(irisSeparateFeatures)\n  .withColumn(\"standardizedLength\", getElement($\"standardized\", lit(0)))\ndisplay(irisStandardizedLength)\n\n// COMMAND ----------\n\ndisplay(irisStandardizedLength.describe(\"sepalLength\", \"standardizedLength\"))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What if instead we wanted to normalize the data?  For example, we might want to normalize each set of features (per row) to have length one using an \\\\( l^2 \\\\) norm.  That would cause the sum of the features squared to be one: \\\\( \\sum_{i\u003d1}^d x_i^2 \u003d 1 \\\\).  This is could be useful if we wanted to compare observations based on a distance metric like in k-means clustering.\n// MAGIC  \n// MAGIC Normalizer can be found in [pyspark.ml.feature](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Normalizer) for Python and the [org.apache.spark.ml.feature](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.Normalizer) package for Scala.\n// MAGIC  \n// MAGIC Let\u0027s implement `Normalizer` and transform our features.  Make sure to use a `P` of 2.0 and to name the output column to \"featureNorm\".  Remember that we\u0027re working with the `irisTwoFeatures` dataset.\n\n// COMMAND ----------\n\n// ANSWER\nimport org.apache.spark.ml.feature.Normalizer\n \nval normalizer \u003d new Normalizer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"featureNorm\")\n  .setP(2.0)\n \nval irisNormalized \u003d normalizer.transform(irisTwoFeatures)  // Note that we\u0027re calling transform here\ndisplay(irisNormalized)\n\n// COMMAND ----------\n\n// TEST\nimport org.apache.spark.mllib.linalg.Vectors\nval firstVector \u003d irisNormalized.select(\"featureNorm\").map(_.getAs[Vector](0)).first\nval vectorNorm \u003d Vectors.norm(firstVector, 2.0)\n \nassert(math.round(vectorNorm) \u003d\u003d 1.0,\n       \"incorrect setup of normalizer\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC ## Part 3\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s just check and see that our norms are equal to 1.0\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n \nval l2Norm \u003d udf { v: Vector \u003d\u003e Vectors.norm(v, 2.0) }\n \nval featureLengths \u003d irisNormalized.select(l2Norm($\"features\").alias(\"featuresLength\"),\n                                           l2Norm($\"featureNorm\").alias(\"featureNormLength\"))\ndisplay(featureLengths)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, let\u0027s bucketize our features.  This will allow us to convert continuous features into discrete buckets.  This is often desirable for logistic regression which we\u0027ll be performing later in this lab.\n// MAGIC  \n// MAGIC We\u0027ll use the following splits: -infinity, -.5, 0.0, .5, +infinity.  Note that in Python infinity can be represented using `float(\u0027inf\u0027)` and that in Scala `Double.NegativeInfinity` and `Double.PositiveInfinity` can be used.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.Bucketizer\n \nval splits \u003d Array(Double.NegativeInfinity, -.5, 0.0, .5, Double.PositiveInfinity)\n \nval lengthBucketizer \u003d new Bucketizer()\n  .setInputCol(\"sepalLength\")\n  .setOutputCol(\"lengthFeatures\")\n  .setSplits(splits)\n \nval irisBucketizedLength \u003d lengthBucketizer.transform(irisSeparateFeatures)\ndisplay(irisBucketizedLength)\n\n// COMMAND ----------\n\nval widthBucketizer \u003d new Bucketizer()\n  .setInputCol(\"sepalWidth\")\n  .setOutputCol(\"widthFeatures\")\n  .setSplits(splits)\n \nval irisBucketizedWidth \u003d widthBucketizer.transform(irisBucketizedLength)\ndisplay(irisBucketizedWidth)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s combine the two bucketizers into a [Pipeline](http://spark.apache.org/docs/latest/ml-guide.html#pipeline-components) that performs both bucketizations.  A `Pipeline` is make up of stages which can be set using `setStages` and passing in a `list` of stages in Python or an `Array` of stages in `Scala`.  `Pipeline` is an estimator, which means it implements a `fit` method which returns a `PipelineModel`.  A `PipelineModel` is a transformer, which means that it implements a `transform` method which can be used to run the stages.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.Pipeline\n \nval pipelineBucketizer \u003d new Pipeline().setStages(Array(lengthBucketizer, widthBucketizer))\n \nval pipelineModelBucketizer \u003d pipelineBucketizer.fit(irisSeparateFeatures)\nval irisBucketized \u003d pipelineModelBucketizer.transform(irisSeparateFeatures)\n \ndisplay(irisBucketized)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Now that we have created two new features through bucketing, let\u0027s combined those two features into a `Vector` with `VectorAssembler`.\n// MAGIC  \n// MAGIC Set the params of `assembler` so that both \"lengthFeatures\" and \"widthFeatures\" are assembled into a column called \"featuresBucketized\".\n// MAGIC  \n// MAGIC Then, set the stages of `pipeline` to include both bucketizers and the assembler as the last stage.\n// MAGIC  \n// MAGIC Finally, use `pipeline` to generate a new `DataFrame` called `irisAssembled`.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.VectorAssembler\nval pipeline \u003d new Pipeline()\nval assembler \u003d new VectorAssembler()\n \nprintln(assembler.explainParams())\nprintln(\"\\n\" + pipeline.explainParams() + \"\\n\\n\")\n\n// COMMAND ----------\n\n// ANSWER\n// Set assembler params\nassembler\n  .setInputCols(Array(\"lengthFeatures\", \"widthFeatures\"))\n  .setOutputCol(\"featuresBucketized\")\n \npipeline.setStages(Array(lengthBucketizer, widthBucketizer, assembler))\nval irisAssembled \u003d pipeline.fit(irisSeparateFeatures).transform(irisSeparateFeatures)\ndisplay(irisAssembled)\n\n// COMMAND ----------\n\n// TEST\nimport org.apache.spark.mllib.linalg.Vectors\nval firstAssembly \u003d irisAssembled.select(\"lengthFeatures\", \"widthFeatures\", \"featuresBucketized\").first\nassert(Vectors.dense(Array(firstAssembly.getAs[Double](0), firstAssembly.getAs[Double](1))) \u003d\u003d firstAssembly(2),\n       \"incorrect value for column featuresBucketized\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC ## Part 4\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC  \n// MAGIC #### Logistic Regression\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC First let\u0027s look at our data by label.\n\n// COMMAND ----------\n\ndisplay(irisSeparateFeatures.groupBy(\"label\").count().orderBy(\"label\"))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s build a model that tries to differentiate between the first two classes.\n\n// COMMAND ----------\n\nval irisTwoClass \u003d irisSeparateFeatures.filter($\"label\" \u003c 2)\ndisplay(irisTwoClass.groupBy(\"label\").count().orderBy(\"label\"))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll split our dataset into test and train sets.\n\n// COMMAND ----------\n\nval Array(irisTest, irisTrain) \u003d irisTwoClass.randomSplit(Array(.25, .75), seed\u003d0)\n \n// Cache as we\u0027ll be using these several times\nirisTest.cache()\nirisTrain.cache()\n \nprintln(s\"Items in test datset: ${irisTest.count}\")\nprintln(s\"Items in train dataset: ${irisTrain.count}\\n\\n\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC And let\u0027s build our logistic regression model.\n// MAGIC  \n// MAGIC Make sure to set the featuresCol to \"featuresBucketized\" the regParam to 0.0 the labelCol to \"label\" and the maxIter to 1000.\n// MAGIC  \n// MAGIC Also, set the pipeline stages to include the two bucketizers, assembler, and logistic regression.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n \nval lr \u003d new LogisticRegression()\n  .setFeaturesCol(\"featuresBucketized\")\n  .setRegParam(0.0)\n  .setLabelCol(\"label\")\n  .setMaxIter(1000)\n \npipeline.setStages(Array(lengthBucketizer, widthBucketizer, assembler, lr))\n \nval pipelineModelLR \u003d pipeline.fit(irisTrain)\n \nval irisTestPredictions \u003d pipelineModelLR.transform(irisTest).cache\n \ndisplay(irisTestPredictions)\n\n// COMMAND ----------\n\n// TEST\nimport org.apache.spark.mllib.linalg.DenseVector\nassert(irisTestPredictions.select(\"probability\").first.getAs[DenseVector](0).toArray.sum \u003e .99,\n       \"incorrect build of the logistic model.\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC ## Part 5\n\n// COMMAND ----------\n\nimport scala.runtime.ScalaRunTime.stringOf\nprintln(stringOf(pipelineModelLR.stages))\nprintln(pipelineModelLR.stages.last.asInstanceOf[LogisticRegressionModel].weights)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Leaving our features to range from 0 to 3 means that a value of 2 has twice the impact in our model than a value of 1.  Since these buckets were based on increasing numeric values this is not unreasonable; however, we might want to convert each of these values to a dummy feature that takes on either a 0 or 1 corresponding to whether the value occurs.  This allows the model to measure the impact of the occurrences of the individual values and allows for non-linear relationships.\n// MAGIC  \n// MAGIC To do this we\u0027ll use the `OneHotEncoder` estimator.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.OneHotEncoder\n \nval oneHotLength \u003d new OneHotEncoder()\n  .setInputCol(\"lengthFeatures\")\n  .setOutputCol(\"lengthOneHot\")\n \npipeline.setStages(Array(lengthBucketizer, widthBucketizer, oneHotLength))\n \nval irisWithOneHotLength \u003d pipeline.fit(irisTrain).transform(irisTrain)\ndisplay(irisWithOneHotLength)\n\n// COMMAND ----------\n\nirisWithOneHotLength.select(\"lengthOneHot\").first\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Create a `OneHotEncoder` for width as well, and combine both encoders together into a `featuresBucketized` column.\n\n// COMMAND ----------\n\nval oneHotWidth \u003d new OneHotEncoder()\n  .setInputCol(\"widthFeatures\")\n  .setOutputCol(\"widthOneHot\")\n \nval assembleOneHot \u003d new VectorAssembler()\n  .setInputCols(Array(\"lengthOneHot\", \"widthOneHot\"))\n  .setOutputCol(\"featuresBucketized\")\n \npipeline.setStages(Array(lengthBucketizer, widthBucketizer, oneHotLength, oneHotWidth, assembleOneHot))\n \ndisplay(pipeline.fit(irisTrain).transform(irisTrain))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Create the full `Pipeline` through logistic regression and make predictions on the test data.\n\n// COMMAND ----------\n\npipeline.setStages(Array(lengthBucketizer, widthBucketizer, oneHotLength, oneHotWidth, assembleOneHot, lr))\n \nval pipelineModelLR2 \u003d pipeline.fit(irisTrain)\n \nval irisTestPredictions2 \u003d pipelineModelLR2\n  .transform(irisTest)\n  .cache()\ndisplay(irisTestPredictions2)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What does our new model look like?\n\n// COMMAND ----------\n\nval logisticModel \u003d pipelineModelLR2.stages.last.asInstanceOf[LogisticRegressionModel]\nprintln(logisticModel.intercept)\nprintln(stringOf(logisticModel.weights))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What about model accuracy?\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.DataFrame\n \ndef modelAccuracy(df: DataFrame) \u003d {\n  df\n  .select(($\"prediction\" \u003d\u003d\u003d $\"label\").cast(\"int\").alias(\"correct\"))\n  .groupBy()\n  .avg(\"correct\")\n  .first()(0)\n}\n \nval modelOneAccuracy \u003d modelAccuracy(irisTestPredictions)\nval modelTwoAccuracy \u003d modelAccuracy(irisTestPredictions2)\n \nprintln(s\"modelOneAccuracy: $modelOneAccuracy\")\nprintln(s\"modelTwoAccuracy: $modelTwoAccuracy\\n\\n\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Or we can use SQL instead.\n\n// COMMAND ----------\n\nirisTestPredictions.registerTempTable(\"modelOnePredictions\")\nval sqlResult \u003d sqlContext.sql(\"select avg(int(prediction \u003d\u003d label)) from modelOnePredictions\")\ndisplay(sqlResult)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC An even better option is to use the tools already built-in to Spark.  The MLlib guide has a lot of information regarding [evaluation metrics](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html).  For ML, you can find details in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation) and [Scala](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.evaluation.package) APIs.\n// MAGIC  \n// MAGIC A common metric used for logistic regression is area under the ROC curve (AUC).  We can use the `BinaryClasssificationEvaluator` to obtain the AUC for our two models.  Make sure to set the metric to \"areaUnderROC\" and that you set the rawPrediction column to \"rawPrediction\".\n// MAGIC  \n// MAGIC Recall that `irisTestPredictions` are the test predictions from our first model and `irisTestPredictions2` are the test predictions from our second model.\n\n// COMMAND ----------\n\n// ANSWER\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n \nval binaryEvaluator \u003d new BinaryClassificationEvaluator()\n  .setRawPredictionCol(\"rawPrediction\")\n  .setMetricName(\"areaUnderROC\")\n \nval firstModelTestAUC \u003d binaryEvaluator.evaluate(irisTestPredictions)\nval secondModelTestAUC \u003d binaryEvaluator.evaluate(irisTestPredictions2)\n \nprintln(s\"First model AUC: $firstModelTestAUC\")\nprintln(s\"Second model AUC: $secondModelTestAUC\")\n \nval irisTrainPredictions \u003d pipelineModelLR.transform(irisTrain)\nval irisTrainPredictions2 \u003d pipelineModelLR2.transform(irisTrain)\n \nval firstModelTrainAUC \u003d binaryEvaluator.evaluate(irisTrainPredictions)\nval secondModelTrainAUC \u003d binaryEvaluator.evaluate(irisTrainPredictions2)\n \nprintln(s\"\\nFirst model training AUC: $firstModelTrainAUC\")\nprintln(s\"Second model training AUC: $secondModelTrainAUC\\n\\n\")\n\n// COMMAND ----------\n\nassert(firstModelTestAUC \u003e .95, \"incorrect firstModelTestAUC\")\nassert(secondModelTrainAUC \u003e .95, \"incorrect secondMOdelTrainAUC\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC **Visualization: ROC curve **\n// MAGIC  \n// MAGIC We will now visualize how well the model predicts our target.  To do this we generate a plot of the ROC curve.  The ROC curve shows us the trade-off between the false positive rate and true positive rate, as we liberalize the threshold required to predict a positive outcome.  A random model is represented by the dashed line.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n \nval metric \u003d\"precision\"\n \nval multiclassEval \u003d new MulticlassClassificationEvaluator()\n \nmulticlassEval.setMetricName(metric)\nprintln(s\"Model one $metric: ${multiclassEval.evaluate(irisTestPredictions)}\")\nprintln(s\"Model two $metric: ${multiclassEval.evaluate(irisTestPredictions2)}\\n\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Using MLlib instead of ML\n// MAGIC  \n// MAGIC We\u0027ve been using `ml` transformers, estimators, pipelines, and evaluators.  How can we accomplish the same things with MLlib?\n\n// COMMAND ----------\n\nirisTestPredictions.columns\n\n// COMMAND ----------\n\nirisTestPredictions.take(1)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Pull the data that we need from our `DataFrame` and create `BinaryClassificationMetrics`.\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n \nval modelOnePredictionLabel \u003d irisTestPredictions\n  .select(\"rawPrediction\", \"label\")\n  .rdd\n  .map(r \u003d\u003e (r.getAs[DenseVector](0)(1), r.getAs[Double](1)))\n \nval modelTwoPredictionLabel \u003d irisTestPredictions2\n  .select(\"rawPrediction\", \"label\")\n  .rdd\n  .map(r \u003d\u003e (r.getAs[DenseVector](0)(1), r.getAs[Double](1)))\n \nval metricsOne \u003d new BinaryClassificationMetrics(modelOnePredictionLabel)\nval metricsTwo \u003d new BinaryClassificationMetrics(modelTwoPredictionLabel)\n \nprintln(metricsOne.areaUnderROC)\nprintln(metricsTwo.areaUnderROC)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC To build a logistic regression model with MLlib we\u0027ll need the data to be an RDD of `LabeledPoints`.  For testing purposes we\u0027ll pull out the label and features into a tuple, since we\u0027ll want to make predictions directly on the features and not on a `LabeledPoint`.\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.regression.LabeledPoint\n \nval irisTrainRDD \u003d irisTrainPredictions\n  .select(\"label\", \"featuresBucketized\")\n  .map(r \u003d\u003e LabeledPoint(r.getAs[Double](0), r.getAs[Vector](1)))\n  .cache\n \nval irisTestRDD \u003d irisTestPredictions\n  .select(\"label\", \"featuresBucketized\")\n  .map(r \u003d\u003e (r.getAs[Double](0), r.getAs[Vector](1)))\n  .cache\n \nirisTrainRDD.take(2).foreach(println)\nirisTestRDD.take(2).foreach(println)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Now, we can use MLlib\u0027s logistic regression on our `RDD` of `LabeledPoints`.  Note that we\u0027ll use `LogisticRegressionWithLBFGS` as it tends to converge faster than `LogisticRegressionWithSGD`.\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n \nval mllibModel \u003d new LogisticRegressionWithLBFGS()\n  .run(irisTrainRDD)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s calculate our accuracy using `RDDs`.\n\n// COMMAND ----------\n\nval rddPredictions \u003d mllibModel.predict(irisTestRDD.values)\nval predictAndLabels \u003d rddPredictions.zip(irisTestRDD.keys)\n \nval mllibAccuracy \u003d predictAndLabels.map( x \u003d\u003e if (x._1 \u003d\u003d x._2) 1 else 0).mean()\nprintln(s\"MLlib model accuracy: $mllibAccuracy\")\n",
      "dateUpdated": "Oct 27, 2015 10:49:14 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445942864742_801900244",
      "id": "20151027-104744_1289708129",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "baseDir: String \u003d /mnt/ml-amsterdam/\njava.lang.AssertionError: assertion failed: No predefined schema found, and no Parquet data files or summary files found under file:/mnt/ml-amsterdam/irisTwoFeatures.parquet.\n\tat scala.Predef$.assert(Predef.scala:179)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$MetadataCache$$readSchema(ParquetRelation.scala:478)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache$$anonfun$13.apply(ParquetRelation.scala:404)\n\tat scala.Option.orElse(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$MetadataCache.refresh(ParquetRelation.scala:404)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache$lzycompute(ParquetRelation.scala:145)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.org$apache$spark$sql$execution$datasources$parquet$ParquetRelation$$metadataCache(ParquetRelation.scala:143)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$6.apply(ParquetRelation.scala:196)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation.dataSchema(ParquetRelation.scala:196)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:561)\n\tat org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:560)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.\u003cinit\u003e(LogicalRelation.scala:31)\n\tat org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:395)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:267)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:102)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:107)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:111)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:113)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:115)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:117)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:119)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:121)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:123)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:125)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:127)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:129)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:131)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:133)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:135)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:137)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:139)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:141)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:143)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:145)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:147)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:149)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:151)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:153)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:155)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:157)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:159)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:161)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:163)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:165)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:167)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:169)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:171)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:173)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:175)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:177)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:179)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:181)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:183)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:185)\n\tat \u003cinit\u003e(\u003cconsole\u003e:187)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:191)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Oct 27, 2015 10:47:44 AM",
      "dateStarted": "Oct 27, 2015 10:49:14 AM",
      "dateFinished": "Oct 27, 2015 10:49:14 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445942892289_367549384",
      "id": "20151027-104812_1305587955",
      "dateCreated": "Oct 27, 2015 10:48:12 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "MLlib/04: ML Pipeline - Logistic Regression",
  "id": "2B4QMV9QS",
  "angularObjects": {
    "2AR33ZMZJ": [],
    "2AS9P7JSA": [],
    "2ARR8UZDJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}