{
  "paragraphs": [
    {
      "text": "%pyspark\n\n# Databricks notebook source exported at Tue, 27 Oct 2015 09:42:15 UTC\n# MAGIC %md\n# MAGIC # Analytics Example -- Principal Components Analysis (PCA)\n# MAGIC  \n# MAGIC In this exercise, you\u0027ll explore PCA through several visualizations, and you\u0027ll write a function that computes principal components using outer products to distribute the work of calculating the covariance matrix across the cluster.\n# MAGIC  \n# MAGIC Note that, for reference, you can look up the details of the relevant Spark methods in [Spark\u0027s Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC  \n# MAGIC ### **Part 1: Work through the steps of PCA on a sample dataset**\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **Visualization 1: Two-dimensional Gaussians**\n# MAGIC \n# MAGIC Principal Component Analysis, or PCA, is a strategy for dimensionality reduction. To better understand PCA, we\u0027ll work with synthetic data generated by sampling from the [two-dimensional Gaussian distribution](http://en.wikipedia.org/wiki/Multivariate_normal_distribution).  This distribution takes as input the mean and variance of each dimension, as well as the covariance between the two dimensions.\n# MAGIC  \n# MAGIC \n# MAGIC In our visualizations below, we will specify the mean of each dimension to be 50 and the variance along each dimension to be 1.  We will explore two different values for the covariance: 0 and 0.9. When the covariance is zero, the two dimensions are uncorrelated, and hence the data looks spherical.  In contrast, when the covariance is 0.9, the two dimensions are strongly (positively) correlated and thus the data is non-spherical.  As we\u0027ll see in Parts 1 and 2, the non-spherical data is amenable to dimensionality reduction via PCA, while the spherical data is not.\n\n# COMMAND ----------\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef preparePlot(xticks, yticks, figsize\u003d(10.5, 6), hideLabels\u003dFalse, gridColor\u003d\u0027#999999\u0027,\n                gridWidth\u003d1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax \u003d plt.subplots(figsize\u003dfigsize, facecolor\u003d\u0027white\u0027, edgecolor\u003d\u0027white\u0027)\n    ax.axes.tick_params(labelcolor\u003d\u0027#999999\u0027, labelsize\u003d\u002710\u0027)\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position(\u0027none\u0027)\n        axis.set_ticks(ticks)\n        axis.label.set_color(\u0027#999999\u0027)\n        if hideLabels: axis.set_ticklabels([])\n    plt.grid(color\u003dgridColor, linewidth\u003dgridWidth, linestyle\u003d\u0027-\u0027)\n    map(lambda position: ax.spines[position].set_visible(False), [\u0027bottom\u0027, \u0027top\u0027, \u0027left\u0027, \u0027right\u0027])\n    return fig, ax\n\ndef create2DGaussian(mn, sigma, cov, n):\n    \"\"\"Randomly sample points from a two-dimensional Gaussian distribution\"\"\"\n    np.random.seed(142)\n    return np.random.multivariate_normal(np.array([mn, mn]), np.array([[sigma, cov], [cov, sigma]]), n)\n\n# COMMAND ----------\n\ndataRandom \u003d create2DGaussian(mn\u003d50, sigma\u003d1, cov\u003d0, n\u003d100)\n\n# generate layout and plot data\nfig, ax \u003d preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\nax.set_xlabel(r\u0027Simulated $x_1$ values\u0027), ax.set_ylabel(r\u0027Simulated $x_2$ values\u0027)\nax.set_xlim(45, 54.5), ax.set_ylim(45, 54.5)\nplt.scatter(dataRandom[:,0], dataRandom[:,1], s\u003d14**2, c\u003d\u0027#d6ebf2\u0027, edgecolors\u003d\u0027#8cbfd0\u0027, alpha\u003d0.75)\ndisplay(fig) \npass\n\n# COMMAND ----------\n\ndataCorrelated \u003d create2DGaussian(mn\u003d50, sigma\u003d1, cov\u003d.9, n\u003d100)\n\n# generate layout and plot data\nfig, ax \u003d preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\nax.set_xlabel(r\u0027Simulated $x_1$ values\u0027), ax.set_ylabel(r\u0027Simulated $x_2$ values\u0027)\nax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\nplt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s\u003d14**2, c\u003d\u0027#d6ebf2\u0027,\n            edgecolors\u003d\u0027#8cbfd0\u0027, alpha\u003d0.75)\ndisplay(fig) \npass\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **(1a) Interpreting PCA**\n# MAGIC \n# MAGIC PCA can be interpreted as identifying the \"directions\" along which the data vary the most. In the first step of PCA, we must first center our data.  Working with our correlated dataset, first compute the mean of each feature (column) in the dataset.  Then for each observation, modify the features by subtracting their corresponding mean, to create a zero mean dataset.\n# MAGIC \n# MAGIC Note that `correlatedData` is an RDD of NumPy arrays.  This allows us to perform certain operations more succinctly.  For example, we can sum the columns of our dataset using `correlatedData.sum()`.\n\n# COMMAND ----------\n\n# TODO: Replace \u003cFILL IN\u003e with appropriate code\ncorrelatedData \u003d sc.parallelize(dataCorrelated)\n\nmeanCorrelated \u003d \u003cFILL IN\u003e\ncorrelatedDataZeroMean \u003d correlatedData.\u003cFILL IN\u003e\n\nprint meanCorrelated\nprint correlatedData.take(1)\nprint correlatedDataZeroMean.take(1)\n\n# COMMAND ----------\n\n# TEST Interpreting PCA (1a)\nfrom test_helper import Test\nTest.assertTrue(np.allclose(meanCorrelated, [49.95739037, 49.97180477]),\n                \u0027incorrect value for meanCorrelated\u0027)\nTest.assertTrue(np.allclose(correlatedDataZeroMean.take(1)[0], [-0.28561917, 0.10351492]),\n                \u0027incorrect value for correlatedDataZeroMean\u0027)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **(1b) Sample covariance matrix**\n# MAGIC \n# MAGIC We are now ready to compute the sample covariance matrix. If we define \\\\(\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\\\) as the zero mean data matrix, then the sample covariance matrix is defined as: \\\\[ \\mathbf{C}_{\\mathbf X} \u003d \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} \\,.\\\\]  To compute this matrix, compute the outer product of each data point, add together these outer products, and divide by the number of data points. The data are two dimensional, so the resulting covariance matrix should be a 2x2 matrix.\n# MAGIC  \n# MAGIC \n# MAGIC Note that [np.outer()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html) can be used to calculate the outer product of two NumPy arrays.\n\n# COMMAND ----------\n\n# TODO: Replace \u003cFILL IN\u003e with appropriate code\n# Compute the covariance matrix using outer products and correlatedDataZeroMean\ncorrelatedCov \u003d \u003cFILL IN\u003e\nprint correlatedCov\n\n# COMMAND ----------\n\n# TEST Sample covariance matrix (1b)\ncovResult \u003d [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\nTest.assertTrue(np.allclose(covResult, correlatedCov), \u0027incorrect value for correlatedCov\u0027)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **(1c) Covariance Function**\n# MAGIC \n# MAGIC Next, use the expressions above to write a function to compute the sample covariance matrix for an arbitrary `data` RDD.\n\n# COMMAND ----------\n\n# TODO: Replace \u003cFILL IN\u003e with appropriate code\ndef estimateCovariance(data):\n    \"\"\"Compute the covariance matrix for a given rdd.\n\n    Note:\n        The multi-dimensional covariance array should be calculated using outer products.  Don\u0027t\n        forget to normalize the data by first subtracting the mean.\n\n    Args:\n        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.\n\n    Returns:\n        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n            length of the arrays in the input `RDD`.\n    \"\"\"\n    \u003cFILL IN\u003e\n\ncorrelatedCovAuto\u003d estimateCovariance(correlatedData)\nprint correlatedCovAuto\n\n# COMMAND ----------\n\n# TEST Covariance function (1c)\ncorrectCov \u003d [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\nTest.assertTrue(np.allclose(correctCov, correlatedCovAuto),\n                \u0027incorrect value for correlatedCovAuto\u0027)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **(1d) Eigendecomposition**\n# MAGIC \n# MAGIC Now that we\u0027ve computed the sample covariance matrix, we can use it to find directions of maximal variance in the data.  Specifically, we can perform an eigendecomposition of this matrix to find its eigenvalues and eigenvectors.  The \\\\(\\scriptsize d \\\\) eigenvectors of the covariance matrix give us the directions of maximal variance, and are often called the \"principal components.\"  The associated eigenvalues are the variances in these directions.  In particular, the eigenvector corresponding to the largest eigenvalue is the direction of maximal variance (this is sometimes called the \"top\" eigenvector). Eigendecomposition of a \\\\(\\scriptsize d \\times d \\\\) covariance matrix has a (roughly) cubic runtime complexity with respect to \\\\(\\scriptsize d \\\\).  Whenever \\\\(\\scriptsize d \\\\) is relatively small (e.g., less than a few thousand) we can quickly perform this eigendecomposition locally.\n# MAGIC  \n# MAGIC \n# MAGIC Use a function from `numpy.linalg` called [eigh](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) to perform the eigendecomposition.  Next, sort the eigenvectors based on their corresponding eigenvalues (from high to low), yielding a matrix where the columns are the eigenvectors (and the first column is the top eigenvector).  Note that [np.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy-argsort) can be used to obtain the indices of the eigenvalues that correspond to the ascending order of eigenvalues.  Finally, set the `topComponent` variable equal to the top eigenvector or prinicipal component, which is a \\\\(\\scriptsize 2 \\\\)-dimensional vector (array with two values).\n# MAGIC \n# MAGIC Note that the eigenvectors returned by `eigh` appear in the columns and not the rows.  For example, the first eigenvector of `eigVecs` would be found in the first column and could be accessed using `eigVecs[:,0]`.\n\n# COMMAND ----------\n\n# TODO: Replace \u003cFILL IN\u003e with appropriate code\nfrom numpy.linalg import eigh\n\n# Calculate the eigenvalues and eigenvectors from correlatedCovAuto\neigVals, eigVecs \u003d \u003cFILL IN\u003e\nprint \u0027eigenvalues: {0}\u0027.format(eigVals)\nprint \u0027\\neigenvectors: \\n{0}\u0027.format(eigVecs)\n\n# Use np.argsort to find the top eigenvector based on the largest eigenvalue\ninds \u003d np.argsort(\u003cFILL IN\u003e)\ntopComponent \u003d \u003cFILL IN\u003e\nprint \u0027\\ntop principal component: {0}\u0027.format(topComponent)\n\n# COMMAND ----------\n\n# TEST Eigendecomposition (1d)\ndef checkBasis(vectors, correct):\n    return np.allclose(vectors, correct) or np.allclose(np.negative(vectors), correct)\nTest.assertTrue(checkBasis(topComponent, [0.68915649, 0.72461254]),\n                \u0027incorrect value for topComponent\u0027)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **(1e) PCA scores**\n# MAGIC \n# MAGIC We just computed the top principal component for a 2-dimensional non-spherical dataset.  Now let\u0027s use this principal component to derive a one-dimensional representation for the original data. To compute these compact representations, which are sometimes called PCA \"scores\", calculate the dot product between each data point in the raw data and the top principal component.\n\n# COMMAND ----------\n\n# TODO: Replace \u003cFILL IN\u003e with appropriate code\n# Use the topComponent and the data from correlatedData to generate PCA scores\ncorrelatedDataScores \u003d \u003cFILL IN\u003e\nprint \u0027one-dimensional data (first three):\\n{0}\u0027.format(np.asarray(correlatedDataScores.take(3)))\n\n# COMMAND ----------\n\n# TEST PCA Scores (1e)\nfirstThree \u003d [70.51682806, 69.30622356, 71.13588168]\nTest.assertTrue(checkBasis(correlatedDataScores.take(3), firstThree),\n                \u0027incorrect value for correlatedDataScores\u0027)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ### **Part 2: Write a PCA function and evaluate PCA on sample datasets**\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **(2a) PCA function**\n# MAGIC \n# MAGIC We now have all the ingredients to write a general PCA function.  Instead of working with just the top principal component, our function will compute the top \\\\(\\scriptsize k\\\\) principal components and principal scores for a given dataset. Write this general function `pca`, and run it with `correlatedData` and \\\\(\\scriptsize k \u003d 2\\\\). Hint: Use results from Part (1c), Part (1d), and Part (1e).\n# MAGIC  \n# MAGIC \n# MAGIC Note: As discussed in lecture, our implementation is a reasonable strategy when \\\\(\\scriptsize d \\\\) is small, though more efficient distributed algorithms exist when \\\\(\\scriptsize d \\\\) is large.\n\n# COMMAND ----------\n\n# TODO: Replace \u003cFILL IN\u003e with appropriate code\ndef pca(data, k\u003d2):\n    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n\n    Note:\n        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n\n    Args:\n        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.\n        k (int): The number of principal components to return.\n\n    Returns:\n        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n            rows equals the length of the arrays in the input `RDD` and the number of columns equals\n            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n            of length `k`.  Eigenvalues is an array of length d (the number of features).\n    \"\"\"\n    \u003cFILL IN\u003e\n    # Return the `k` principal components, `k` scores, and all eigenvalues\n    \u003cFILL IN\u003e\n\n# Run pca on correlatedData with k \u003d 2\ntopComponentsCorrelated, correlatedDataScoresAuto, eigenvaluesCorrelated \u003d \u003cFILL IN\u003e\n\n# Note that the 1st principal component is in the first column\nprint \u0027topComponentsCorrelated: \\n{0}\u0027.format(topComponentsCorrelated)\nprint (\u0027\\ncorrelatedDataScoresAuto (first three): \\n{0}\u0027\n       .format(\u0027\\n\u0027.join(map(str, correlatedDataScoresAuto.take(3)))))\nprint \u0027\\neigenvaluesCorrelated: \\n{0}\u0027.format(eigenvaluesCorrelated)\n\n# Create a higher dimensional test set\npcaTestData \u003d sc.parallelize([np.arange(x, x + 4) for x in np.arange(0, 20, 4)])\ncomponentsTest, testScores, eigenvaluesTest \u003d pca(pcaTestData, 3)\n\nprint \u0027\\npcaTestData: \\n{0}\u0027.format(np.array(pcaTestData.collect()))\nprint \u0027\\ncomponentsTest: \\n{0}\u0027.format(componentsTest)\nprint (\u0027\\ntestScores (first three): \\n{0}\u0027\n       .format(\u0027\\n\u0027.join(map(str, testScores.take(3)))))\nprint \u0027\\neigenvaluesTest: \\n{0}\u0027.format(eigenvaluesTest)\n\n# COMMAND ----------\n\n# TEST PCA Function (2a)\nTest.assertTrue(checkBasis(topComponentsCorrelated.T,\n                           [[0.68915649,  0.72461254], [-0.72461254, 0.68915649]]),\n                \u0027incorrect value for topComponentsCorrelated\u0027)\nfirstThreeCorrelated \u003d [[70.51682806, 69.30622356, 71.13588168], [1.48305648, 1.5888655, 1.86710679]]\nTest.assertTrue(np.allclose(firstThreeCorrelated,\n                            np.vstack(np.abs(correlatedDataScoresAuto.take(3))).T),\n                \u0027incorrect value for firstThreeCorrelated\u0027)\nTest.assertTrue(np.allclose(eigenvaluesCorrelated, [1.94345403, 0.13820481]),\n                           \u0027incorrect values for eigenvaluesCorrelated\u0027)\ntopComponentsCorrelatedK1, correlatedDataScoresK1, eigenvaluesCorrelatedK1 \u003d pca(correlatedData, 1)\nTest.assertTrue(checkBasis(topComponentsCorrelatedK1.T, [0.68915649,  0.72461254]),\n               \u0027incorrect value for components when k\u003d1\u0027)\nTest.assertTrue(np.allclose([70.51682806, 69.30622356, 71.13588168],\n                            np.vstack(np.abs(correlatedDataScoresK1.take(3))).T),\n                \u0027incorrect value for scores when k\u003d1\u0027)\nTest.assertTrue(np.allclose(eigenvaluesCorrelatedK1, [1.94345403, 0.13820481]),\n                           \u0027incorrect values for eigenvalues when k\u003d1\u0027)\nTest.assertTrue(checkBasis(componentsTest.T[0], [ .5, .5, .5, .5]),\n                \u0027incorrect value for componentsTest\u0027)\nTest.assertTrue(np.allclose(np.abs(testScores.first()[0]), 3.),\n                \u0027incorrect value for testScores\u0027)\nTest.assertTrue(np.allclose(eigenvaluesTest, [ 128, 0, 0, 0 ]), \u0027incorrect value for eigenvaluesTest\u0027)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **(2b) PCA on `dataRandom`**\n# MAGIC \n# MAGIC Next, use the PCA function we just developed to find the top two principal components of the spherical `dataRandom` we created in Visualization 1.\n\n# COMMAND ----------\n\n# TODO: Replace \u003cFILL IN\u003e with appropriate code\nrandomData \u003d sc.parallelize(dataRandom)\n\n# Use pca on randomData\ntopComponentsRandom, randomDataScoresAuto, eigenvaluesRandom \u003d \u003cFILL IN\u003e\n\nprint \u0027topComponentsRandom: \\n{0}\u0027.format(topComponentsRandom)\nprint (\u0027\\nrandomDataScoresAuto (first three): \\n{0}\u0027\n       .format(\u0027\\n\u0027.join(map(str, randomDataScoresAuto.take(3)))))\nprint \u0027\\neigenvaluesRandom: \\n{0}\u0027.format(eigenvaluesRandom)\n\n# COMMAND ----------\n\n# TEST PCA on `dataRandom` (2b)\nTest.assertTrue(checkBasis(topComponentsRandom.T,\n                           [[-0.2522559 ,  0.96766056], [-0.96766056,  -0.2522559]]),\n                \u0027incorrect value for topComponentsRandom\u0027)\nfirstThreeRandom \u003d [[36.61068572,  35.97314295,  35.59836628],\n                    [61.3489929 ,  62.08813671,  60.61390415]]\nTest.assertTrue(np.allclose(firstThreeRandom, np.vstack(np.abs(randomDataScoresAuto.take(3))).T),\n                \u0027incorrect value for randomDataScoresAuto\u0027)\nTest.assertTrue(np.allclose(eigenvaluesRandom, [1.4204546, 0.99521397]),\n                            \u0027incorrect value for eigenvaluesRandom\u0027)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC **Visualization 2: PCA projection**\n# MAGIC \n# MAGIC Plot the original data and the 1-dimensional reconstruction using the top principal component to see how the PCA solution looks.  The original data is plotted as before; however, the 1-dimensional reconstruction (projection) is plotted in green on top of the original data and the vectors (lines) representing the two principal components are shown as dotted lines.\n\n# COMMAND ----------\n\ndef projectPointsAndGetLines(data, components, xRange):\n    \"\"\"Project original data onto first component and get line details for top two components.\"\"\"\n    topComponent\u003d components[:, 0]\n    slope1, slope2 \u003d components[1, :2] / components[0, :2]\n\n    means \u003d data.mean()[:2]\n    demeaned \u003d data.map(lambda v: v - means)\n    projected \u003d demeaned.map(lambda v: (v.dot(topComponent) /\n                                        topComponent.dot(topComponent)) * topComponent)\n    remeaned \u003d projected.map(lambda v: v + means)\n    x1,x2 \u003d zip(*remeaned.collect())\n\n    lineStartP1X1, lineStartP1X2 \u003d means - np.asarray([xRange, xRange * slope1])\n    lineEndP1X1, lineEndP1X2 \u003d means + np.asarray([xRange, xRange * slope1])\n    lineStartP2X1, lineStartP2X2 \u003d means - np.asarray([xRange, xRange * slope2])\n    lineEndP2X1, lineEndP2X2 \u003d means + np.asarray([xRange, xRange * slope2])\n\n    return ((x1, x2), ([lineStartP1X1, lineEndP1X1], [lineStartP1X2, lineEndP1X2]),\n            ([lineStartP2X1, lineEndP2X1], [lineStartP2X2, lineEndP2X2]))\n\n# COMMAND ----------\n\n((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) \u003d \\\n    projectPointsAndGetLines(correlatedData, topComponentsCorrelated, 5)\n\n# generate layout and plot data\nfig, ax \u003d preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize\u003d(7, 7))\nax.set_xlabel(r\u0027Simulated $x_1$ values\u0027), ax.set_ylabel(r\u0027Simulated $x_2$ values\u0027)\nax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\nplt.plot(line1X1, line1X2, linewidth\u003d3.0, c\u003d\u0027#8cbfd0\u0027, linestyle\u003d\u0027--\u0027)\nplt.plot(line2X1, line2X2, linewidth\u003d3.0, c\u003d\u0027#d6ebf2\u0027, linestyle\u003d\u0027--\u0027)\nplt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s\u003d14**2, c\u003d\u0027#d6ebf2\u0027,\n            edgecolors\u003d\u0027#8cbfd0\u0027, alpha\u003d0.75)\nplt.scatter(x1, x2, s\u003d14**2, c\u003d\u0027#62c162\u0027, alpha\u003d.75)\ndisplay(fig) \npass\n\n# COMMAND ----------\n\n((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) \u003d \\\n    projectPointsAndGetLines(randomData, topComponentsRandom, 5)\n\n# generate layout and plot data\nfig, ax \u003d preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize\u003d(7, 7))\nax.set_xlabel(r\u0027Simulated $x_1$ values\u0027), ax.set_ylabel(r\u0027Simulated $x_2$ values\u0027)\nax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\nplt.plot(line1X1, line1X2, linewidth\u003d3.0, c\u003d\u0027#8cbfd0\u0027, linestyle\u003d\u0027--\u0027)\nplt.plot(line2X1, line2X2, linewidth\u003d3.0, c\u003d\u0027#d6ebf2\u0027, linestyle\u003d\u0027--\u0027)\nplt.scatter(dataRandom[:,0], dataRandom[:,1], s\u003d14**2, c\u003d\u0027#d6ebf2\u0027,\n            edgecolors\u003d\u0027#8cbfd0\u0027, alpha\u003d0.75)\nplt.scatter(x1, x2, s\u003d14**2, c\u003d\u0027#62c162\u0027, alpha\u003d.75)\ndisplay(fig) \npass\n\n# COMMAND ----------\n\n\n",
      "dateUpdated": "Oct 28, 2015 5:56:17 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446054964150_1707919702",
      "id": "20151028-175604_60161231",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 160, in \u003cmodule\u003e\n    compiledCode \u003d compile(final_code, \"\u003cstring\u003e\", \"exec\")\n  File \"\u003cstring\u003e\", line 37\n    meanCorrelated \u003d \u003cFILL IN\u003e\n                     ^\nSyntaxError: invalid syntax\n"
      },
      "dateCreated": "Oct 28, 2015 5:56:04 PM",
      "dateStarted": "Oct 28, 2015 5:56:17 PM",
      "dateFinished": "Oct 28, 2015 5:56:33 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446054977204_-361259882",
      "id": "20151028-175617_156438461",
      "dateCreated": "Oct 28, 2015 5:56:17 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "NumPy/01: Principle Component Analysis",
  "id": "2B58RQM7D",
  "angularObjects": {
    "2AR33ZMZJ": [],
    "2AS9P7JSA": [],
    "2ARR8UZDJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}