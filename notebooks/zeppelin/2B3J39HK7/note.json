{
  "paragraphs": [
    {
      "text": "// Databricks notebook source exported at Tue, 27 Oct 2015 09:45:18 UTC\n// MAGIC %md\n// MAGIC # Wikipedia: LDA\n// MAGIC  \n// MAGIC This lab explores building a Latent Dirichlet allocation (LDA) model.  We\u0027ll use LDA to generate 10 topics that correspond to the Wikipedia data.  These topics will correspond to words found in the articles.  We\u0027ll take an article and see which of the topics it is associated with.  LDA won\u0027t just categories the article into one category but will give a numeric value that corresponds to the articles relevance to each of the 10 topics.\n// MAGIC  \n// MAGIC LDA is currently only implemented in MLlib.  Details can be found in the [MLlib clustering guide](http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda).\n// MAGIC  \n// MAGIC Additional details about the algorithm can be found on [Wikipedia](http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda).\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Load in the data.\n// MAGIC  \n// MAGIC First, we\u0027ll load the `DataFrame` which was called `noStopWords` in the wiki-etl-eda notebook.  This way we can avoid some of the pre-processing steps.\n\n// COMMAND ----------\n\nval noStopWords \u003d sqlContext.read.parquet(\"/mnt/ml-amsterdam/oneWords.parquet\").cache // noStopWords from wiki-etl-eda\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Recall that even though we removed a series of stop words, the word counts for certain works were very high and they were words that didn\u0027t seem to convey much meaning for an article.  We\u0027ll view words ordered by their frequency to find a cutoff point for removing additional stop words.\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.functions.{explode, count, desc}\nval wordCounts \u003d noStopWords\n  .select(explode($\"words\").as(\"word\"))\n  .groupBy(\"word\")\n  .agg(count($\"word\").alias(\"counts\"))\n  .orderBy(desc(\"counts\"))\ndisplay(wordCounts)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Create a new list of stop words based on our cutoff.\n\n// COMMAND ----------\n\nval newStopWords \u003d wordCounts.filter($\"counts\" \u003e 55700).select(\"word\").map(_.getAs[String](0)).collect() // 3000 small, 55700 one, all 5500000\nprintln(newStopWords)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Use `StopWordsRemover` to remove our new list of stop words from the dataset.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.StopWordsRemover\nimport org.apache.spark.sql.functions.size\n \nval sw \u003d new StopWordsRemover()\n  .setInputCol(\"words\")\n  .setOutputCol(\"fewerWords\")\n  .setStopWords(newStopWords)\n \nval updatedStopWords \u003d sw.transform(noStopWords).filter(size($\"fewerWords\") \u003e 0)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, review a few examples to see if we have improved our word list.\n\n// COMMAND ----------\n\ndisplay(updatedStopWords.select(\"fewerWords\"))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Then, we\u0027ll use `CountVectorizer` to obtain word counts per article.  Note that these will be stored as `SparseVectors` within our `DataFrame`.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.CountVectorizer\nval cv \u003d new CountVectorizer()\nprintln(cv.explainParams)\n\n// COMMAND ----------\n\ncv\n  .setInputCol(\"fewerWords\")\n  .setOutputCol(\"counts\")\n \nval cvModel \u003d cv.fit(updatedStopWords)\nval wordCounts \u003d cvModel.transform(updatedStopWords)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We\u0027ll save our vocabulary so that we can skip some steps if we want to use the LDA model later.\n\n// COMMAND ----------\n\nval cvVocabRDD \u003d sc.parallelize(cvModel.vocabulary)\n//cvVocabRDD.saveAsTextFile(\"/mnt/ml-amsterdam/oneCVVocab.txt\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC This is how we can reload in the vocabulary from a text file.\n\n// COMMAND ----------\n\nval cvVocabRDD \u003d sc.textFile(\"/mnt/ml-amsterdam/oneCVVocab.txt\")\ncvVocabRDD.take(3)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Recall that `CountVectorizer` returns a `SparseVector`.\n\n// COMMAND ----------\n\ndisplay(wordCounts.select(\"counts\"))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s go ahead and build that LDA model.  Since `LDA` falls within `mllib` it doesn\u0027t take in a `DataFrame`.  We need to provide an `RDD`.  `LDA` expects an `RDD` that contains a tuple of (index, `Vector`) pairs.  Under the hood LDA uses GraphX which performs shuffles which change the order of the data, so the usual `mllib` strategy of zipping the results together will not work.  With LDA we\u0027ll use joins based on the articles indices.\n\n// COMMAND ----------\n\n// 4-5 mins to run w/ 3 nodes\nimport org.apache.spark.mllib.clustering.LDA\nimport org.apache.spark.mllib.linalg.Vector\n \nval corpus \u003d wordCounts\n  .select(\"counts\", \"title\")\n  .map(r \u003d\u003e (r.getAs[Vector](0), r.getAs[String](1)))\n  .zipWithIndex\n  .map(_.swap)\n  .cache\n \nval ldaModel \u003d new LDA()\n  .setK(10)\n  .run(corpus.mapValues(_._1))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC The below command was used to save the LDA model for later use.\n\n// COMMAND ----------\n\n//ldaModel.save(sc, \"/mnt/ml-amsterdam/oneLDA.model\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC This is how we load back in the saved model.\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.clustering.DistributedLDAModel\nval ldaModel \u003d DistributedLDAModel.load(sc, \"/mnt/ml-amsterdam/oneLDA.model\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What\u0027s stored in an LDA model?\n\n// COMMAND ----------\n\ndisplay(dbutils.fs.ls(\"/mnt/ml-amsterdam/oneLDA.model/\"))\n\n// COMMAND ----------\n\ndisplay(dbutils.fs.ls(\"/mnt/ml-amsterdam/oneLDA.model/data/\"))\n\n// COMMAND ----------\n\ndisplay(dbutils.fs.ls(\"/mnt/ml-amsterdam/oneLDA.model/data/tokenCounts/\"))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s view the first three words that are most relevant for our 10 topics.  The first array references work indices and is their relevance to this topic.\n\n// COMMAND ----------\n\nimport runtime.ScalaRunTime.stringOf\nldaModel.describeTopics(3).foreach(l \u003d\u003e println(stringOf(l)))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s use our `CountVectorizer` vocabulary to generate more readable topics.  Note that this makes use of LDA\u0027s `describeTopics`.\n\n// COMMAND ----------\n\nval indexToWord \u003d cvVocabRDD.zipWithIndex.map(_.swap).collectAsMap\n \ndef describeTopicsWithWords(num: Int) \u003d {\n  val topicIndex \u003d ldaModel.describeTopics(num)\n  val withWords \u003d topicIndex.map(topic \u003d\u003e topic._1.map(indexToWord(_)))\n  withWords\n}\n \nval topicCategories \u003d describeTopicsWithWords(10).map(_.mkString(\"-\"))\ntopicCategories.foreach(println)\nprintln(\"\\n\\n\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Now let\u0027s view the top documents for our 10 topics.  Note that this makes use of `topDocumentsPerTopic` but joins in titles to make the results more readable.  `topDocumentsPerTopic` is available for `DistributedLDAModels`.\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.clustering.DistributedLDAModel\n \ndef topDocumentsWithTitle(num: Int) \u003d {\n  val topDocs \u003d ldaModel.asInstanceOf[DistributedLDAModel].topDocumentsPerTopic(num)\n \n  topDocs\n    .map(topic \u003d\u003e {\n      val ids \u003d topic._1\n      val idsRDD \u003d sc.parallelize(ids.zipWithIndex)\n      val joined \u003d idsRDD.join(corpus.mapValues(_._2)).collectAsMap\n      ids.map(joined(_))\n    })\n}\n \ntopDocumentsWithTitle(7).zip(topicCategories).foreach(x \u003d\u003e {\n  println(s\"Next Topic: ${x._2}\")\n  x._1.foreach(println)\n  println()\n})\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC How many articles are we working with?\n\n// COMMAND ----------\n\ncorpus.count\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll search for an article based on a keyword and then see how that article is classified into topics.  The below example searches for european football related articles.\n\n// COMMAND ----------\n\nval idTitleDF \u003d corpus.map(x \u003d\u003e (x._1, x._2._2)).toDF(\"id\", \"title\")\ndisplay(idTitleDF.select(\"title\", \"id\").where($\"title\".like(\"%UEFA%\")))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC The article about Belgium seems interesting.  Let\u0027s use id 2880 and see what topics are ranked as most relevant.  Note that this uses `topicDistributions` which is a `DistributedLDAModel` method.\n\n// COMMAND ----------\n\nval topicDist \u003d ldaModel.asInstanceOf[DistributedLDAModel].topicDistributions\n \ndef getTopicDistForID(id: Int) {\n  println(\"Score\\tTopic\")\n  topicDist\n    .filter(_._1 \u003d\u003d id)\n    .map(_._2.toArray)\n    .first // now we are working locally\n    .zip(topicCategories)\n    .sortWith(_._1 \u003e _._1)\n    .foreach(x \u003d\u003e println(f\"${x._1}%.3f\\t${x._2}\"))\n}\n \ngetTopicDistForID(2880) //2880\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We\u0027ll register our `DataFrame` with ids and titles, so that we can query it from SQL.\n\n// COMMAND ----------\n\nidTitleDF.registerTempTable(\"idTitle\")\n\n// COMMAND ----------\n\n// MAGIC %sql\n// MAGIC select title, id from idTitle where title like \"%$Query%\"\n\n// COMMAND ----------\n\n//women%cycling\ngetTopicDistForID(27441) //27441\n",
      "dateUpdated": "Oct 27, 2015 10:59:19 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445943524644_662200287",
      "id": "20151027-105844_711621331",
      "dateCreated": "Oct 27, 2015 10:58:44 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Wiki NLP/05: LDA Topics",
  "id": "2B3J39HK7",
  "angularObjects": {
    "2AR33ZMZJ": [],
    "2AS9P7JSA": [],
    "2ARR8UZDJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}