{
  "paragraphs": [
    {
      "text": "// Databricks notebook source exported at Tue, 27 Oct 2015 09:44:53 UTC\n// MAGIC %md\n// MAGIC # Wikipedia: Exploratory Data Analysis (EDA) using DataFrames\n// MAGIC  \n// MAGIC This lab explores English wikipedia articles using `DataFrames`.  You\u0027ll learn about `DataFrame`, `Column`, and `GroupedData` objects and the `functions` package.  After you complete this lab you should be able to use much of the functionality found in Spark SQL and know where to find additional reference material.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Load the data and start the EDA\n// MAGIC  \n// MAGIC We\u0027ll be mostly using functions and objects that are found in Spark SQL.  The [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package) APIs and the [Spark SQL and DataFrame Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html) are all very useful references.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC To start, work from the small sample to speed the EDA process.\n\n// COMMAND ----------\n\nval baseDir \u003d \"/root/pipeline/datasets/misc/\"\nval dfSmall \u003d sqlContext.read.parquet(baseDir + \"smallwiki.parquet\").cache\nprintln(dfSmall.count)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s take a look at how our `DataFrame` is represented.\n\n// COMMAND ----------\n\nprintln(s\"dfSmall: $dfSmall\")\nprintln(s\"dfSmall.getClass: ${dfSmall.getClass}\")\n\n// COMMAND ----------\n\nprintln(dfSmall.schema)\ndfSmall.printSchema\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We can see the our schema is make up of a `StructType` that contains `StructField` objects.  These `StructField` objects have several properties including: a name, data type, whether they can be null, and metadata.  Note that The list of fields for a `StructType` can also include other `StructType` objects to allow for nested structures.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll create an example `DataFrame` where we specify the schema using `StructType` and `StructField`.  Schema can also be inferred by Spark during `DataFrame` creation.\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, BooleanType, StringType, LongType, MetadataBuilder}\n \nval titleMetadata \u003d new MetadataBuilder()\n  .putString(\"language\", \"english\")\n  .build()\nval schema \u003d StructType(Array(StructField(\"title\", StringType, false, titleMetadata),\n                     StructField(\"numberOfEdits\", LongType),\n                     StructField(\"redacted\", BooleanType)))\n \nval exampleData \u003d sc.parallelize(Seq(Row(\"Baade\u0027s Window\", 100L, false),\n                                     Row(\"Zenomia\", 10L, true),\n                                     Row(\"United States Bureau of Mines\", 5280L, true)))\n \nval exampleDF \u003d sqlContext.createDataFrame(exampleData, schema)\ndisplay(exampleDF)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s view the schema that we created.\n\n// COMMAND ----------\n\nprintln(exampleDF.schema)\nexampleDF.printSchema\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Our `metadata` for the `title` field has also been captured.  We might create a new `DataFrame` from this `DataFrame` using a transformer and we could pass along or modify this `metadata` in the process.\n\n// COMMAND ----------\n\nprintln(exampleDF.schema.fields(0).metadata)\nprintln(exampleDF.schema.fields(1).metadata)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What does a row of wikipedia data look like?  Let\u0027s take a look at the first observation.\n\n// COMMAND ----------\n\nprintln(dfSmall.first)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What are our column names?\n\n// COMMAND ----------\n\ndfSmall.columns\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC The text is long and obscures the rest of the data.  Let\u0027s use `drop` to remove the text.\n\n// COMMAND ----------\n\nprintln(dfSmall.drop(\"text\").first)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, let\u0027s view the text in a format that more closely resembles how it would be displayed.\n\n// COMMAND ----------\n\nprintln(dfSmall.select(\"text\").first()(0))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC When we parsed the XML we stored `\u003cPARSE ERROR\u003e` as the title for any record that our XML parser couldn\u0027t handle.  Let\u0027s see how many records had errors.\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.functions.col\nval errors \u003d dfSmall.filter(col(\"title\") \u003d\u003d\u003d \"\u003cPARSE ERROR\u003e\")\nval errorCount \u003d errors.count()\nprintln(errorCount)\nprintln(errorCount.toDouble / dfSmall.count)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We can also do the `Column` selection several different ways.\n\n// COMMAND ----------\n\nprintln(dfSmall.filter(dfSmall(\"title\") \u003d\u003d\u003d \"\u003cPARSE ERROR\u003e\").count)\nprintln(dfSmall.filter($\"title\" \u003d\u003d\u003d \"\u003cPARSE ERROR\u003e\").count)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We can see that `errors` contains those items with a title that equals `\u003cPARSE ERROR\u003e`.  Note that we can rename our column using `.alias()` and display our `DataFrame` using `.show()`.  `alias` is a method that we are calling on a `Column` and `show` is a method called on the `DataFrame`.\n\n// COMMAND ----------\n\n// We could also us as instead of alias\nerrors.select($\"title\".alias(\"badTitle\")).show(3)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC And what does an error look like?\n\n// COMMAND ----------\n\nprintln(errors.select(\"text\").first()(0))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s use some `Column` and `DataFrame` operations to inspect the `redirect_title` column.\n\n// COMMAND ----------\n\ndfSmall\n  .select($\"redirect_title\".isNotNull.as(\"hasRedirect\"))\n  .groupBy(\"hasRedirect\")\n  .count\n  .show\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Now, let\u0027s filter out the data that has a parse error, is a redirect, or doesn\u0027t have any text.\n\n// COMMAND ----------\n\nval filtered \u003d dfSmall.filter(($\"title\" !\u003d\u003d \"\u003cPARSE ERROR\u003e\") \u0026\u0026\n                              $\"redirect_title\".isNull \u0026\u0026\n                              $\"text\".isNotNull)\nprintln(filtered.count)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Helpful functions\n// MAGIC  \n// MAGIC In addition to the functions that can be called on a `DataFrame`, `Column`, or `GroupedData`, Spark SQL also has a `functions` package that provides functions like those typically built into a database system that can be called from SQL.  This include functions for performing math, handling dates and times, string manipulation, and more.\n// MAGIC  \n// MAGIC The [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) APIs have good descriptions for these functions.\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.{functions \u003d\u003e func}\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll use the time functions to convert our timestamp into Central European Summer Time (CEST).\n\n// COMMAND ----------\n\nfiltered.select(\"timestamp\").show(5)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s try applying `date_format` to see how it operates.\n\n// COMMAND ----------\n\nfiltered\n  .select($\"timestamp\", func.date_format($\"timestamp\", \"MM/dd/yyyy\").as(\"date\"))\n  .show(5)\n\n// COMMAND ----------\n\nval withDate \u003d filtered.withColumn(\"date\", func.date_format($\"timestamp\", \"MM/dd/yyyy\"))\nwithDate.printSchema\n \nwithDate\n  .select(\"title\", \"timestamp\", \"date\")\n  .show(3)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC It seems like we want a different function for time zone manipulation and to store the object as a timestamp rather than a string.  Let\u0027s use `from_utc_timestamp` to get a timestamp object back with the correct time zone.\n\n// COMMAND ----------\n\nval withCEST \u003d withDate.withColumn(\"cest_time\", func.from_utc_timestamp($\"timestamp\", \"Europe/Amsterdam\"))\nwithCEST.printSchema\n \nwithCEST\n  .select(\"timestamp\", \"cest_time\")\n  .show(3, false)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, let\u0027s convert the text field to lowercase.  We\u0027ll use the `lower` function for this.\n\n// COMMAND ----------\n\nval lowered \u003d withCEST.select($\"*\", func.lower($\"text\").as(\"lowerText\"))\n \nprintln(lowered.select(\"lowerText\").first)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What columns do we have now?\n\n// COMMAND ----------\n\nlowered.columns\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s go ahead and drop the columns we don\u0027t want and rename `lowerText` to `text`.\n\n// COMMAND ----------\n\nval parsed \u003d lowered\n  .drop(\"text\")\n  .drop(\"timestamp\")\n  .drop(\"date\")\n  .withColumnRenamed(\"lowerText\", \"text\")\n \nprintln(\"Columns:\")\nparsed.columns.foreach(println)\n \nprintln(\"\\n\\n\" + parsed.select(\"text\").first)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, let\u0027s convert our text into a list of words so that we can perform some analysis at the word level.  For this will use a feature transformer called `RegexTokenizer` which splits up strings into tokens (words in our case) based on a split pattern.  We\u0027ll split our text on anything that matches one or more non-word character.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.RegexTokenizer\n \nval tokenizer \u003d new RegexTokenizer()\n  .setInputCol(\"text\")\n  .setOutputCol(\"words\")\n  .setPattern(\"\\\\W+\")\nval wordsDF \u003d tokenizer.transform(parsed)\n\n// COMMAND ----------\n\nwordsDF.select(\"words\").first\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC There are some very common words in our list of words which won\u0027t be that useful for our later analysis.  We\u0027ll create a UDF to remove them.\n// MAGIC  \n// MAGIC [StopWordsRemover](http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover) is implemented for Scala but not yet for Python.  We\u0027ll use the same [list](http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words) of stop words it uses to build a user-defined function (UDF).\n\n// COMMAND ----------\n\nval stopWords \u003d sc.textFile(\"/mnt/ml-amsterdam/stop_words.txt\").collect.toSet\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Create our function for removing words.\n\n// COMMAND ----------\n\nimport scala.collection.mutable.WrappedArray\n \nval stopWordsBroadcast \u003d sc.broadcast(stopWords)\n \ndef isDigitOrUnderscore(c: Char) \u003d {\n    Character.isDigit(c) || c \u003d\u003d \u0027_\u0027\n}\n \ndef keepWord(word: String) \u003d word match {\n    case x if x.length \u003c 3 \u003d\u003e false\n    case x if stopWordsBroadcast.value(x) \u003d\u003e false\n    case x if x exists isDigitOrUnderscore \u003d\u003e false\n    case _ \u003d\u003e true\n}\n \ndef removeWords(words: WrappedArray[String]) \u003d {\n    words.filter(keepWord(_))\n}\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Test the function locally.\n\n// COMMAND ----------\n\nremoveWords(Array(\"test\", \"cat\", \"do343\", \"343\", \"spark\", \"the\", \"and\", \"hy-phen\", \"under_score\"))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Create a UDF from our function.\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.functions.udf\nval removeWordsUDF \u003d udf { removeWords _ }\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Register this function so that we can call it later from another notebook.  Note that in Scala `register` also returns a `udf` that we can use, so we could have combined the above step into this step.\n\n// COMMAND ----------\n\nsqlContext.udf.register(\"removeWords\", removeWords _)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Apply our function to the `wordsDF` `DataFrame`.\n\n// COMMAND ----------\n\nval noStopWords \u003d wordsDF\n  .withColumn(\"noStopWords\", removeWordsUDF($\"words\"))\n  .drop(\"words\")\n  .withColumnRenamed(\"noStopWords\", \"words\")\n \nnoStopWords.select(\"words\").take(2)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We can save our work at this point by writing out a parquet file.\n\n// COMMAND ----------\n\n//noStopWords.write.parquet(\"/mnt/ml-amsterdam/smallWords.parquet\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What is the `DataFrame` doing in the background?\n\n// COMMAND ----------\n\nprintln(noStopWords.explain(true))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s cache `noStopWords` as we\u0027ll use it multiple times shortly.\n\n// COMMAND ----------\n\nnoStopWords.cache\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Calculate the number of words in `noStopWords`.  Recall that each row contains an array of words.\n// MAGIC  \n// MAGIC One strategy would be to take the length of each row and sum the lengths.  To do this use `functions.size`, `functions.sum`, and call `.agg` on the `DataFrame`.\n// MAGIC  \n// MAGIC Don\u0027t forget to make use of the  [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package) APIs.  For example you\u0027ll find detail for the function `size` in the [functions module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.size) in Python and the [functions package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) in Scala.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC First, create a `DataFrame` named sized that has a `size` column with the size of each array of words.  Here you can use `func.size`.\n\n// COMMAND ----------\n\n// ANSWER\nval sized \u003d noStopWords.withColumn(\"size\", func.size($\"words\"))\n \nval sizedFirst \u003d sized.select(\"size\", \"words\").first()\nprintln(sizedFirst(0))\n\n// COMMAND ----------\n\nsizedFirst(1)\n\n// COMMAND ----------\n\n// TEST\nassert(sizedFirst(0) \u003d\u003d sizedFirst.getAs[IndexedSeq[String]](1).size, \"incorrect implementation for sized\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, you\u0027ll need to aggregate the counts.  You can do this using `func.sum` in either a `.select` or `.agg` method call on the `DataFrame`.  Make sure to give your `Column` the alias `numberOfWords`.  There are some examples in [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) in the APIs.\n\n// COMMAND ----------\n\n// ANSWER\nval numberOfWords \u003d sized.agg(func.sum(\"size\").as(\"numberOfWords\"))\n \nval wordCount \u003d numberOfWords.first()(0)\nprintln(wordCount)\n\n// COMMAND ----------\n\n// TEST\nassert(wordCount \u003d\u003d 1903220, \"incorrect word count\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll compute the word count using `select` the function `func.explode()` and then taking a `count()` on the `DataFrame`.  Make sure to name the column returned by `explode` to `word`.\n\n// COMMAND ----------\n\n// ANSWER\nval wordList \u003d noStopWords.select(func.explode($\"words\").as(\"word\"))\n \n// Note that we have one word per Row now\nwordList.take(3).foreach(println)\nval wordListCount \u003d wordList.count()\nprintln(wordListCount)\n\n// COMMAND ----------\n\nassert(wordListCount \u003d\u003d 1903220, \"incorrect value for wordListCount\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC For your final task, you\u0027ll group by word and count the number of times each word occurs.  Make sure to return the counts in descending order and to call them `counts`.\n// MAGIC  \n// MAGIC For this task, you can use:\n// MAGIC  * `DataFrame` operations `groupBy`, `agg`, and `sort`\n// MAGIC  * the `Column` operation `alias`\n// MAGIC  * functions `func.count` and `func.desc`.\n\n// COMMAND ----------\n\n// ANSWER\nval wordGroupCount \u003d wordList\n  .groupBy(\"word\")  // group\n  .agg(func.count(\"word\").as(\"counts\"))  // aggregate\n  .sort(func.desc(\"counts\"))  // sort\n \nwordGroupCount.take(5).foreach(println)\n\n// COMMAND ----------\n\n// TEST\nassert(wordGroupCount.first() \u003d\u003d Row(\"ref\", 29263), \"incorrect counts.\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We could also use SQL to accomplish this counting.\n\n// COMMAND ----------\n\nwordList.registerTempTable(\"wordList\")\n\n// COMMAND ----------\n\nval wordGroupCount2 \u003d sqlContext.sql(\"select word, count(word) as counts from wordList group by word order by counts desc\")\nwordGroupCount2.take(5).foreach(println)\n\n// COMMAND ----------\n\n// MAGIC %sql\n// MAGIC select word, count(word) as counts from wordList group by word order by counts desc\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Finally, let\u0027s see how many distinct words we are working with.\n\n// COMMAND ----------\n\nval distinctWords \u003d wordList.distinct\ndistinctWords.take(5).foreach(println)\n\n// COMMAND ----------\n\ndistinctWords.count\n",
      "dateUpdated": "Oct 27, 2015 1:57:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445943441462_343103887",
      "id": "20151027-105721_1811224068",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "File name too long"
      },
      "dateCreated": "Oct 27, 2015 10:57:21 AM",
      "dateStarted": "Oct 27, 2015 1:57:13 PM",
      "dateFinished": "Oct 27, 2015 1:57:13 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445943459294_1257883530",
      "id": "20151027-105739_231723946",
      "dateCreated": "Oct 27, 2015 10:57:39 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Wiki NLP/02: Feature Engineering",
  "id": "2B4HTCAQY",
  "angularObjects": {
    "2AR33ZMZJ": [],
    "2AS9P7JSA": [],
    "2ARR8UZDJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}