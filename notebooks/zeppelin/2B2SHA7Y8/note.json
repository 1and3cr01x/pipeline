{
  "paragraphs": [
    {
      "text": "%dep\nz.reset()\nz.addRepo(\"maven central\").url(\"search.maven.org\")\nz.load(\"com.datastax.spark:spark-cassandra-connector_2.10:1.4.0\")\nz.load(\"org.elasticsearch:elasticsearch-spark_2.10:2.1.2\")\nz.load(\"com.databricks:spark-csv_2.10:1.2.0\")\nz.load(\"org.apache.spark:spark-streaming-kafka-assembly_2.10:1.5.1\")\nz.load(\"redis.clients:jedis:2.7.3\")\nz.load(\"/root/zeppelin-0.6.0-spark-1.5.1-hadoop-2.6.0-fluxcapacitor/lib/mysql-connector-java.jar\")\nz.load(\"/root/pipeline/myapps/datasource/target/scala-2.10/datasource_2.10-1.0.jar\")\n//z.load(\"edu.berkeley.cs.amplab:keystoneml_2.10:0.2\")\nz.load(\"edu.stanford.nlp:stanford-corenlp:3.5.2\")\nz.load(\"/root/pipeline/myapps/nlp/target/scala-2.10/nlp_2.10-1.0.jar\")\nz.load(\"/root/pipeline/myapps/nlp/lib/stanford-corenlp-3.5.2-models.jar\")",
      "dateUpdated": "Nov 24, 2015 2:40:18 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445081890962_769572236",
      "id": "20151017-113810_697892520",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res0: org.apache.zeppelin.spark.dep.Dependency \u003d org.apache.zeppelin.spark.dep.Dependency@4da764d4\n"
      },
      "dateCreated": "Oct 17, 2015 11:38:10 AM",
      "dateStarted": "Nov 24, 2015 2:40:18 PM",
      "dateFinished": "Nov 24, 2015 2:40:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\nimport java.util.Properties\nimport scala.collection.JavaConversions._\nimport edu.stanford.nlp.pipeline._\nimport edu.stanford.nlp.hcoref.data._\nimport edu.stanford.nlp.dcoref._\nimport edu.stanford.nlp.dcoref.CorefCoreAnnotations\nimport edu.stanford.nlp.dcoref.CorefCoreAnnotations._\nimport edu.stanford.nlp.util._\nimport edu.stanford.nlp.semgraph.SemanticGraph\nimport edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations\nimport edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation\nimport edu.stanford.nlp.ling.CoreAnnotations\nimport edu.stanford.nlp.ling.CoreAnnotations._;\nimport edu.stanford.nlp.trees.TreeCoreAnnotations\nimport edu.stanford.nlp.trees.TreeCoreAnnotations._;\n\n    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution\n    val props \u003d new Properties();\n    props.setProperty(\"annotators\", \"tokenize, ssplit, pos, lemma, ner, parse, dcoref\");\n    val pipeline \u003d new StanfordCoreNLP(props);\n\n    // read some text in the text variable\n    val text \u003d \"Stanford University is located in California. It is a great university.\" // Add your text here!\n\n    // create an empty Annotation just with the given text\n    val document \u003d new Annotation(text);\n\n    // run all Annotators on this text\n    pipeline.annotate(document);\n\n    // these are all the sentences in this document\n    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types\n    val sentences \u003d document.get(new SentencesAnnotation().getClass);\n\n    for(sentence \u003c- sentences) {\n      // traversing the words in the current sentence\n      // a CoreLabel is a CoreMap with additional token-specific methods\n      for (token \u003c- sentence.get(new TokensAnnotation().getClass)) {\n        // this is the text of the token\n        val word \u003d token.get(new TextAnnotation().getClass);\n        // this is the POS tag of the token\n        val pos \u003d token.get(new PartOfSpeechAnnotation().getClass);\n        // this is the NER label of the token\n        val ne \u003d token.get(new NamedEntityTagAnnotation().getClass);\n      }\n\n      // this is the parse tree of the current sentence\n      val tree \u003d sentence.get(new TreeAnnotation().getClass);\n\n      // this is the Stanford dependency graph of the current sentence\n      val dependencies \u003d sentence.get(new CollapsedCCProcessedDependenciesAnnotation().getClass);\n    }\n\n    // This is the coreference link graph\n    // Each chain stores a set of mentions that link to each other,\n    // along with a method for getting the most representative mention\n    // Both sentence and token offsets start at 1!\n    val graph \u003d document.get(new CorefChainAnnotation().getClass);\n    System.out.println(graph)\n  ",
      "dateUpdated": "Nov 24, 2015 11:13:12 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447924210352_-1354953812",
      "id": "20151119-091010_319366055",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import java.util.Properties\nimport scala.collection.JavaConversions._\nimport edu.stanford.nlp.pipeline._\nimport edu.stanford.nlp.hcoref.data._\nimport edu.stanford.nlp.dcoref._\nimport edu.stanford.nlp.dcoref.CorefCoreAnnotations\nimport edu.stanford.nlp.dcoref.CorefCoreAnnotations._\nimport edu.stanford.nlp.util._\nimport edu.stanford.nlp.semgraph.SemanticGraph\nimport edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations\nimport edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation\nimport edu.stanford.nlp.ling.CoreAnnotations\nimport edu.stanford.nlp.ling.CoreAnnotations._\nimport edu.stanford.nlp.trees.TreeCoreAnnotations\nimport edu.stanford.nlp.trees.TreeCoreAnnotations._\nprops: java.util.Properties \u003d {}\nres4: Object \u003d null\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1671)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1345)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat java.util.HashMap.readObject(HashMap.java:1394)\n\tat sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:314)\n\tat edu.stanford.nlp.dcoref.Dictionaries.loadGenderNumber(Dictionaries.java:393)\n\tat edu.stanford.nlp.dcoref.Dictionaries.\u003cinit\u003e(Dictionaries.java:557)\n\tat edu.stanford.nlp.dcoref.Dictionaries.\u003cinit\u003e(Dictionaries.java:466)\n\tat edu.stanford.nlp.dcoref.SieveCoreferenceSystem.\u003cinit\u003e(SieveCoreferenceSystem.java:283)\n\tat edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.\u003cinit\u003e(DeterministicCorefAnnotator.java:57)\n\tat edu.stanford.nlp.pipeline.AnnotatorImplementations.coref(AnnotatorImplementations.java:186)\n\tat edu.stanford.nlp.pipeline.AnnotatorFactories$12.create(AnnotatorFactories.java:486)\n\tat edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)\n\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:289)\n\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.\u003cinit\u003e(StanfordCoreNLP.java:126)\n\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.\u003cinit\u003e(StanfordCoreNLP.java:122)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:77)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:82)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:84)\n\n"
      },
      "dateCreated": "Nov 19, 2015 9:10:10 AM",
      "dateStarted": "Nov 24, 2015 11:13:12 AM",
      "dateFinished": "Nov 24, 2015 11:14:05 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//import com.databricks.spark.corenlp.CoreNLP\n\n// val input \u003d sqlContext.createDataFrame(Seq(\n//   (1, \"\u003cxml\u003eStanford University is located in California. It is a great university.\u003c/xml\u003e\")\n// )).toDF(\"id\", \"text\")\n// val coreNLP \u003d new CoreNLP()\n//   .setInputCol(\"text\")\n//   .setAnnotators(Array(\"tokenize\", \"cleanxml\", \"ssplit\"))\n//   .setFlattenNestedFields(Array(\"sentence_token_word\", \"sentence_characterOffsetBegin\"))\n//   .setOutputCol(\"parsed\")\n// val parsed \u003d coreNLP.transform(input)\n//   .select(\"parsed.sentence_token_word\", \"parsed.sentence_characterOffsetBegin\")\n// println(parsed.first())\n",
      "dateUpdated": "Nov 24, 2015 10:35:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447909357405_-1529711515",
      "id": "20151119-050237_1902761270",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Nov 19, 2015 5:02:37 AM",
      "dateStarted": "Nov 19, 2015 6:37:26 AM",
      "dateFinished": "Nov 19, 2015 6:37:38 AM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Populate ActressesAndActorsDF Reference Data",
      "text": "val actressesAndActorsDF \u003d sqlContext.sql(\"SELECT id, name, bio, img FROM actresses_and_actors_perm\")\n\nactressesAndActorsDF.show()",
      "dateUpdated": "Nov 24, 2015 10:35:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445081927425_320383091",
      "id": "20151017-113847_1233175220",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 17, 2015 11:38:47 AM",
      "dateStarted": "Nov 19, 2015 5:02:17 AM",
      "dateFinished": "Nov 19, 2015 5:02:31 AM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert \"bio\" column into Array[String]",
      "text": "import org.apache.spark.ml.feature.Tokenizer\n\nval tokenizerTransformer \u003d new Tokenizer().setInputCol(\"bio\").setOutputCol(\"wordsFeatureVectors\")\n\nval wordsFeatureVectorsDF \u003d tokenizerTransformer.transform(actressesAndActorsDF)\n\nwordsFeatureVectorsDF.select(\"name\", \"bio\", \"wordsFeatureVectors\").show()",
      "dateUpdated": "Nov 24, 2015 10:35:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445102998027_-1055707710",
      "id": "20151017-172958_1780692227",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 17, 2015 5:29:58 PM",
      "dateStarted": "Nov 2, 2015 5:49:13 PM",
      "dateFinished": "Nov 2, 2015 5:49:14 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Filter Out Stop Words",
      "text": "import org.apache.spark.ml.feature.StopWordsRemover\n\nval stopWordRemoverTransformer \u003d new StopWordsRemover().setInputCol(\"wordsFeatureVectors\").setOutputCol(\"filteredWordsFeatureVectors\")\n\nval filteredWordsFeatureVectorsDF \u003d stopWordRemoverTransformer.transform(wordsFeatureVectorsDF)\n\nfilteredWordsFeatureVectorsDF.select(\"name\", \"bio\", \"filteredWordsFeatureVectors\").show()",
      "dateUpdated": "Nov 24, 2015 10:35:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445104905188_1508787084",
      "id": "20151017-180145_1212516680",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 17, 2015 6:01:45 PM",
      "dateStarted": "Nov 2, 2015 5:49:14 PM",
      "dateFinished": "Nov 2, 2015 5:49:15 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "TF/IDF Featurizer",
      "text": "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n\nval hashingTFTransformer \u003d new HashingTF().setInputCol(\"filteredWordsFeatureVectors\").setOutputCol(\"tfWordsFeatureVectors\").setNumFeatures(1000)\n\nval tfWordsFeatureVectorsDF \u003d hashingTFTransformer.transform(filteredWordsFeatureVectorsDF)\n\nval idfEstimator \u003d new IDF().setInputCol(\"tfWordsFeatureVectors\").setOutputCol(\"tfIdfWordsFeatureVectors\")\n\nval idfModelTransformer \u003d idfEstimator.fit(tfWordsFeatureVectorsDF)\n\nval tfIdfWordsFeatureVectorsDF \u003d idfModelTransformer.transform(tfWordsFeatureVectorsDF)\n\ntfIdfWordsFeatureVectorsDF.select(\"name\", \"bio\", \"tfIdfWordsFeatureVectors\").show()",
      "dateUpdated": "Nov 24, 2015 10:35:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445099320523_1491093045",
      "id": "20151017-162840_1956587375",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 17, 2015 4:28:40 PM",
      "dateStarted": "Nov 2, 2015 5:49:14 PM",
      "dateFinished": "Nov 2, 2015 5:49:17 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "TODO:  You\u0027ll want to normalize otherwise absolute value (magnitude) will affect outcome",
      "text": "",
      "dateUpdated": "Nov 24, 2015 10:35:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445949366628_2065789687",
      "id": "20151027-123606_45737102",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 27, 2015 12:36:06 PM",
      "dateStarted": "Nov 2, 2015 5:49:15 PM",
      "dateFinished": "Nov 2, 2015 5:49:17 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Word2Vec Featurizer",
      "text": "import org.apache.spark.ml.feature.Word2Vec\n\nval word2VecEstimator \u003d new Word2Vec()\n  .setInputCol(\"filteredWordsFeatureVectors\")\n  .setOutputCol(\"word2vecWordsFeatureVectors\")\n  .setMinCount(2)\n\nval word2VecModelTransformer \u003d word2VecEstimator.fit(filteredWordsFeatureVectorsDF)\n\nval word2VecWordFeatureVectorsDF \u003d word2VecModelTransformer.transform(filteredWordsFeatureVectorsDF)\n\nword2VecWordFeatureVectorsDF.select(\"name\", \"bio\", \"word2vecWordsFeatureVectors\").show()",
      "dateUpdated": "Nov 24, 2015 10:35:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445099681901_-853444159",
      "id": "20151017-163441_477395570",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 17, 2015 4:34:41 PM",
      "dateStarted": "Nov 2, 2015 5:49:17 PM",
      "dateFinished": "Nov 2, 2015 5:49:18 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Count Featurizer - Words",
      "text": "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n\n// fit a CountVectorizerModel from the corpus\nval countWordsVectorizerEstimator \u003d new CountVectorizer()\n  .setInputCol(\"filteredWordsFeatureVectors\")\n  .setOutputCol(\"countWordsFeatureVectors\")\n  .setMinDF(2) // a term must appear in more or equal to 2 documents to be included in the vocabulary\n\nval countWordsVectorizerModelTransformer \u003d countWordsVectorizerEstimator.fit(filteredWordsFeatureVectorsDF)\n\nval countWordsFeatureVectorsDF \u003d countWordsVectorizerModelTransformer.transform(filteredWordsFeatureVectorsDF).select(\"name\", \"bio\", \"filteredWordsFeatureVectors\", \"countWordsFeatureVectors\")\n\ncountWordsFeatureVectorsDF.show()",
      "dateUpdated": "Nov 24, 2015 10:35:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445101913136_1473887137",
      "id": "20151017-171153_871858",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 17, 2015 5:11:53 PM",
      "dateStarted": "Nov 2, 2015 5:49:17 PM",
      "dateFinished": "Nov 2, 2015 5:49:19 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "N-Gram Featurizer",
      "text": "import org.apache.spark.ml.feature.NGram\n\nval ngramTransformer \u003d new NGram().setN(2).setInputCol(\"filteredWordsFeatureVectors\").setOutputCol(\"ngramsFeatureVectors\")\n\nval ngramsFeatureVectorsDF \u003d ngramTransformer.transform(filteredWordsFeatureVectorsDF)\n\nngramsFeatureVectorsDF.select(\"name\", \"bio\", \"ngramsFeatureVectors\").show()",
      "dateUpdated": "Nov 24, 2015 10:35:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445102307362_-534718304",
      "id": "20151017-171827_540520315",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 17, 2015 5:18:27 PM",
      "dateStarted": "Nov 2, 2015 5:49:19 PM",
      "dateFinished": "Nov 2, 2015 5:49:20 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "TODO:  Add categorical Features (Genres?) and One Hot Encode",
      "dateUpdated": "Nov 24, 2015 10:35:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445210443739_1347878706",
      "id": "20151018-232043_2008195623",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 18, 2015 11:20:43 PM",
      "dateStarted": "Nov 2, 2015 5:49:21 PM",
      "dateFinished": "Nov 2, 2015 5:49:21 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Nov 24, 2015 10:35:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445209878832_1660663714",
      "id": "20151018-231118_1458525638",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Scheduler already terminated"
      },
      "dateCreated": "Oct 18, 2015 11:11:18 PM",
      "dateStarted": "Nov 2, 2015 5:49:22 PM",
      "dateFinished": "Nov 2, 2015 5:49:22 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Profile NLP/01: Feature Engineering",
  "id": "2B2SHA7Y8",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}
