{
  "paragraphs": [
    {
      "text": "%dep\nz.reset()\nz.addRepo(\"maven central\").url(\"search.maven.org\")\nz.load(\"com.datastax.spark:spark-cassandra-connector_2.10:1.4.0\")\nz.load(\"org.elasticsearch:elasticsearch-spark_2.10:2.1.0\")\nz.load(\"com.databricks:spark-csv_2.10:1.2.0\")\nz.load(\"org.apache.spark:spark-streaming-kafka-assembly_2.10:1.5.1\")\nz.load(\"/root/zeppelin-0.6.0-spark-1.5.1-hadoop-2.6.0-fluxcapacitor/lib/mysql-connector-java.jar\")\nz.load(\"/root/pipeline/myapps/datasource/target/scala-2.10/datasource_2.10-1.0.jar\")",
      "dateUpdated": "Oct 26, 2015 6:12:12 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1444104301541_-1668023437",
      "id": "20151006-040501_1444996520",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Must be used before SparkInterpreter (%spark) initialized"
      },
      "dateCreated": "Oct 6, 2015 4:05:01 AM",
      "dateStarted": "Oct 26, 2015 6:15:24 PM",
      "dateFinished": "Oct 26, 2015 6:15:24 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val options \u003d Map(\"start\" -\u003e \"101\", \"end\" -\u003e \"228\")\nval startingValuesDF \u003d sqlContext.load(\"com.fluxcapacitor.pipeline.spark.sql.IntegerRangeRelationProvider\", options)\nstartingValuesDF.select($\"int_col\").show(150)",
      "dateUpdated": "Oct 29, 2015 7:00:39 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1444104338511_-1259173273",
      "id": "20151006-040538_1295650640",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "options: scala.collection.immutable.Map[String,String] \u003d Map(start -\u003e 101, end -\u003e 228)\nwarning: there were 1 deprecation warning(s); re-run with -deprecation for details\nstartingValuesDF: org.apache.spark.sql.DataFrame \u003d [int_col: int]\n+-------+\n|int_col|\n+-------+\n|    101|\n|    102|\n|    103|\n|    104|\n|    105|\n|    106|\n|    107|\n|    108|\n|    109|\n|    110|\n|    111|\n|    112|\n|    113|\n|    114|\n|    115|\n|    116|\n|    117|\n|    118|\n|    119|\n|    120|\n|    121|\n|    122|\n|    123|\n|    124|\n|    125|\n|    126|\n|    127|\n|    128|\n|    129|\n|    130|\n|    131|\n|    132|\n|    133|\n|    134|\n|    135|\n|    136|\n|    137|\n|    138|\n|    139|\n|    140|\n|    141|\n|    142|\n|    143|\n|    144|\n|    145|\n|    146|\n|    147|\n|    148|\n|    149|\n|    150|\n|    151|\n|    152|\n|    153|\n|    154|\n|    155|\n|    156|\n|    157|\n|    158|\n|    159|\n|    160|\n|    161|\n|    162|\n|    163|\n|    164|\n|    165|\n|    166|\n|    167|\n|    168|\n|    169|\n|    170|\n|    171|\n|    172|\n|    173|\n|    174|\n|    175|\n|    176|\n|    177|\n|    178|\n|    179|\n|    180|\n|    181|\n|    182|\n|    183|\n|    184|\n|    185|\n|    186|\n|    187|\n|    188|\n|    189|\n|    190|\n|    191|\n|    192|\n|    193|\n|    194|\n|    195|\n|    196|\n|    197|\n|    198|\n|    199|\n|    200|\n|    201|\n|    202|\n|    203|\n|    204|\n|    205|\n|    206|\n|    207|\n|    208|\n|    209|\n|    210|\n|    211|\n|    212|\n|    213|\n|    214|\n|    215|\n|    216|\n|    217|\n|    218|\n|    219|\n|    220|\n|    221|\n|    222|\n|    223|\n|    224|\n|    225|\n|    226|\n|    227|\n|    228|\n+-------+\n\n"
      },
      "dateCreated": "Oct 6, 2015 4:05:38 AM",
      "dateStarted": "Oct 29, 2015 7:00:39 PM",
      "dateFinished": "Oct 29, 2015 7:00:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, IntegerType}\n\nval schema \u003d\n  StructType(\n    StructField(\"int_col\", IntegerType, true) :: Nil)\n    \nval insertsDF \u003d sqlContext.createDataFrame(sc.parallelize(10 to 55).map(Row(_)), schema)\ninsertsDF.show()",
      "dateUpdated": "Oct 29, 2015 7:01:01 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1444083194557_-1029599800",
      "id": "20151005-221314_57539024",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, IntegerType}\nschema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(int_col,IntegerType,true))\ninsertsDF: org.apache.spark.sql.DataFrame \u003d [int_col: int]\n+-------+\n|int_col|\n+-------+\n|     10|\n|     11|\n|     12|\n|     13|\n|     14|\n|     15|\n|     16|\n|     17|\n|     18|\n|     19|\n|     20|\n|     21|\n|     22|\n|     23|\n|     24|\n|     25|\n|     26|\n|     27|\n|     28|\n|     29|\n+-------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Oct 5, 2015 10:13:14 PM",
      "dateStarted": "Oct 29, 2015 7:01:01 PM",
      "dateFinished": "Oct 29, 2015 7:01:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val unionedDF \u003d startingValuesDF.unionAll(insertsDF)\nunionedDF.show(200)",
      "dateUpdated": "Oct 29, 2015 7:01:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445565073809_1122234116",
      "id": "20151023-015113_1114226600",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "unionedDF: org.apache.spark.sql.DataFrame \u003d [int_col: int]\n+-------+\n|int_col|\n+-------+\n|    101|\n|    102|\n|    103|\n|    104|\n|    105|\n|    106|\n|    107|\n|    108|\n|    109|\n|    110|\n|    111|\n|    112|\n|    113|\n|    114|\n|    115|\n|    116|\n|    117|\n|    118|\n|    119|\n|    120|\n|    121|\n|    122|\n|    123|\n|    124|\n|    125|\n|    126|\n|    127|\n|    128|\n|    129|\n|    130|\n|    131|\n|    132|\n|    133|\n|    134|\n|    135|\n|    136|\n|    137|\n|    138|\n|    139|\n|    140|\n|    141|\n|    142|\n|    143|\n|    144|\n|    145|\n|    146|\n|    147|\n|    148|\n|    149|\n|    150|\n|    151|\n|    152|\n|    153|\n|    154|\n|    155|\n|    156|\n|    157|\n|    158|\n|    159|\n|    160|\n|    161|\n|    162|\n|    163|\n|    164|\n|    165|\n|    166|\n|    167|\n|    168|\n|    169|\n|    170|\n|    171|\n|    172|\n|    173|\n|    174|\n|    175|\n|    176|\n|    177|\n|    178|\n|    179|\n|    180|\n|    181|\n|    182|\n|    183|\n|    184|\n|    185|\n|    186|\n|    187|\n|    188|\n|    189|\n|    190|\n|    191|\n|    192|\n|    193|\n|    194|\n|    195|\n|    196|\n|    197|\n|    198|\n|    199|\n|    200|\n|    201|\n|    202|\n|    203|\n|    204|\n|    205|\n|    206|\n|    207|\n|    208|\n|    209|\n|    210|\n|    211|\n|    212|\n|    213|\n|    214|\n|    215|\n|    216|\n|    217|\n|    218|\n|    219|\n|    220|\n|    221|\n|    222|\n|    223|\n|    224|\n|    225|\n|    226|\n|    227|\n|    228|\n|     10|\n|     11|\n|     12|\n|     13|\n|     14|\n|     15|\n|     16|\n|     17|\n|     18|\n|     19|\n|     20|\n|     21|\n|     22|\n|     23|\n|     24|\n|     25|\n|     26|\n|     27|\n|     28|\n|     29|\n|     30|\n|     31|\n|     32|\n|     33|\n|     34|\n|     35|\n|     36|\n|     37|\n|     38|\n|     39|\n|     40|\n|     41|\n|     42|\n|     43|\n|     44|\n|     45|\n|     46|\n|     47|\n|     48|\n|     49|\n|     50|\n|     51|\n|     52|\n|     53|\n|     54|\n|     55|\n+-------+\n\n"
      },
      "dateCreated": "Oct 23, 2015 1:51:13 AM",
      "dateStarted": "Oct 29, 2015 7:01:07 PM",
      "dateFinished": "Oct 29, 2015 7:01:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "unionedDF.write.mode(\"append\").format(\"com.fluxcapacitor.pipeline.spark.sql.IntegerRangeRelationProvider\").save(\"/tmp/unionedDF.txt\")",
      "dateUpdated": "Oct 23, 2015 3:05:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1444767902147_793540924",
      "id": "20151013-202502_431405783",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.util.NoSuchElementException: key not found: start\n\tat scala.collection.MapLike$class.default(MapLike.scala:228)\n\tat scala.collection.AbstractMap.default(Map.scala:58)\n\tat scala.collection.MapLike$class.apply(MapLike.scala:141)\n\tat scala.collection.AbstractMap.apply(Map.scala:58)\n\tat com.fluxcapacitor.pipeline.spark.sql.IntegerRangeRelationProvider.createRelation(IntegerDataSource.scala:54)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:170)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:39)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:44)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:46)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:48)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:50)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:52)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:54)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:56)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat \u003cinit\u003e(\u003cconsole\u003e:60)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:64)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Oct 13, 2015 8:25:02 PM",
      "dateStarted": "Oct 23, 2015 3:05:44 PM",
      "dateFinished": "Oct 23, 2015 3:05:44 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Oct 23, 2015 3:05:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445564294581_-1871218476",
      "id": "20151023-013814_1168228197",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Oct 23, 2015 1:38:14 AM",
      "dateStarted": "Oct 23, 2015 3:05:44 PM",
      "dateFinished": "Oct 23, 2015 3:05:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "DataSources/02: Creating a Custom Data Source",
  "id": "2AZ37ZY6H",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}