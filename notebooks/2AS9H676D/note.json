{
  "paragraphs": [
    {
      "text": "val esConfig \u003d Map(\"pushdown\" -\u003e \"true\", \"es.nodes\" -\u003e \"52.27.56.210\", \"es.port\" -\u003e \"9200\")",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435990160812_-1555340595",
      "id": "20150704-060920_41021679",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "esConfig: scala.collection.immutable.Map[String,String] \u003d Map(pushdown -\u003e true, es.nodes -\u003e 52.27.56.210, es.port -\u003e 9200)\n"
      },
      "dateCreated": "Jul 4, 2015 6:09:20 AM",
      "dateStarted": "Jul 7, 2015 6:48:38 PM",
      "dateFinished": "Jul 7, 2015 6:48:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "case class Recommendation(id: Long, name: String, bio: String, img: String, rank: Double)\ncase class PersonalizedRecommendations(user_id: Long, recommendations: Seq[Recommendation], timestamp: Long)\ncase class NonPersonalizedRecommendations(recommendations: Seq[Recommendation], timestamp: Long)",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435989983272_1100718980",
      "id": "20150704-060623_1625777771",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class Recommendation\ndefined class PersonalizedRecommendations\ndefined class NonPersonalizedRecommendations\n"
      },
      "dateCreated": "Jul 4, 2015 6:06:23 AM",
      "dateStarted": "Jul 7, 2015 6:48:39 PM",
      "dateFinished": "Jul 7, 2015 6:48:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val generationTime \u003d compat.Platform.currentTime\nval recommendations \u003d PersonalizedRecommendations(10002,Seq[Recommendation](Recommendation(90003, \"name1\", \"bio1\", \"img1\", 5.0),(Recommendation(90004,\"name2\",\"bio2\",\"img2\", 2.0))), generationTime)",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435987372333_-1887764168",
      "id": "20150704-052252_1368454748",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "generationTime: Long \u003d 1436294931196\nrecommendations: PersonalizedRecommendations \u003d PersonalizedRecommendations(10002,List(Recommendation(90003,name1,bio1,img1,5.0), Recommendation(90004,name2,bio2,img2,2.0)),1436294931196)\n"
      },
      "dateCreated": "Jul 4, 2015 5:22:52 AM",
      "dateStarted": "Jul 7, 2015 6:48:50 PM",
      "dateFinished": "Jul 7, 2015 6:48:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val recommendationsDF \u003d sc.parallelize(recommendations::Nil).toDF()",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435990340553_-1458142498",
      "id": "20150704-061220_638302960",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "recommendationsDF: org.apache.spark.sql.DataFrame \u003d [user_id: bigint, recommendations: array\u003cstruct\u003cid:bigint,name:string,bio:string,img:string,rank:double\u003e\u003e, timestamp: bigint]\n"
      },
      "dateCreated": "Jul 4, 2015 6:12:20 AM",
      "dateStarted": "Jul 7, 2015 6:48:51 PM",
      "dateFinished": "Jul 7, 2015 6:48:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.elasticsearch.spark.sql._ \nrecommendationsDF.saveToEs(\"sparkafterdark/personalized-als\")",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435990468412_-274823447",
      "id": "20150704-061428_1989928322",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.elasticsearch.spark.sql._\n"
      },
      "dateCreated": "Jul 4, 2015 6:14:28 AM",
      "dateStarted": "Jul 7, 2015 6:48:52 PM",
      "dateFinished": "Jul 7, 2015 6:48:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d sqlContext.read.format(\"org.elasticsearch.spark.sql\").options(esConfig).load(\"sparkafterdark/personalized-als\", \"?q\u003duser_id\u003d10002\")",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435980872267_1181124472",
      "id": "20150704-033432_1647942179",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:26: error: overloaded method value load with alternatives:\n  ()org.apache.spark.sql.DataFrame \u003cand\u003e\n  (path: String)org.apache.spark.sql.DataFrame\n cannot be applied to (String, String)\n       val df \u003d sqlContext.read.format(\"org.elasticsearch.spark.sql\").options(esConfig).load(\"sparkafterdark/personalized-als\", \"?q\u003duser_id\u003d10002\")\n                                                                                        ^\n"
      },
      "dateCreated": "Jul 4, 2015 3:34:32 AM",
      "dateStarted": "Jul 7, 2015 6:49:27 PM",
      "dateFinished": "Jul 7, 2015 6:49:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.collect()",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435987248426_-958347098",
      "id": "20150704-052048_27493674",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 4.0 failed 1 times, most recent failure: Lost task 4.0 in stage 4.0 (TID 25, localhost): java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:313)\n\tat scala.None$.get(Option.scala:311)\n\tat org.elasticsearch.spark.sql.RowValueReader$class.rowOrder(RowValueReader.scala:24)\n\tat org.elasticsearch.spark.sql.ScalaRowValueReader.rowOrder(ScalaEsRowValueReader.scala:13)\n\tat org.elasticsearch.spark.sql.ScalaRowValueReader.createMap(ScalaEsRowValueReader.scala:32)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:620)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:559)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.list(ScrollReader.java:600)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:562)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:636)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:559)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:358)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:293)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:188)\n\tat org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:167)\n\tat org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:403)\n\tat org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:76)\n\tat org.elasticsearch.spark.rdd.AbstractEsRDDIterator.hasNext(AbstractEsRDDIterator.scala:43)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\n"
      },
      "dateCreated": "Jul 4, 2015 5:20:48 AM",
      "dateStarted": "Jul 7, 2015 6:49:07 PM",
      "dateFinished": "Jul 7, 2015 6:49:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1436294254512_520191011",
      "id": "20150707-183734_1325417522",
      "dateCreated": "Jul 7, 2015 6:37:34 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Tmp-ElasticSearch",
  "id": "2AS9H676D",
  "angularObjects": {},
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}