{
  "metadata" : {
    "name" : "Ratings from Kafka",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/root/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M3", "org.apache.spark:spark-streaming-kafka_2.10:1.4.1", "org.apache.spark % spark-graphx_2.10 % 1.4.1", "- org.apache.spark % spark-core_2.10 % _", "- org.apache.spark % spark-streaming_2.10 % _", "- org.apache.hadoop % _ % _" ],
    "customImports" : null,
    "customSparkConf" : {
      "spark.cassandra.connection.host" : "127.0.0.1",
      "spark.master" : "spark://127.0.0.1:7077",
      "spark.executor.cores" : "2",
      "spark.executor.memory" : "512m",
      "spark.cores.max" : "2",
      "spark.eventLog.enabled" : "true",
      "spark.eventLog.dir" : "logs/spark"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Setup (boilerplate)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@12cc1d1d\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Creating the domain object"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "case class Rating(fromUserId: Int, toUserId: Int, rating: Int) {\n  def toCSV=s\"$fromUserId,$rating,$toUserId\"\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class Rating\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Streaming: Kafka"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<span style=\"font-size:15pt;color:red;\">(**NOT** needed if _feeder_ is running)</span>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Kafka Producer"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "import java.util.Properties\n\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.Future\n\nimport org.apache.kafka.clients.producer.{KafkaProducer,ProducerConfig,ProducerRecord}\n\nval props = new Properties()\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092,localhost:9093\")\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\nval producer = new KafkaProducer[String, String](props)\n\n// Guard to stop the producer\nvar stopSending = false\n\n// future that issues a future.\n// a future sends a message after having waited for up to 500ms\ndef sendMsg:Unit = Future {\n  Thread.sleep((scala.util.Random.nextDouble * 500).toLong)\n  producer.send {\n    val msg = Rating(scala.util.Random.nextInt(10000), scala.util.Random.nextInt(10), scala.util.Random.nextInt(10000))\n    new ProducerRecord[String, String](\"ratings\", msg.fromUserId.toString, msg.toCSV)\n  }\n  if (!stopSending) sendMsg\n}\nsendMsg",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Viz"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Reactive list of data (capped at 20 elements)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val list = ul(20)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "list: notebook.front.DataConnectedWidget[String]{implicit val singleCodec: notebook.Codec[play.api.libs.json.JsValue,String]; def data: Seq[String]; def data_=(x$1: Seq[String]): Unit; lazy val toHtml: scala.xml.Elem; def append(s: String): Unit; def appendAll(s: Seq[String]): Unit} = <anon$1 widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<ul data-bind=\"foreach: value\">\n      <li data-bind=\"text: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anonc2c33194ae2addb2d370285944f897eb&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n          /*]]>*/</script></ul>"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Reactive line plot"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val line = widgets.LineChart[Seq[(Long, Double)]](Seq.empty[(Long, Double)], fields=Some((\"X\", \"Y\")), maxPoints=100)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "line: notebook.front.widgets.LineChart[Seq[(Long, Double)]] = <LineChart widget>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonf7dbe3199e4ccb2b4419e0fbc04ec0a6&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;1582941700&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/lineChart'], \n      function(playground, _magiclineChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiclineChart,\n    \"o\": {\"x\":\"X\",\"y\":\"Y\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Spark Streaming consuming Kafka"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "The context and kafka conf"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\n\nval ssc = new StreamingContext(sc, Seconds(2))\nval brokers = \"localhost:9092,localhost:9093\"\nval topics = Set(\"ratings\")\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@5e3e58b7\nbrokers: String = localhost:9092,localhost:9093\ntopics: scala.collection.immutable.Set[String] = Set(ratings)\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> localhost:9092,localhost:9093)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonab8b619e789ae0d202fd7f719beba03f&quot;,&quot;dataInit&quot;:[{&quot;X&quot;:&quot;metadata.broker.list&quot;,&quot;Y&quot;:&quot;localhost:9092,localhost:9093&quot;}],&quot;genId&quot;:&quot;1790265577&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <ul class=\"nav nav-tabs\" id=\"ul1790265577\"><li>\n              <a href=\"#tab1790265577-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab1790265577-1\"><i class=\"fa fa-pie-chart\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab1790265577\"><div class=\"tab-pane\" id=\"tab1790265577-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anona98784a4981ab0e6b03b1bd5029949f3&quot;,&quot;dataInit&quot;:[{&quot;X&quot;:&quot;metadata.broker.list&quot;,&quot;Y&quot;:&quot;localhost:9092,localhost:9093&quot;}],&quot;genId&quot;:&quot;1610463916&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"X\",\"Y\"],\"nrow\":1,\"shown\":1,\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div>\n            </div><div class=\"tab-pane\" id=\"tab1790265577-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonde8558c0f780aed40a43beb64ec44619&quot;,&quot;dataInit&quot;:[{&quot;X&quot;:&quot;metadata.broker.list&quot;,&quot;Y&quot;:&quot;localhost:9092,localhost:9093&quot;}],&quot;genId&quot;:&quot;1025060989&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pieChart'], \n      function(playground, _magicpieChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpieChart,\n    \"o\": {\"series\":\"X\",\"p\":\"Y\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div>\n            </div></div>\n      </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Biz:\n* Consuming Kafka, \n* creating Ratings, \n* computing moving average\n* update list and line plot"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\n// connect (direct) to kafka\nval ratingsStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n  ssc, kafkaParams, topics)\n\n// transform the CSV Strings into Ratings\nval msgs = ratingsStream.transform {\n  (message: RDD[(String, String)], batchTime: Time) => {\n    // convert each RDD from the batch into a DataFrame\n    val df = message.map(_._2.split(\",\"))\n                    .map(rating => \n                         Rating(rating(0).trim.toInt, rating(1).trim.toInt, rating(2).trim.toInt)\n                    ).toDF(\"fromuserid\", \"touserid\", \"rating\")\n\n    // add the batch time to the DataFrame\n    val dfWithBatchTime = df.withColumn(\"batchtime\", org.apache.spark.sql.functions.lit(batchTime.milliseconds))\n    dfWithBatchTime.rdd\n  }\n}\n\n//show the first ten of each RDD\nmsgs.foreachRDD(rdd => list.appendAll(rdd.take(10).toList.map(_.toString)))\n\n// show the moving average of 10s in the plot\nval data = new collection.mutable.ArrayBuffer[(Long, Double)]()\nval w = msgs.window(Seconds(10), Seconds(2)).foreachRDD{(rdd, t) => \n              val ndata = Seq((t.milliseconds, rdd.map(_.getAs[Int](\"rating\")).mean.toDouble))\n              org.apache.log4j.Logger.getLogger(\"lines\").info(\"# \" + ndata.size)\n              line.addAndApply(data ++= ndata)\n             }\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 1 feature warning(s); re-run with -feature for details\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\nratingsStream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@59a35ae4\nmsgs: org.apache.spark.streaming.dstream.DStream[org.apache.spark.sql.Row] = org.apache.spark.streaming.dstream.TransformedDStream@73a45cb5\ndata: scala.collection.mutable.ArrayBuffer[(Long, Double)] = ArrayBuffer()\nw: Unit = ()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Start consuming"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "ssc.start()",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Stop all"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Stop the streaming context (keeping the spark context up, just in case)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "//ssc.stop(false) // commented in case of a Run All ^^",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Stop the producer"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "//stopSending = true // commented in case of a Run All ^^",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}