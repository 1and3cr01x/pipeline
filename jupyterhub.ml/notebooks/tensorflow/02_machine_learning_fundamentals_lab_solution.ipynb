{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import helpers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to toy around with [UCI Wine Quality data set](https://archive.ics.uci.edu/ml/datasets/Wine+Quality). The data  The below code downloads the data and info file to the correct directory (`data/02/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "data_dir = 'data/02'\n",
    "helpers.mkdir(data_dir)\n",
    "data_path = helpers.download('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', \n",
    "                             os.path.join(data_dir, 'winequality-white.csv'))\n",
    "names_path = helpers.download('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names',\n",
    "                              os.path.join(data_dir, 'winequality.names'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data in to Pandas, shuffling it along the way\n",
    "data = pd.read_csv(data_path, sep=';')\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into x inputs and y labels\n",
    "x = data.iloc[:, :11].as_matrix()\n",
    "y = data.iloc[:, -1].as_matrix()\n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split data into training, validation, and test sets\n",
    "\n",
    "* Use a 60/20/20 percentage split between the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = len(data)\n",
    "train_split = 0.6\n",
    "valid_split = 0.2\n",
    "\n",
    "# Number of examples in training set\n",
    "num_train = int((num_rows * train_split) // 1)\n",
    "# Number of examples in validation set\n",
    "num_valid = int((num_rows * valid_split) // 1)\n",
    "# Number of examples in test set \n",
    "# = num_rows - num_train - num_valid\n",
    "\n",
    "# Training data inputs\n",
    "train_data = x[:num_train, :]\n",
    "# Training data labels\n",
    "train_labels = y[:num_train]\n",
    "# Validation data inputs\n",
    "valid_data = x[num_train:num_train+num_valid, :]\n",
    "# Validation data labels\n",
    "valid_labels = y[num_train:num_train+num_valid]\n",
    "# Test data inputs\n",
    "test_data = x[num_train+num_valid:, :]\n",
    "# Test data labels\n",
    "test_labels = y[num_train+num_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity checking that the size of corresponding input and label data are the same size\n",
    "assert len(train_data) == len(train_labels), 'Train data input/label size mismatch'\n",
    "assert len(valid_data) == len(valid_labels), 'Validation data input/label size mismatch'\n",
    "assert len(test_data) == len(test_labels), 'Test data input/label size mismatch'\n",
    "\n",
    "# Print out number of training examples\n",
    "print('Number of training examples: \\t{}'.format(len(train_data)))\n",
    "print('Number of validation examples: \\t{}'.format(len(valid_data)))\n",
    "print('Number of test examples: \\t{}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a basic linear model\n",
    "\n",
    "* Your weights should be initialized with `tf.truncated_normal()`\n",
    "* Your bias should be initialized to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple graph, nothing fancy\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.float32, [None, 11], name='inputs')\n",
    "        labels = tf.placeholder(tf.float32, [None], name='labels')\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    with tf.name_scope('model'):\n",
    "        w = tf.Variable(tf.truncated_normal([11, 1], stddev=0.01), name='w')\n",
    "        b = tf.Variable(tf.zeros([]), name='b')\n",
    "        y_hat = tf.matmul(inputs, w) + b\n",
    "    with tf.name_scope('loss'):\n",
    "        error = tf.reduce_mean(tf.square(y_hat - labels), name='MSE')\n",
    "        train = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)\n",
    "    with tf.name_scope('global_step'):\n",
    "        global_step = tf.Variable(0, trainable=False, name='inputs')\n",
    "        inc_step = tf.assign_add(global_step, 1, name='increment_step')\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train your model\n",
    "\n",
    "* You'll have to experiment with different learning rates\n",
    "* Print out your loss information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Session and initialize Variables\n",
    "sess = tf.Session(graph=graph)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dict = {inputs: train_data, labels: train_labels, learning_rate: 0.000005}\n",
    "valid_dict = {inputs: valid_data, labels: valid_labels}\n",
    "test_dict = {inputs: test_data, labels: test_labels}\n",
    "\n",
    "for i in range(500):\n",
    "    err, step, _ = sess.run([error, inc_step, train], train_dict)\n",
    "    if step % 50 == 0:\n",
    "        # Get validation data loss\n",
    "        v_err = sess.run(error, valid_dict)\n",
    "        print('Step: {}\\n\\ttrain error:\\t\\t{}\\n\\tvalidation error:\\t{}'.format(step, err, v_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When satisfied with model, close Session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Soup it up\n",
    "\n",
    "Let's add some TensorBoard features:\n",
    "\n",
    "* Add a scalar summary for your loss\n",
    "* Open up a FileWriter to save summaries to disk\n",
    "* Periodically write summary data to disk\n",
    "* Add some name scopes if you haven't already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fancier graph\n",
    "fancy_graph = tf.Graph()\n",
    "with fancy_graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.float32, [None, 11], name='inputs')\n",
    "        labels = tf.placeholder(tf.float32, [None], name='labels')\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    with tf.name_scope('model'):\n",
    "        w = tf.Variable(tf.truncated_normal([11, 1], stddev=0.01), name='w')\n",
    "        b = tf.Variable(tf.zeros([]), name='b')\n",
    "        y_hat = tf.matmul(inputs, w) + b\n",
    "    with tf.name_scope('loss'):\n",
    "        error = tf.reduce_mean(tf.square(y_hat - labels), name='MSE')\n",
    "        train = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)\n",
    "    with tf.name_scope('global_step'):\n",
    "        global_step = tf.Variable(0, trainable=False, name='inputs')\n",
    "        inc_step = tf.assign_add(global_step, 1, name='increment_step')\n",
    "\n",
    "    error_summ = tf.summary.scalar('error_summary', error)\n",
    "    w_summ = tf.summary.histogram('w_summary', w)\n",
    "    b_summ = tf.summary.scalar('b_summary', b)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Session and initialize Variables\n",
    "sess = tf.Session(graph=fancy_graph)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb_base_path = 'tbout/02_lab_solution'\n",
    "i = 0\n",
    "tb_path = os.path.join(tb_base_path, str(i))\n",
    "while os.path.exists(tb_path) and os.path.isdir(tb_path):\n",
    "    i += 1\n",
    "    tb_path = os.path.join(tb_base_path, str(i))\n",
    "\n",
    "# Open a FileWriter to create TensorBoard summaries\n",
    "train_writer = tf.summary.FileWriter(os.path.join(tb_path, 'training'), graph=fancy_graph)\n",
    "valid_writer = tf.summary.FileWriter(os.path.join(tb_path, 'validation'))\n",
    "\n",
    "train_dict = {inputs: train_data, labels: train_labels, learning_rate: 0.000005}\n",
    "valid_dict = {inputs: valid_data, labels: valid_labels}\n",
    "test_dict = {inputs: test_data, labels: test_labels}\n",
    "\n",
    "for i in range(500):\n",
    "    err, step, summaries, _ = sess.run([error, inc_step, summary_op, train], train_dict)\n",
    "    if step % 50 == 0 or step == 1:\n",
    "        # Get validation data loss\n",
    "        v_summary = sess.run(error_summ, valid_dict)\n",
    "        train_writer.add_summary(summaries, step)\n",
    "        valid_writer.add_summary(v_summary, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard by navigating to the directory holding this notebook and running this command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=tbout/02_lab_solution/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
