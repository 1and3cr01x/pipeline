{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where Am I?\n",
    "\n",
    "## Startup.ML Conference - San Francisco - Jan 20, 2017\n",
    "![StartupML Conference](https://s3.amazonaws.com/fluxcapacitor.com/img/startup-ml-conf-01-21-2017-san-francisco.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fcb54439-1c2c-46ce-9d92-e77df43393db"
    }
   },
   "source": [
    "# Who Am I?\n",
    "## Chris Fregly\n",
    "\n",
    "![Chris Fregly](https://s3.amazonaws.com/fluxcapacitor.com/img/fregly-300x300.png)\n",
    "\n",
    "![Chris Fregly Emoji](https://s3.amazonaws.com/fluxcapacitor.com/img/fregly-emoji-300x300.png)\n",
    "### Research Scientist @ **[PipelineIO](http://pipeline.io)** \n",
    "\n",
    "![PipelineIO](http://pipeline.io/images/pipeline-io-logo-shadow-210x186.png)\n",
    "\n",
    "### Video Series Author \"High Performance Tensorflow in Production\" @ [OReilly](http://oreilly.com) (Coming Soon)\n",
    "\n",
    "![OReilly Media](https://s3.amazonaws.com/fluxcapacitor.com/img/oreilly-logo.png)\n",
    "\n",
    "### Founder @ **[Advanced Spark and Tensorflow Meetup](http://http://www.meetup.com/Advanced-Spark-and-TensorFlow-Meetup/)**\n",
    "\n",
    "![Advanced Spark and Tensorflow Meetup](https://s3.amazonaws.com/fluxcapacitor.com/img/advanced-spark-tensorflow-meetup-logo-green.png)\n",
    "\n",
    "### **[Github Repo](https://github.com/fluxcapacitor/pipeline)**\n",
    "\t\n",
    "![Github Repo](https://s3.amazonaws.com/fluxcapacitor.com/img/pipeline-io-github-1000-stars.png)\n",
    "\n",
    "### **[DockerHub Repo](https://hub.docker.com/u/fluxcapacitor/)**\n",
    "\n",
    "![DockerHub](https://s3.amazonaws.com/fluxcapacitor.com/img/dockerhub-logo.png)\n",
    "\n",
    "### **[Slideshare](http://www.slideshare.net/cfregly)**\n",
    "\n",
    "![Slideshare](http://advancedspark.com/img/slideshare.png)\n",
    "\n",
    "### **[YouTube](https://www.youtube.com/playlist?list=PL7pBcJ870QHeNRBXdKirc4fdtbtbB5Xy-)**\n",
    "\n",
    "![YouTube](http://advancedspark.com/img/youtube-300x134.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Was I?\n",
    "### Software Engineer @ **Netflix**, **Databricks**, **IBM Spark Tech Center**\n",
    "\n",
    "![Netflix](http://pipeline.io/images/netflixoss-logo-white-295x55.png) \n",
    "\n",
    "![Databricks](http://pipeline.io/images/databricks-logo-350x69.png) \n",
    "\n",
    "![IBM Spark Tech Center](https://s3.amazonaws.com/fluxcapacitor.com/img/ibm-spark-logo-197x148.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "023050ea-c894-4ffe-869f-2f27f03e524a"
    }
   },
   "source": [
    "# 1. Infrastructure and Tools\n",
    "### Docker\n",
    "Images, Containers\n",
    "\n",
    "Useful Docker Image:  [AWS + GPU + Docker + Tensorflow + Spark](https://github.com/fluxcapacitor/pipeline/wiki/AWS-GPU-TensorFlow-Docker)\n",
    "\n",
    "![Docker](http://pipeline.io/images/docker-logo-150x126.png)\n",
    "\n",
    "### Kubernetes\n",
    "Container Orchestration Across Clusters\n",
    "\n",
    "![Kubernetes](http://pipeline.io/images/kubernetes-logo-200x171.png) \n",
    "\n",
    "### Weavescope\n",
    "Kubernetes Cluster Visualization\n",
    "\n",
    "![PipelineIO Kubernetes](https://s3.amazonaws.com/fluxcapacitor.com/img/weavescope-pipelineio.png)\n",
    "\n",
    "### Jupyter Notebooks\n",
    "What We're Using Here for Everything!\n",
    "\n",
    "![Jupyter](http://pipeline.io/images/jupyter-logo-105x106.png) \n",
    "\n",
    "### Airflow\n",
    "Invoke Any Type of Workflow on Any Type of Schedule \n",
    "\n",
    "![Airflow](https://s3.amazonaws.com/fluxcapacitor.com/img/airflow-continuous-deployment.png)\n",
    " \n",
    "### Github\n",
    "Commit New Model to Github, Airflow Workflow Triggered for Continuous Deployment \n",
    "\t\n",
    "![Github Repo](https://s3.amazonaws.com/fluxcapacitor.com/img/pipeline-io-github-1000-stars.png)\n",
    "\n",
    "### DockerHub\n",
    "Maintains Docker Images \n",
    "\n",
    "![DockerHub](https://s3.amazonaws.com/fluxcapacitor.com/img/dockerhub-logo.png)\n",
    "\n",
    "### Continuous Deployment\n",
    "Not Just for Code, Also for ML/AI Models!\n",
    "\n",
    "### Canary Release\n",
    "Deploy and Compare New Model Alongside Existing\n",
    "\n",
    "### Metrics and Dashboards\n",
    "Not Just System Metrics, ML/AI Model Prediction Metrics\n",
    "\n",
    "NetflixOSS-based\n",
    "\n",
    "Prometheus\n",
    "\n",
    "Grafana\n",
    "\n",
    "Elasticsearch\n",
    "\n",
    "### Separate Cluster Concerns\n",
    "Training/Admin Cluster\n",
    "\n",
    "Prediction Cluster\n",
    "\n",
    "### Hybrid Cloud Deployment for eXtreme High Availability (XHA)\n",
    "AWS and Google Cloud\n",
    "\n",
    "### Apache Spark\n",
    "![Spark](http://pipeline.io/images/spark-logo-150x78.png) \n",
    "\n",
    "### Tensorflow + Tensorflow Serving\n",
    "![Tensorflow](http://pipeline.io/images/tensorflow-logo-150x128.png)\n",
    "\n",
    "![Architecture](https://s3.amazonaws.com/fluxcapacitor.com/img/tensorflow-serving-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ca731c95-6e39-4db8-a51c-f467a315616d"
    }
   },
   "source": [
    "# 2. Model Deployment Bundles\n",
    "### KeyValue\n",
    "ie. Recommendations\n",
    "\n",
    "In-memory: Redis, Memcache\n",
    "\n",
    "On-disk:  Cassandra, RocksDB\n",
    "\n",
    "First-class Servable in [Tensorflow Serving](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/servables)\n",
    "\n",
    "### PMML\n",
    "It's Useful and [Well-Supported](https://github.com/jpmml)\n",
    "\n",
    "Apple, Cisco, [Airbnb](http://nerds.airbnb.com/architecting-machine-learning-system-risk/), HomeAway, etc\n",
    "\n",
    "Please Don't Re-build It - Reduce Your Technical Debt!\n",
    "\n",
    "### Native Code\n",
    "Hand-coded (Python + Pickling)\n",
    "\n",
    "Generated Java Code from PMML\n",
    "\n",
    "### Tensorflow Model Exports\n",
    "[freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py): Combine Tensorflow Graph (Static) with Trained Weights (Checkpoints) into Single Deployable Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0dd485d-6cdf-4ccc-ba0c-60b2ed06bac7"
    }
   },
   "source": [
    "# 3. Model Deployments and Rollbacks\n",
    "### Mutable\n",
    "Each New Model is Deployed to Live, Running Container\n",
    "\n",
    "### Immutable\n",
    "Each New Model is a New Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizing Tensorflow Models for Serving\n",
    "## Python Scripts\n",
    "[optimize_graph_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py)\n",
    "\n",
    "[Pete Warden's Blog](https://petewarden.com/2016/12/30/rewriting-tensorflow-graphs-with-the-gtt/)\n",
    "\n",
    "[Graph Transform Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md)\n",
    "\n",
    "\n",
    "## Compile (Tensorflow 1.0+)\n",
    "### [XLA](https://www.tensorflow.org/versions/master/experimental/xla/) Compiler\n",
    "Compiles 3 graph operations (input, operation, output) into 1 operation\n",
    "\n",
    "Removes need for Tensorflow Runtime (20 MB is significant on tiny devices)\n",
    "\n",
    "Allows new backends for hardware-specific optimizations (better portability)\n",
    "\n",
    "### [tfcompile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/aot)\n",
    "Convert Graph into executable code\n",
    "\n",
    "### Compress/Distill Ensemble Models\n",
    "Convert ensembles or other complex models into smaller models\n",
    "\n",
    "Re-score training data with output of model being distilled \n",
    "\n",
    "Train smaller model to produce same output\n",
    "\n",
    "Output of smaller model learns more information than original label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimizing Serving Runtime Environment\n",
    "### Throughput\n",
    "**Option 1**:  Add more Tensorflow Serving servers behind **load balancer**\n",
    "\n",
    "**Option 2**:  Enable **request batching** in each Tensorflow Serving\n",
    "\n",
    "**Option Trade-offs**:  Higher Latency (bad) for Higher Throughput (good)\n",
    "\n",
    "```\n",
    "$TENSORFLOW_SERVING_HOME/bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server \n",
    "--port=9000 \n",
    "--model_name=tensorflow_minimal \n",
    "--model_base_path=/root/models/tensorflow_minimal/export\n",
    "--enable_batching=true\n",
    "--max_batch_size=1000000\n",
    "--batch_timeout_micros=10000\n",
    "--max_enqueued_batches=1000000\n",
    "```\n",
    "![Request Batching](https://s3.amazonaws.com/fluxcapacitor.com/img/matrix-multiply-request-batching-handwritten.png)\n",
    "\n",
    "### Latency\n",
    "The deeper the model, the longer the latency\n",
    "\n",
    "Start inference in parallel where possible (ie. user inference in parallel with item inference)\n",
    "\n",
    "Pre-load common inputs from database (ie. user attributes, item attributes) \n",
    "\n",
    "Pre-compute/partial-compute common inputs (ie. popular word embeddings)\n",
    "\n",
    "### Memory\n",
    "Word embeddings are **huge**!\n",
    "\n",
    "Use **hashId** for each word\n",
    "\n",
    "Off-load embedding matrices to **parameter server** and share between serving servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Demos!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy Tensorflow AI Model (Simple Model, Immutable Deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Tensorflow AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make things wide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def=None, width=1200, height=800, max_const_size=32, ungroup_gradients=False):\n",
    "    if not graph_def:\n",
    "        graph_def = tf.get_default_graph().as_graph_def()\n",
    "        \n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    data = str(strip_def)\n",
    "    if ungroup_gradients:\n",
    "        data = data.replace('\"gradients/', '\"b_')\n",
    "        #print(data)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(data), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:{}px;height:{}px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(width, height, code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If this errors out, increment the `export_version` variable, restart the Kernel, and re-run\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\"batch_size\", 10, \"The batch size to train\")\n",
    "flags.DEFINE_integer(\"epoch_number\", 10, \"Number of epochs to run trainer\")\n",
    "flags.DEFINE_integer(\"steps_to_validate\", 1,\n",
    "                     \"Steps to validate and print loss\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoint/\",\n",
    "                    \"indicates the checkpoint dirctory\")\n",
    "#flags.DEFINE_string(\"model_path\", \"./model/\", \"The export path of the model\")\n",
    "flags.DEFINE_string(\"model_path\", \"/root/pipeline/prediction.ml/tensorflow/models/tensorflow_minimal/export/\", \"The export path of the model\")\n",
    "flags.DEFINE_integer(\"export_version\", 25, \"The version number of the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-f107a23917ee>:25 in main.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Continue training from the model ./checkpoint/checkpoint.ckpt\n",
      "save/Const:0\n",
      "save/restore_all\n",
      "[0.0026826858520507812] Epoch: 0\n",
      "[0.13412237167358398] Epoch: 1\n",
      "[0.14891576766967773] Epoch: 2\n",
      "[0.11788558959960938] Epoch: 3\n",
      "[0.11348485946655273] Epoch: 4\n",
      "[0.17435503005981445] Epoch: 5\n",
      "[0.1426706314086914] Epoch: 6\n",
      "[0.14076995849609375] Epoch: 7\n",
      "[0.17664051055908203] Epoch: 8\n",
      "[0.15097856521606445] Epoch: 9\n",
      "The model of w: 0.5000000596046448, b: 0.5000000596046448\n",
      "Exporting trained model to /root/pipeline/prediction.ml/tensorflow/models/tensorflow_minimal/export/\n",
      "INFO:tensorflow:/root/pipeline/prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025-tmp/export is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Done exporting!\n"
     ]
    }
   ],
   "source": [
    "# If this errors out, increment the `export_version` variable, restart the Kernel, and re-run\n",
    "\n",
    "def main():\n",
    "  # Define training data\n",
    "  x = np.ones(FLAGS.batch_size)\n",
    "  y = np.ones(FLAGS.batch_size)\n",
    "\n",
    "  # Define the model\n",
    "  X = tf.placeholder(tf.float32, shape=[None], name=\"X\")\n",
    "  Y = tf.placeholder(tf.float32, shape=[None], name=\"yhat\")\n",
    "  w = tf.Variable(1.0, name=\"weight\")\n",
    "  b = tf.Variable(1.0, name=\"bias\")\n",
    "  loss = tf.square(Y - tf.mul(X, w) - b)\n",
    "  train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  predict_op  = tf.mul(X, w) + b\n",
    "\n",
    "  saver = tf.train.Saver()\n",
    "  checkpoint_dir = FLAGS.checkpoint_dir\n",
    "  checkpoint_file = checkpoint_dir + \"/checkpoint.ckpt\"\n",
    "  if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "  # Start the session\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      print(\"Continue training from the model {}\".format(ckpt.model_checkpoint_path))\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    saver_def = saver.as_saver_def()\n",
    "    print(saver_def.filename_tensor_name)\n",
    "    print(saver_def.restore_op_name)\n",
    "\n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(FLAGS.epoch_number):\n",
    "      sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "\n",
    "      # Start validating\n",
    "      if epoch % FLAGS.steps_to_validate == 0:\n",
    "        end_time = time.time()\n",
    "        print(\"[{}] Epoch: {}\".format(end_time - start_time, epoch))\n",
    "\n",
    "        saver.save(sess, checkpoint_file)\n",
    "        tf.train.write_graph(sess.graph_def, checkpoint_dir, 'trained_model.pb', as_text=False)\n",
    "        tf.train.write_graph(sess.graph_def, checkpoint_dir, 'trained_model.txt', as_text=True)\n",
    "\n",
    "        start_time = end_time\n",
    "\n",
    "    # Print model variables\n",
    "    w_value, b_value = sess.run([w, b])\n",
    "    print(\"The model of w: {}, b: {}\".format(w_value, b_value))\n",
    "\n",
    "    # Export the model\n",
    "    print(\"Exporting trained model to {}\".format(FLAGS.model_path))\n",
    "    model_exporter = exporter.Exporter(saver)\n",
    "    model_exporter.init(\n",
    "      sess.graph.as_graph_def(),\n",
    "      named_graph_signatures={\n",
    "        'inputs': exporter.generic_signature({\"features\": X}),\n",
    "        'outputs': exporter.generic_signature({\"prediction\": predict_op})\n",
    "      })\n",
    "    model_exporter.export(FLAGS.model_path, tf.constant(FLAGS.export_version), sess)\n",
    "    print('Done exporting!')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:800px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.43761753188779573&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;yhat&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weight/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weight&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weight/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;weight&quot;\\n  input: &quot;weight/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weight&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weight/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;weight&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weight&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bias/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bias&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;bias&quot;\\n  input: &quot;bias/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;weight/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;yhat&quot;\\n  input: &quot;Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;sub&quot;\\n  input: &quot;bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/Fill&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square_grad/mul/x&quot;\\n  input: &quot;sub_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/Square_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub_1_grad/Shape&quot;\\n  input: &quot;gradients/sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_1_grad/Sum&quot;\\n  input: &quot;gradients/sub_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub_1_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_1_grad/Neg&quot;\\n  input: &quot;gradients/sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub_1_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_1_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;yhat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/sub_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Sum&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/sub_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;X&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Mul_grad/Shape&quot;\\n  input: &quot;gradients/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  input: &quot;weight/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Mul_grad/mul&quot;\\n  input: &quot;gradients/Mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Mul_grad/Sum&quot;\\n  input: &quot;gradients/Mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Mul_grad/mul_1&quot;\\n  input: &quot;gradients/Mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_weight/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;weight&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/Mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weight&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/sub_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_weight/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;weight/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Mul_1&quot;\\n  input: &quot;bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;bias&quot;\\n        string_val: &quot;weight&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;bias&quot;\\n  input: &quot;weight&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;weight&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_1/tensor_names&quot;\\n  input: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;weight&quot;\\n  input: &quot;save/RestoreV2_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weight&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^weight/Assign&quot;\\n  input: &quot;^bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 25\\n      }\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.43761753188779573&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit and Deploy New Tensorflow AI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commit Model to Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 19 22:39 00000001\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 21 20:09 00000005\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 21 21:24 00000007\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 21 21:30 00000009\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 21 21:35 00000011\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 21 21:36 00000013\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 21 22:02 00000015\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 21 22:18 00000017\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 22 00:00 00000019\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 22 05:15 00000021\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 22 06:49 00000025\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /root/pipeline/prediction.ml/tensorflow/models/tensorflow_minimal/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\r\n",
      "-rw-r--r-- 1 root root   241 Jan 22 06:49 checkpoint\r\n",
      "-rw-r--r-- 1 root root     8 Jan 22 06:49 export.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 root root   142 Jan 22 06:49 export.index\r\n",
      "-rw-r--r-- 1 root root 19394 Jan 22 06:49 export.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /root/pipeline/prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is up-to-date with 'origin/master'.\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "\r\n",
      "\t\u001b[31mmodified:   StartupML - San Francisco - Jan 20, 2017.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\r\n",
      "\t\u001b[31m../../prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!git add --all /root/pipeline/prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master eb055c2] updated tensorflow model\r\n",
      " 4 files changed, 2 insertions(+)\r\n",
      " create mode 100644 prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025/checkpoint\r\n",
      " create mode 100644 prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025/export.data-00000-of-00001\r\n",
      " create mode 100644 prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025/export.index\r\n",
      " create mode 100644 prediction.ml/tensorflow/models/tensorflow_minimal/export/00000025/export.meta\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"updated tensorflow model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If this fails with \"Permission denied\", use terminal within jupyter to manually `git push`\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Airflow Workflow](http://demo.pipeline.io:8080/admin/) Deploys New Model through Github Post-Commit [Webhook](https://github.com/fluxcapacitor/pipeline/blob/master/scheduler.ml/airflow/github_webhook) to Triggers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "html = '<iframe width=100% height=500px src=\"http://demo.pipeline.io:8080/admin\">'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy Spark ML Model (Airbnb Model, Mutable Deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Out Spark Training Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes [CLI](https://github.com/fluxcapacitor/pipeline/wiki/Kubernetes-Commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!kubectl scale --context=awsdemo --replicas=2 rc spark-worker-2-0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!kubectl get pod --context=awsdemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weavescope Kubernetes [AWS Cluster](http://kubernetes-aws.demo.pipeline.io) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "html = '<iframe width=100% height=500px src=\"http://kubernetes-aws.demo.pipeline.io\">'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate PMML from Spark ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0**: Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\").option(\"header\", \"true\") \\\n",
    "  .load(\"s3a://datapalooza/airbnb/airbnb.csv.bz2\")\n",
    "\n",
    "df.registerTempTable(\"df\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Clean, Filter, and Summarize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_filtered = df.filter(\"price >= 50 AND price <= 750 AND bathrooms > 0.0 AND bedrooms is not null\")\n",
    "\n",
    "df_filtered.registerTempTable(\"df_filtered\")\n",
    "\n",
    "df_final = spark.sql(\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        cast(price as double) as price,\n",
    "        cast(bathrooms as double) as bathrooms,\n",
    "        cast(bedrooms as double) as bedrooms,\n",
    "        room_type,\n",
    "        host_is_super_host,\n",
    "        cancellation_policy,\n",
    "        cast(case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as double) as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        cast(case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as double) as number_of_reviews,\n",
    "        cast(case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as double) as extra_people,\n",
    "        instant_bookable,\n",
    "        cast(case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as double) as cleaning_fee,\n",
    "        cast(case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as double) as review_scores_rating,\n",
    "        cast(case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as double) as square_feet\n",
    "    from df_filtered\n",
    "\"\"\").persist()\n",
    "\n",
    "df_final.registerTempTable(\"df_final\")\n",
    "\n",
    "df_final.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_final.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most popular cities\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as ct,\n",
    "        avg(price) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df_final\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Most expensive popular cities\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as ct,\n",
    "        avg(price) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df_final\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"ct > 25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Define Continous and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_features = [\"bathrooms\", \\\n",
    "                       \"bedrooms\", \\\n",
    "                       \"security_deposit\", \\\n",
    "                       \"cleaning_fee\", \\\n",
    "                       \"extra_people\", \\\n",
    "                       \"number_of_reviews\", \\\n",
    "                       \"square_feet\", \\\n",
    "                       \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\", \\\n",
    "                        \"host_is_super_host\", \\\n",
    "                        \"cancellation_policy\", \\\n",
    "                        \"instant_bookable\", \\\n",
    "                        \"state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Split Data into Training and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[training_dataset, validation_dataset] = df_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "continuous_feature_assembler = VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\n",
    "\n",
    "continuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\", \\\n",
    "                                           withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_feature_indexers = [StringIndexer(inputCol=x, \\\n",
    "                                              outputCol=\"{}_index\".format(x)) \\\n",
    "                                for x in categorical_features]\n",
    "\n",
    "categorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), \\\n",
    "                                                      outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) \\\n",
    "                                        for x in categorical_feature_indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6**: Assemble our Features and Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols_lr = [x.getOutputCol() \\\n",
    "                   for x in categorical_feature_one_hot_encoders]\n",
    "feature_cols_lr.append(\"scaled_continuous_features\")\n",
    "\n",
    "feature_assembler_lr = VectorAssembler(inputCols=feature_cols_lr, \\\n",
    "                                       outputCol=\"features_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7**: Train a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(featuresCol=\"features_lr\", \\\n",
    "                                     labelCol=\"price\", \\\n",
    "                                     predictionCol=\"price_prediction\", \\\n",
    "                                     maxIter=10, \\\n",
    "                                     regParam=0.3, \\\n",
    "                                     elasticNetParam=0.8)\n",
    "\n",
    "estimators_lr = \\\n",
    "  [continuous_feature_assembler, continuous_feature_scaler] \\\n",
    "  + categorical_feature_indexers + categorical_feature_one_hot_encoders \\\n",
    "  + [feature_assembler_lr] + [linear_regression]\n",
    "\n",
    "pipeline = Pipeline(stages=estimators_lr)\n",
    "\n",
    "pipeline_model = pipeline.fit(training_dataset)\n",
    "\n",
    "print(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8**:  Convert PipelineModel to PMML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from jpmml import toPMMLBytes\n",
    "\n",
    "pmmlBytes = toPMMLBytes(spark, training_dataset, pipeline_model)\n",
    "\n",
    "print(pmmlBytes.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push PMML to Live, Running Spark ML Model Server (Mutable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "update_url = 'http://prediction-pmml-aws.demo.pipeline.io/update-pmml/pmml_airbnb'\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'application/xml'\n",
    "\n",
    "req = urllib.request.Request(update_url, \\\n",
    "                             headers=update_headers, \\\n",
    "                             data=pmmlBytes)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.status) # Should return Http Status 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "update_url = 'http://prediction-pmml-gcp.demo.pipeline.io/update-pmml/pmml_airbnb'\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'application/xml'\n",
    "\n",
    "req = urllib.request.Request(update_url, \\\n",
    "                             headers=update_headers, \\\n",
    "                             data=pmmlBytes)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.status) # Should return Http Status 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "evaluate_url = 'http://prediction-pmml-aws.demo.pipeline.io/evaluate-pmml/pmml_airbnb'\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "\n",
    "input_params = '{\"bathrooms\":2.0, \\\n",
    "                 \"bedrooms\":2.0, \\\n",
    "                 \"security_deposit\":175.00, \\\n",
    "                 \"cleaning_fee\":25.0, \\\n",
    "                 \"extra_people\":1.0, \\\n",
    "                 \"number_of_reviews\": 2.0, \\\n",
    "                 \"square_feet\": 250.0, \\\n",
    "                 \"review_scores_rating\": 2.0, \\\n",
    "                 \"room_type\": \"Entire home/apt\", \\\n",
    "                 \"host_is_super_host\": \"0.0\", \\\n",
    "                 \"cancellation_policy\": \"flexible\", \\\n",
    "                 \"instant_bookable\": \"1.0\", \\\n",
    "                 \"state\": \"CA\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(evaluate_url, \\\n",
    "                             headers=evaluate_headers, \\\n",
    "                             data=encoded_input_params)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "evaluate_url = 'http://prediction-pmml-aws.demo.pipeline.io/evaluate-pmml/pmml_airbnb'\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "\n",
    "input_params = '{\"bathrooms\":2.0, \\\n",
    "                 \"bedrooms\":2.0, \\\n",
    "                 \"security_deposit\":175.00, \\\n",
    "                 \"cleaning_fee\":25.0, \\\n",
    "                 \"extra_people\":1.0, \\\n",
    "                 \"number_of_reviews\": 2.0, \\\n",
    "                 \"square_feet\": 250.0, \\\n",
    "                 \"review_scores_rating\": 2.0, \\\n",
    "                 \"room_type\": \"Entire home/apt\", \\\n",
    "                 \"host_is_super_host\": \"0.0\", \\\n",
    "                 \"cancellation_policy\": \"flexible\", \\\n",
    "                 \"instant_bookable\": \"1.0\", \\\n",
    "                 \"state\": \"CA\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(evaluate_url, \\\n",
    "                             headers=evaluate_headers, \\\n",
    "                             data=encoded_input_params)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "evaluate_url = 'http://prediction-pmml-gcp.demo.pipeline.io/evaluate-pmml/pmml_airbnb'\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "\n",
    "input_params = '{\"bathrooms\":2.0, \\\n",
    "                 \"bedrooms\":2.0, \\\n",
    "                 \"security_deposit\":175.00, \\\n",
    "                 \"cleaning_fee\":25.0, \\\n",
    "                 \"extra_people\":1.0, \\\n",
    "                 \"number_of_reviews\": 2.0, \\\n",
    "                 \"square_feet\": 250.0, \\\n",
    "                 \"review_scores_rating\": 2.0, \\\n",
    "                 \"room_type\": \"Entire home/apt\", \\\n",
    "                 \"host_is_super_host\": \"0.0\", \\\n",
    "                 \"cancellation_policy\": \"flexible\", \\\n",
    "                 \"instant_bookable\": \"1.0\", \\\n",
    "                 \"state\": \"CA\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(evaluate_url, \\\n",
    "                             headers=evaluate_headers, \\\n",
    "                             data=encoded_input_params)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Java Model (Simple Model, Mutable Deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "sourceBytes = '                                                      \\n\\\n",
    "  private String str;                                                \\n\\\n",
    "                                                                     \\n\\\n",
    "  public void initialize(Map<String, Object> args) {                 \\n\\\n",
    "  }                                                                  \\n\\\n",
    "                                                                     \\n\\\n",
    "  public Object predict(Map<String, Object> inputs) {                \\n\\\n",
    "      String id = (String)inputs.get(\"id\");                          \\n\\\n",
    "                                                                     \\n\\\n",
    "      return id.equals(\"21619\");                                     \\n\\\n",
    "  }                                                                  \\n\\\n",
    "'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_equals'\n",
    "update_url = 'http://prediction-codegen-aws.demo.pipeline.io/update-codegen/%s/' % name\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'text/plain'\n",
    "\n",
    "req = request.Request(\"%s\" % update_url, headers=update_headers, data=sourceBytes)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "generated_code = resp.read()\n",
    "print(generated_code.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_equals'\n",
    "update_url = 'http://prediction-codegen-gcp.demo.pipeline.io/update-codegen/%s/' % name\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'text/plain'\n",
    "\n",
    "req = request.Request(\"%s\" % update_url, headers=update_headers, data=sourceBytes)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "generated_code = resp.read()\n",
    "print(generated_code.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_equals'\n",
    "evaluate_url = 'http://prediction-codegen-aws.demo.pipeline.io/evaluate-codegen/%s' % name\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "input_params = '{\"id\":\"21618\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = request.Request(evaluate_url, headers=evaluate_headers, data=encoded_input_params)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "print(resp.read()) # Should return true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_equals'\n",
    "evaluate_url = 'http://prediction-codegen-aws.demo.pipeline.io/evaluate-codegen/%s' % name\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "input_params = '{\"id\":\"21619\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = request.Request(evaluate_url, headers=evaluate_headers, data=encoded_input_params)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "print(resp.read()) # Should return false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Java Model (HttpClient Model, Mutable Deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "sourceBytes = '                                                         \\n\\\n",
    "  public Map<String, Object> data = new HashMap<String, Object>();      \\n\\\n",
    "                                                                        \\n\\\n",
    "  public void initialize(Map<String, Object> args) {                    \\n\\\n",
    "    data.put(\"url\", \"http://demo.pipeline.io:9040/prediction/\");        \\n\\\n",
    "  }                                                                     \\n\\\n",
    "                                                                        \\n\\\n",
    "  public Object predict(Map<String, Object> inputs) {                   \\n\\\n",
    "    try {                                                               \\n\\\n",
    "      String userId = (String)inputs.get(\"userId\");                     \\n\\\n",
    "      String itemId = (String)inputs.get(\"itemId\");                     \\n\\\n",
    "      String url = data.get(\"url\") + \"/\" + userId + \"/\" + itemId;       \\n\\\n",
    "                                                                        \\n\\\n",
    "      return org.apache.http.client.fluent.Request                      \\n\\\n",
    "        .Get(url)                                                       \\n\\\n",
    "        .execute()                                                      \\n\\\n",
    "        .returnContent();                                               \\n\\\n",
    "                                                                        \\n\\\n",
    "    } catch(Exception exc) {                                            \\n\\\n",
    "      System.out.println(exc);                                          \\n\\\n",
    "      throw exc;                                                        \\n\\\n",
    "    }                                                                   \\n\\\n",
    "  }                                                                     \\n\\\n",
    "'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_httpclient'\n",
    "# Note:  Must have trailing '/'\n",
    "update_url = 'http://prediction-codegen-aws.demo.pipeline.io/update-codegen/%s/' % name\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'text/plain'\n",
    "\n",
    "req = request.Request(\"%s\" % update_url, headers=update_headers, data=sourceBytes)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "generated_code = resp.read()\n",
    "print(generated_code.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_httpclient'\n",
    "# Note:  Must have trailing '/'\n",
    "update_url = 'http://prediction-codegen-gcp.demo.pipeline.io/update-codegen/%s/' % name\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'text/plain'\n",
    "\n",
    "req = request.Request(\"%s\" % update_url, headers=update_headers, data=sourceBytes)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "generated_code = resp.read()\n",
    "print(generated_code.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_httpclient'\n",
    "evaluate_url = 'http://prediction-codegen-aws.demo.pipeline.io/evaluate-codegen/%s' % name\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "input_params = '{\"userId\":\"21619\", \"itemId\":\"10006\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = request.Request(evaluate_url, headers=evaluate_headers, data=encoded_input_params)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "print(resp.read()) # Should return float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "name = 'codegen_httpclient'\n",
    "evaluate_url = 'http://prediction-codegen-gcp.demo.pipeline.io/evaluate-codegen/%s' % name\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "input_params = '{\"userId\":\"21619\", \"itemId\":\"10006\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = request.Request(evaluate_url, headers=evaluate_headers, data=encoded_input_params)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "print(resp.read()) # Should return float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test and Compare Cloud Providers (AWS and Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Performance Across Cloud Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetflixOSS Services [Dashboard](http://hystrix.demo.pipeline.io/hystrix-dashboard/monitor/monitor.html?streams=%5B%7B%22name%22%3A%22Predictions%20-%20AWS%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-aws.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%2C%7B%22name%22%3A%22Predictions%20-%20GCP%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-gcp.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%5D) (Hystrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "html = '<iframe width=100% height=500px src=\"http://hystrix.demo.pipeline.io/hystrix-dashboard/monitor/monitor.html?streams=%5B%7B%22name%22%3A%22Predictions%20-%20AWS%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-aws.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%2C%7B%22name%22%3A%22Predictions%20-%20GCP%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-gcp.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%5D\">'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Load Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run JMeter Tests from Local Laptop (Limited by Laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Headless JMeter Tests from Training Clusters in Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark ML - PMML - Airbnb\n",
    "!kubectl create --context=awsdemo -f /root/pipeline/loadtest.ml/loadtest-aws-airbnb-rc.yaml\n",
    "!kubectl create --context=gcpdemo -f /root/pipeline/loadtest.ml/loadtest-aws-airbnb-rc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Codegen - Java - Simple\n",
    "!kubectl create --context=awsdemo -f /root/pipeline/loadtest.ml/loadtest-aws-equals-rc.yaml\n",
    "!kubectl create --context=gcpdemo -f /root/pipeline/loadtest.ml/loadtest-aws-equals-rc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow AI - Tensorflow Serving - Simple \n",
    "!kubectl create --context=awsdemo -f /root/pipeline/loadtest.ml/loadtest-aws-minimal-rc.yaml\n",
    "!kubectl create --context=gcpdemo -f /root/pipeline/loadtest.ml/loadtest-aws-minimal-rc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Load Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!kubectl delete --context=awsdemo rc loadtest-aws-airbnb\n",
    "!kubectl delete --context=gcpdemo rc loadtest-aws-airbnb\n",
    "!kubectl delete --context=awsdemo rc loadtest-aws-equals\n",
    "!kubectl delete --context=gcpdemo rc loadtest-aws-equals\n",
    "!kubectl delete --context=awsdemo rc loadtest-aws-minimal\n",
    "!kubectl delete --context=gcpdemo rc loadtest-aws-minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Deploy Tensorflow AI (Simple Model, Immutable Deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes [CLI](https://github.com/fluxcapacitor/pipeline/wiki/Kubernetes-Commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!kubectl rolling-update prediction-tensorflow --context=awsdemo --image-pull-policy=Always --image=fluxcapacitor/prediction-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl get pod --context=awsdemo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl rolling-update prediction-tensorflow --context=gcpdemo --image-pull-policy=Always --image=fluxcapacitor/prediction-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl get pod --context=gcpdemo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Q&A"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "0e6bc8b6-3cc5-496c-a34b-11d192afa91e": {
     "id": "0e6bc8b6-3cc5-496c-a34b-11d192afa91e",
     "prev": "aca594eb-585a-4405-b367-08bbe36b7e46",
     "regions": {
      "42084308-8543-4a24-918b-e452e3cba2e8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4d6901a2-06fc-4cc2-9168-0b4dd20436c3",
        "part": "whole"
       },
       "id": "42084308-8543-4a24-918b-e452e3cba2e8"
      }
     }
    },
    "10e8bf08-9a8f-4615-824e-4c015cb48051": {
     "id": "10e8bf08-9a8f-4615-824e-4c015cb48051",
     "prev": "6495f7a9-692a-452e-8a08-d34afb46a92a",
     "regions": {
      "f8860415-d84f-453f-b095-5a68337325ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "de258cfb-7c21-40cf-b3ee-04228689eab0",
        "part": "whole"
       },
       "id": "f8860415-d84f-453f-b095-5a68337325ac"
      }
     }
    },
    "1ce58837-1471-4d9e-90bd-56c2482444bf": {
     "id": "1ce58837-1471-4d9e-90bd-56c2482444bf",
     "prev": "c121e0e8-a5cb-4e28-a578-f4246b9bc82e",
     "regions": {
      "53848919-ae68-4e10-b72b-40191b3af7bb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "023050ea-c894-4ffe-869f-2f27f03e524a",
        "part": "whole"
       },
       "id": "53848919-ae68-4e10-b72b-40191b3af7bb"
      }
     }
    },
    "32b2322c-9925-4e38-a692-05f3544576c3": {
     "id": "32b2322c-9925-4e38-a692-05f3544576c3",
     "prev": "10e8bf08-9a8f-4615-824e-4c015cb48051",
     "regions": {
      "251138f3-82aa-49e6-8e01-621ca02eb8fe": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b08fcd17-3ecd-4e73-a494-570728a3e129",
        "part": "whole"
       },
       "id": "251138f3-82aa-49e6-8e01-621ca02eb8fe"
      }
     }
    },
    "5e9dc412-fd7e-4a17-a5bd-1ac44cc9cdc6": {
     "id": "5e9dc412-fd7e-4a17-a5bd-1ac44cc9cdc6",
     "prev": "ae6895f7-9ad3-4ac7-a3a7-b23dddc95276",
     "regions": {
      "8a35ac4a-1620-48f4-90ec-3a9c5c577271": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "34e2d7fe-478c-4f42-aab6-0e17e4424576",
        "part": "whole"
       },
       "id": "8a35ac4a-1620-48f4-90ec-3a9c5c577271"
      }
     }
    },
    "6495f7a9-692a-452e-8a08-d34afb46a92a": {
     "id": "6495f7a9-692a-452e-8a08-d34afb46a92a",
     "prev": "5e9dc412-fd7e-4a17-a5bd-1ac44cc9cdc6",
     "regions": {
      "8c118309-a5f7-4970-8997-fa6d31be0cf9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a0dd485d-6cdf-4ccc-ba0c-60b2ed06bac7",
        "part": "whole"
       },
       "id": "8c118309-a5f7-4970-8997-fa6d31be0cf9"
      }
     }
    },
    "6a260c4b-6faa-456e-9680-dd7cbebe0933": {
     "id": "6a260c4b-6faa-456e-9680-dd7cbebe0933",
     "prev": "32b2322c-9925-4e38-a692-05f3544576c3",
     "regions": {
      "f65b7ebd-c02e-4088-95dc-8c90a5dffd09": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f5525416-d535-4377-bdf8-b91463fce353",
        "part": "whole"
       },
       "id": "f65b7ebd-c02e-4088-95dc-8c90a5dffd09"
      }
     }
    },
    "9c2c1cd9-1811-417c-8a27-edfef797f18b": {
     "id": "9c2c1cd9-1811-417c-8a27-edfef797f18b",
     "prev": "ee9cdb3f-6fb6-4497-ba5d-0fcddf43b8fd",
     "regions": {
      "867affd3-66d0-4835-bf85-a7b0e596d78e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ca731c95-6e39-4db8-a51c-f467a315616d",
        "part": "whole"
       },
       "id": "867affd3-66d0-4835-bf85-a7b0e596d78e"
      }
     }
    },
    "aca594eb-585a-4405-b367-08bbe36b7e46": {
     "id": "aca594eb-585a-4405-b367-08bbe36b7e46",
     "prev": "9c2c1cd9-1811-417c-8a27-edfef797f18b",
     "regions": {
      "22168c7b-acfd-4487-b57e-97aa88e5f3e8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ca8cd2ab-3693-4b40-90c0-d4b6adf8c20e",
        "part": "whole"
       },
       "id": "22168c7b-acfd-4487-b57e-97aa88e5f3e8"
      }
     }
    },
    "ae6895f7-9ad3-4ac7-a3a7-b23dddc95276": {
     "id": "ae6895f7-9ad3-4ac7-a3a7-b23dddc95276",
     "prev": "0e6bc8b6-3cc5-496c-a34b-11d192afa91e",
     "regions": {
      "1bf1c48a-1011-4992-8eaf-ef8e89a3d9c9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bd5629b7-e48e-42b7-ae26-38ed501f0246",
        "part": "whole"
       },
       "id": "1bf1c48a-1011-4992-8eaf-ef8e89a3d9c9"
      }
     },
     "theme": null
    },
    "c121e0e8-a5cb-4e28-a578-f4246b9bc82e": {
     "id": "c121e0e8-a5cb-4e28-a578-f4246b9bc82e",
     "prev": null,
     "regions": {
      "bf22675f-7d52-4ded-b588-7ddc477b17d6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fcb54439-1c2c-46ce-9d92-e77df43393db",
        "part": "whole"
       },
       "id": "bf22675f-7d52-4ded-b588-7ddc477b17d6"
      }
     }
    },
    "ee9cdb3f-6fb6-4497-ba5d-0fcddf43b8fd": {
     "id": "ee9cdb3f-6fb6-4497-ba5d-0fcddf43b8fd",
     "prev": "ff384e75-fefa-4966-a506-a12d5c4c770f",
     "regions": {
      "8d7dad4a-1d39-4ea7-8e9d-938bb9e7c00f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e4189884-f8d1-4d9a-827d-5ffe9bf7b21b",
        "part": "whole"
       },
       "id": "8d7dad4a-1d39-4ea7-8e9d-938bb9e7c00f"
      }
     }
    },
    "ff384e75-fefa-4966-a506-a12d5c4c770f": {
     "id": "ff384e75-fefa-4966-a506-a12d5c4c770f",
     "prev": "1ce58837-1471-4d9e-90bd-56c2482444bf",
     "regions": {
      "dd92502b-768b-431a-a5c3-6601da433340": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5dc02aa0-da66-4cce-b6f0-4e2253f8743c",
        "part": "whole"
       },
       "id": "dd92502b-768b-431a-a5c3-6601da433340"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
