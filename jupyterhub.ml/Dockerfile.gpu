#FROM fluxcapacitor/package-tensorflow-7a7fe93-4c0052d-full-gpu:master
FROM fluxcapacitor/package-ubuntu-16.04:master

WORKDIR /root

# Based on the following:  https://github.com/jupyterhub/jupyterhub/blob/master/Dockerfile

# install nodejs, utf8 locale
ENV DEBIAN_FRONTEND noninteractive

RUN apt-get -y update && \
    apt-get -y upgrade && \
    apt-get -y install npm nodejs nodejs-legacy wget locales git

# libav-tools for matplotlib anim
RUN apt-get update && \
    apt-get install -y --no-install-recommends libav-tools && \
    apt-get clean
#    rm -rf /var/lib/apt/lists/*

RUN wget -q https://repo.continuum.io/miniconda/Miniconda3-4.3.21-Linux-x86_64.sh -O /tmp/miniconda.sh  && \
    echo 'c1c15d3baba15bf50293ae963abef853 */tmp/miniconda.sh' | md5sum -c - && \
    bash /tmp/miniconda.sh -f -b -p /opt/conda && \
    /opt/conda/bin/conda install --yes python=3.6 sqlalchemy tornado jinja2 traitlets requests pip && \
    /opt/conda/bin/pip install --upgrade pip && \
    rm /tmp/miniconda.sh

ENV \
  PATH=/opt/conda/bin:$PATH

# Install JupyterHub dependencies
RUN npm install -g configurable-http-proxy && rm -rf ~/.npm

RUN \
  pip install jupyterhub==0.7.2 \
  && pip install ipykernel==4.6.1 \
  && pip install notebook==5.0.0 \
  && pip install ipywidgets==6.0.0 \
  && pip install findspark==1.1.0 \
  && pip install tabulate==0.7.7

#RUN \
#  pip install jupyterlab==0.19.0 \
#  && pip install jupyterlab_widgets==0.6.15 \
#  && pip install widgetslabextension==0.1.0

#RUN \
#  jupyter labextension install --sys-prefix --py jupyterlab_widgets \
#  && jupyter labextension enable --sys-prefix --py jupyterlab_widgets \
#  && jupyter serverextension enable --py jupyterlab --sys-prefix

# Install non-secure dummyauthenticator for jupyterhub (dev purposes only)
RUN \
  pip install jupyterhub-dummyauthenticator==0.3.1 \
              oauthenticator==0.5.1 \
              jupyterhub-simplespawner==0.1

RUN \
  pip install nbconvert==5.2.1 \
  && conda install --yes graphviz==2.38.0 \
  && conda install --yes seaborn==0.7.1 \
  && conda install --yes dill==0.2.5

RUN \
  mkdir -p /root/tensorboard

RUN \
  mkdir -p /root/models

RUN \
  mkdir -p /root/logs

# Apache2
RUN \
  apt-get install -y apache2

RUN \
  a2enmod proxy \
  && a2enmod proxy_http \
  && a2dissite 000-default

RUN \
  mv /var/www/html /var/www/html.orig

RUN \
  mv /etc/apache2/apache2.conf /etc/apache2/apache2.conf.orig

# All paths (dirs, not files) up to and including /root must have +x permissions.
# It's just the way linux works.  Don't fight it.
# http://askubuntu.com/questions/537032/how-to-configure-apache2-with-symbolic-links-in-var-www
RUN \
  chmod a+x /root

COPY config/apache2/ config/apache2/
COPY html/ html/

# lib & notebooks provided via Docker or Kubernetes Volume Mount
# from the github.com/fluxcapacitor/source.ml repo -> /root/pipeline/

COPY config/jupyterhub/ config/jupyterhub/
COPY profiles/ /root/.ipython/
COPY img/ img/

RUN \
  pip install git+https://github.com/jupyterhub/kubespawner

RUN \
  conda config --add channels r && \
  conda install --quiet --yes \
    'r-base=3.3.2' \
    'r-irkernel=0.7*' \
    'r-plyr=1.8*' \
    'r-devtools=1.12*' \
    'r-tidyverse=1.0*' \
    'r-shiny=0.14*' \
    'r-rmarkdown=1.2*' \
    'r-forecast=7.3*' \
    'r-rsqlite=1.1*' \
    'r-reshape2=1.4*' \
    'r-nycflights13=0.2*' \
    'r-caret=6.0*' \
    'r-rcurl=1.95*' \
    'r-crayon=1.3*' \
    'r-randomforest=4.6*' && conda clean -tipsy

COPY share/ share/
RUN \
  mv /opt/conda/share/jupyter/hub/templates/login.html /opt/conda/share/jupyter/hub/templates/login.html.orig \
  && mv /opt/conda/share/jupyter/hub/templates/page.html /opt/conda/share/jupyter/hub/templates/page.html.orig \
  && cd /opt/conda/share/jupyter/hub/templates/ \
  && ln -s ~/share/jupyter/hub/templates/login.html \
  && ln -s ~/share/jupyter/hub/templates/page.html 

COPY notebook/ notebook/
RUN \
  mv /opt/conda/lib/python3.6/site-packages/notebook/static/custom/ /opt/conda/lib/python3.6/site-packages/notebook/static/custom.orig \
  && cd /opt/conda/lib/python3.6/site-packages/notebook/static/ \
  && ln -s ~/notebook/static/custom

RUN \
  mv /opt/conda/lib/python3.6/site-packages/notebook/templates/tree.html /opt/conda/lib/python3.6/site-packages/notebook/templates/tree.html.orig \
  && cd /opt/conda/lib/python3.6/site-packages/notebook/templates/ \
  && ln -s ~/notebook/templates/tree.html

COPY oauthenticator/ oauthenticator/
RUN \
  cd oauthenticator \
  && pip install -e .

RUN \
  conda install -y gcc \
  && pip install xgboost

RUN \
  pip install sklearn_pandas==1.4.0 \
  && pip install git+https://github.com/jpmml/sklearn2pmml.git@0.20.3

RUN \
  apt-get update \
  && apt-get install -y default-jdk

ENV \
  JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/

COPY sysutils/ sysutils/

ENV \
  SPARK_VERSION=2.1.0 \
  PYSPARK_VERSION=0.10.4

RUN \
  # This is not a custom version of Spark.  It's merely a version with all the desired -P profiles enabled.
  wget https://s3.amazonaws.com/fluxcapacitor.com/packages/spark-${SPARK_VERSION}-bin-fluxcapacitor.tgz \
  && tar xvzf spark-${SPARK_VERSION}-bin-fluxcapacitor.tgz \
  && rm spark-${SPARK_VERSION}-bin-fluxcapacitor.tgz

ENV \
  SPARK_HOME=/root/spark-${SPARK_VERSION}-bin-fluxcapacitor

# This must be separate from the ${SPARK_HOME} ENV definition or else Docker doesn't recognize it
ENV \
  PATH=${SPARK_HOME}/bin:$PATH

ENV \
  HADOOP_VERSION=2.7.2

RUN \
 wget http://www.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz \
 && tar xvzf hadoop-${HADOOP_VERSION}.tar.gz \
 && rm hadoop-${HADOOP_VERSION}.tar.gz

ENV \
  HADOOP_HOME=/root/hadoop-${HADOOP_VERSION} \
  HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

# This must be separate from the ${HADOOP_HOME} ENV definition or else Docker doesn't recognize it
ENV \
  HADOOP_CONF=${HADOOP_HOME}/etc/hadoop/ \
  PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}

# Required by Spark
ENV \
  HADOOP_CONF_DIR=${HADOOP_CONF}

COPY config/spark/ config/spark/
RUN \
  cd ${SPARK_HOME}/conf \
  && ln -s /root/config/spark/core-site.xml \
  && ln -s /root/config/spark/fairscheduler.xml \
  && ln -s /root/config/spark/hdfs-site.xml \
  && ln -s /root/config/spark/hive-site.xml \
  && ln -s /root/config/spark/spark-defaults.conf

RUN \
  mv ${HADOOP_CONF}/core-site.xml ${HADOOP_CONF}/core-site.xml.orig \
  && cd ${HADOOP_CONF} \
  && ln -s /root/config/spark/core-site.xml

RUN \
  mv ${HADOOP_CONF}/hdfs-site.xml ${HADOOP_CONF}/hdfs-site.xml.orig \
  && cd ${HADOOP_CONF} \
  && ln -s /root/config/spark/conf/hdfs-site.xml

# Required by Tensorflow
ENV \
  HADOOP_HDFS_HOME=${HADOOP_HOME}

# Required by Tensorflow for HDFS
RUN \
  echo 'export CLASSPATH=$(${HADOOP_HDFS_HOME}/bin/hadoop classpath --glob)' >> /root/.bashrc

#ENV \
#  PATH=$TENSORFLOW_HOME/bazel-bin/tensorflow/tools/graph_transforms:$TENSORFLOW_HOME/bazel-bin/tensorflow/python/tools:$TENSORFLOW_HOME/bazel-bin/tensorflow/tools/benchmark/:$TENSORFLOW_HOME/bazel-bin/tensorflow/compiler/aot:$TENSORFLOW_HOME/bazel-bin/tensorflow/compiler/tests:$TENSORFLOW_HOME/bazel-bin/tensorflow/examples/tutorials/word2vec:$TENSORFLOW_HOME/bazel-bin/tensorflow/examples/tutorials/mnist:/root/apache-jmeter-3.1/bin/:/root/scripts:$PATH

RUN \
  pip install tensorflow-gpu==1.2.0

RUN \
  pip install keras==2.0.5

RUN \
  pip install pio-cli==0.68

EXPOSE 6006 8754 8755

COPY run run

CMD ["supervise", "."]
