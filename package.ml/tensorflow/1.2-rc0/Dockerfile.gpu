FROM fluxcapacitor/package-gpu-cuda8-16.04:master

WORKDIR /root

RUN \
  apt-get install -y software-properties-common \
  && add-apt-repository -y ppa:openjdk-r/ppa \
  && apt-get update \
  && apt-get install -y --no-install-recommends openjdk-8-jdk openjdk-8-jre-headless \
  && apt-get install -y apt-transport-https \
  && apt-get install -y wget \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

ENV \
  JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/

ENV \
  BAZEL_VERSION=0.4.5 \
  TENSORFLOW_SERVING_VERSION=0.5.1
 
# TensorFlow Serving Home (not required on PATH)
ENV \
  TENSORFLOW_SERVING_HOME=/root/serving 

# Required by TensorFlow Serving
RUN \
 apt-get update \
 && apt-get install -y \
        build-essential \
        curl \
        libcurl3-dev \
        git \
        libfreetype6-dev \
        libpng12-dev \
        libzmq3-dev \
        pkg-config \
        python-dev \
        python-numpy \
        python-pip \
        software-properties-common \
        swig \
        zip \
        zlib1g-dev

RUN \
  pip install grpcio

# Install Python with conda
RUN wget -q https://repo.continuum.io/miniconda/Miniconda3-4.1.11-Linux-x86_64.sh -O /tmp/miniconda.sh  && \
    echo '874dbb0d3c7ec665adf7231bbb575ab2 */tmp/miniconda.sh' | md5sum -c - && \
    bash /tmp/miniconda.sh -f -b -p /opt/conda && \
    /opt/conda/bin/conda install --yes python=3.5 sqlalchemy tornado jinja2 traitlets requests pip && \
    /opt/conda/bin/pip install --upgrade pip && \
    rm /tmp/miniconda.sh

ENV \
  PATH=/opt/conda/bin:$PATH

RUN \
  conda install --yes openblas scikit-learn numpy scipy matplotlib pandas seaborn

RUN \
  apt-get install -y python-qt4

# Set up Bazel.

# Running bazel inside a `docker build` command causes trouble, cf:
#   https://github.com/bazelbuild/bazel/issues/134
# The easiest solution is to set up a bazelrc file forcing --batch.
RUN echo "startup --batch" >>/etc/bazel.bazelrc
# Similarly, we need to workaround sandboxing issues:
#   https://github.com/bazelbuild/bazel/issues/418
RUN echo "build --spawn_strategy=standalone --genrule_strategy=standalone" \
    >>/etc/bazel.bazelrc
# Install the most recent bazel release.
ENV BAZEL_VERSION 0.4.5
RUN mkdir /root/bazel && \
    cd /root/bazel && \
    curl -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \
#    curl -fSsL -O https://raw.githubusercontent.com/bazelbuild/$BAZEL_VERSION/LICENSE.txt && \
    chmod a+x bazel-*.sh && \
    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \
    rm -f bazel-$BAZEL_VERSION-installer-linux-x86_64.sh

# Clone Tensorflow Serving and the Tensorflow Submodule
RUN \
 cd ~ \
 && git clone -b master --recurse-submodules https://github.com/tensorflow/serving.git \
 && cd $TENSORFLOW_SERVING_HOME \
 && git reset --hard 0669d6d 

ENV TF_NEED_CUDA=1
ENV TF_NEED_GCP=0
ENV TF_NEED_JEMALLOC=1
ENV TF_NEED_HDFS=1
ENV TF_NEED_OPENCL=0
ENV TF_ENABLE_XLA=0
ENV TF_CUDA_VERSION=8.0
ENV TF_CUDNN_VERSION=5
ENV CUDA_PATH="/usr/local/cuda"
ENV CUDA_TOOLKIT_PATH=$CUDA_PATH
ENV CUDNN_INSTALL_PATH=$CUDA_PATH
ENV PYTHON_BIN_PATH=/opt/conda/bin/python
ENV PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages
ENV CI_BUILD_PYTHON=$PYTHON_BIN_PATH
ENV CC_OPT_FLAGS="-march=native"
ENV TF_CUDA_COMPUTE_CAPABILITIES=3.7
# Check the required COMPUTE_CAPABILITIES from the following link:
#  https://developer.nvidia.com/cuda-gpus
# Also, Tensorflow has a minimum-supported COMPUTE CAPABILITY (ie. 3.5)
# ie. here are the AWS and GCP Instance Types and their COMPUTE CAPABILITIES
#
####################
###      AWS     ###
##  P2 Instances  ##
# Tesla K-80 (3.7) #
#                  #
##  G2 Instances  ##
# GRID K520 (3.5)  #
#                  #
###  Google GCP  ###   
# Tesla K-80 (3.7) #
####################

ENV \
  LANG=en_us.UTF-8

RUN \
  cd $TENSORFLOW_SERVING_HOME/tensorflow \
  && tensorflow/tools/ci_build/builds/configured GPU

RUN \
  cd $TENSORFLOW_SERVING_HOME \
  && sed -i.bak '/nccl/d' tensorflow/tensorflow/contrib/BUILD \
  && bazel build -c opt --config=cuda \
      --spawn_strategy=standalone --genrule_strategy=standalone \
      --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 \
      --crosstool_top=@local_config_cuda//crosstool:toolchain \
      tensorflow_serving/model_servers:tensorflow_model_server \
  && chmod a+x bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server \
  && cp bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server /usr/local/bin/ \
  && bazel clean --expunge

# OLD SUCCESSFUL WAY BEGIN #
#RUN \  
#  cd $TENSORFLOW_SERVING_HOME \
#  && sed -i.bak 's/@org_tensorflow\/\/third_party\/gpus\/crosstool/@local_config_cuda\/\/crosstool:toolchain/g' tools/bazel.rc \
#  && sed -i.bak '/nccl/d' tensorflow/tensorflow/contrib/BUILD \
# http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions
#  && bazel build -c opt --config=cuda --spawn_strategy=standalone --genrule_strategy=standalone \
#     --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 \
#     -k //tensorflow_serving/... 

#ENV \
#  PATH=$TENSORFLOW_SERVING_HOME/bazel-bin/tensorflow_serving/model_servers/:$PATH
# OLD SUCCESSFUL WAY END *

# Configure the build for our CUDA configuration.
ENV TF_ENABLE_XLA=1

# Need this inside Docker for nvidia-docker build step HACK
ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH
RUN \
  mkdir -p /usr/local/nvidia/lib64/ \
  && cd /usr/local/nvidia/lib64/ \
  && ln -s /usr/local/cuda-8.0/targets/x86_64-linux/lib/stubs/libcuda.so libcuda.so.1
RUN \
  ldconfig /usr/local/cuda/lib64

# This configuration was inspired by the following resources:
#   https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh
#   https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu
#   http://ci.tensorflow.org/job/tensorflow-master-linux-xla/104/consoleText
#   https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build

ENV \
  TENSORFLOW_HOME=/root/tensorflow

RUN \
 git clone -b r1.2 --recurse-submodules https://github.com/tensorflow/tensorflow.git \
 && cd $TENSORFLOW_HOME \
 && git reset --hard c9dd88f

ENV \ 
  TF_NEED_CUDA=1 \
  TF_ENABLE_XLA=1 \
  TF_CUDA_COMPUTE_CAPABILITIES=3.7

RUN \
  cd $TENSORFLOW_HOME \
  && yes "" | ./configure

RUN \
  # Run bazel test command. Double test timeouts to avoid flakes.
  cd $TENSORFLOW_HOME \
  && bazel build -c opt --config=cuda \
     #--test_tag_filters=-no_gpu,-benchmark-test \
     -k \
     --jobs=1 --test_timeout 300,450,1200,3600 \
     #--build_tests_only \
# Notes:  --local_test_jobs should == TF_GPU_COUNT
     --test_output=errors --local_test_jobs=1 \
# http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions
     --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 \
     --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" \
     --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute -- \
#     tensorflow/tools/pip_package:build_pip_package 
     //tensorflow/...

RUN \
  cd $TENSORFLOW_HOME \
  && bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip \
  && pip --no-cache-dir install --ignore-installed --upgrade /tmp/pip/tensorflow-*.whl

#TODO# Clean up pip wheel and Bazel cache when done.
#RUN tensorflow/tools/ci_build/builds/configured GPU \
    #pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl && \
    #rm -rf /tmp/pip && \
    #rm -rf /root/.cache

RUN \
  TF_CPP_MIN_LOG_LEVEL=0 \
  TF_XLA_FLAGS=--xla_generate_hlo_graph=.*

#RUN \
#  source $TENSORFLOW_HOME/tools/ci_build/builds/libtensorflow.sh \
#  build_libtensorflow_tarball 

## Cleanup for nvidia-docker build step HACK
RUN \
  rm /usr/local/nvidia/lib64/libcuda.so.1
