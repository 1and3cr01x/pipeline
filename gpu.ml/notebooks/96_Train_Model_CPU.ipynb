{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "import pylab\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "version = int(datetime.now().strftime(\"%s\"))\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "  log_device_placement=True,\n",
    ")\n",
    "print(config)\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "print(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Training and Validation Data \n",
    "Note the use of `shuffle_batch()`.  This uses `RandomShuffleQueue` internally.\n",
    "\n",
    "The `min_after_dequeue` parameter is the sample size.  A larger value requires more RAM, but improves the randomness of the shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_queue = tf.train.string_input_producer([\n",
    "  \"hdfs://127.0.0.1:39000/linear/training.csv\",\n",
    "#  \"hdfs://127.0.0.1:39000/linear/validation.csv\",\n",
    "#    \"training.csv\",\n",
    "])\n",
    "\n",
    "training_reader = tf.TextLineReader()\n",
    "_, training_value = training_reader.read(training_queue)\n",
    "\n",
    "x_training, y_training = tf.decode_csv(training_value, [[0.0],[0.0]])\n",
    "\n",
    "x_training_batch, y_training_batch = \\\n",
    "    tf.train.shuffle_batch([x_training, y_training], \n",
    "                           batch_size=100,\n",
    "                           capacity=200,\n",
    "                           min_after_dequeue=100, \n",
    "                           num_threads=20)\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "training_coord = tf.train.Coordinator()\n",
    "\n",
    "training_enqueue_threads = tf.train.start_queue_runners(sess=sess, \n",
    "                                                        coord=training_coord)\n",
    "\n",
    "print(\"Training Enqueue Thread Pool Size: %d\" % len(training_enqueue_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# pylab.plot(x_train, y_train, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#pylab.plot(x_test, y_test, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    W = tf.get_variable(shape=[], name='weights')\n",
    "    print(W)\n",
    "\n",
    "    b = tf.get_variable(shape=[], name='bias')\n",
    "    print(b)\n",
    "\n",
    "#    x_observed_batch = tf.placeholder(shape=[None], \n",
    "#                                      dtype=tf.float32, \n",
    "#                                      name='x_observed')\n",
    "#    print(x_observed_batch)\n",
    "    \n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "#    y_pred_batch = W * x_observed_batch + b\n",
    "    y_pred_batch = W * x_training_batch + b\n",
    "    print(y_pred_batch)\n",
    "\n",
    "    \n",
    "with tf.device(\"/cpu:0\"):\n",
    "#    y_observed_batch = tf.placeholder(shape=[None], dtype=tf.float32, name='y_observed')\n",
    "#    print(y_observed_batch)\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.square(y_pred_batch - y_training_batch))\n",
    "    optimizer_op = tf.train.GradientDescentOptimizer(0.025)\n",
    "    training_op = optimizer_op.minimize(loss_op)  \n",
    "\n",
    "    print(\"Loss Scalar: \", loss_op)\n",
    "    print(\"Optimizer Op: \", optimizer_op)\n",
    "    print(\"Training Op\", training_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Model Graph in Tensorboard\n",
    "\n",
    "Navigate to the Graph tab at this URL:\n",
    "\n",
    "http://[ip-address]:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Initialize Variables (Weights and Bias)\n",
    "The goal is to learn more accurate Weights and Bias during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    print(init_op)\n",
    "\n",
    "sess.run(init_op)\n",
    "print(\"W: %f\" % sess.run(W))\n",
    "print(\"b: %f\" % sess.run(b))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Accuracy of Initial Random Variables (Pre-Training)\n",
    "This should be relatively low because we have not trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0160929\n"
     ]
    }
   ],
   "source": [
    "#def accuracy(x_param):\n",
    "#  return sess.run(loss_op, \n",
    "  #feed_dict={x_observed: x_param, y_observed: y_param})\n",
    "    \n",
    "loss = sess.run(loss_op)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.723112\n",
      "CPU times: user 36 ms, sys: 4 ms, total: 40 ms\n",
      "Wall time: 20.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#try:\n",
    "\n",
    "loss = sess.run(loss_op)\n",
    "\n",
    "print(\"Accuracy: %f\" % (loss*100))\n",
    "\n",
    "#except tf.errors.OutOfRangeError as e:\n",
    "#    print(\"Error '%s'\" % str(e))\n",
    "#finally:\n",
    "#    training_coord.request_stop()\n",
    "#    training_coord.join(enqueue_threads)\n",
    "\n",
    "#print(1 - sess.run(loss_op))\n",
    "                   #, feed_dict={x_observed: x_training_batch,\n",
    "                   #             y_observed: y_training_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_summary_writer = tf.summary.FileWriter('/root/tensorboard/linear/cpu/%s/training' % version, \n",
    "                                                 graph=tf.get_default_graph())\n",
    "\n",
    "#validation_summary_writer = tf.summary.FileWriter('/root/tensorboard/linear/cpu/%s/validation' % version, \n",
    "#                                                 graph=tf.get_default_graph())\n",
    "\n",
    "# TODO\n",
    "#validation_summary_writer = tf.summary.FileWriter('/root/tensorboard/linear/cpu/%s/validation' % version, \n",
    "#validation_summary_writer = tf.summary.FileWriter('/root/tensorboard/linear/cpu/%s/validation' % version, \n",
    "#                                                   graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Loss Summary Operations for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_summary_scalar_op = tf.summary.scalar('loss', loss_op)\n",
    "loss_summary_merge_all_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.3435241, 0.53272933]\n",
      "100 [-0.21680978, 0.47234303]\n",
      "200 [-0.1280355, 0.42268601]\n",
      "300 [-0.063488364, 0.38688746]\n",
      "CPU times: user 13.1 s, sys: 2.74 s, total: 15.9 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_metadata = tf.RunMetadata()\n",
    "\n",
    "max_steps = 400\n",
    "for step in range(max_steps - 1):\n",
    "    if (step < max_steps):\n",
    "        training_summary_log, _ = sess.run([loss_summary_merge_all_op, training_op]) \n",
    "#        validation_summary_log, _ = sess.run([loss_summary_merge_all_op, loss_op])\n",
    "    else:  \n",
    "        training_summary_log, _ = sess.run([loss_summary_merge_all_op, training_op],\n",
    "                                            options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), \n",
    "                                            run_metadata=run_metadata)\n",
    "        \n",
    "#        validation_summary_log, _ = sess.run([loss_summary_merge_all_op, loss_op])\n",
    "    \n",
    "    trace = timeline.Timeline(step_stats=run_metadata.step_stats)    \n",
    "    with open('cpu-timeline.json', 'w') as trace_file:\n",
    "        trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        training_summary_writer.add_summary(training_summary_log, step)\n",
    "        training_summary_writer.flush()\n",
    "#        validation_summary_writer.add_summary(validation_summary_log, step)\n",
    "#        validation_summary_writer.flush()\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run([W, b]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pylab.plot(x_train, y_train, '.', label=\"target\")\n",
    "#pylab.plot(x_train, sess.run(y_pred, feed_dict={x_observed: x_train, y_observed: y_train}), \".\", label=\"predicted\")\n",
    "#pylab.legend()\n",
    "#pylab.ylim(0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Loss Scalar Summary in Tensorboard\n",
    "\n",
    "Navigate to the Scalars tab at this URL:\n",
    "\n",
    "http://[ip-address]:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00122416\n"
     ]
    }
   ],
   "source": [
    "loss = sess.run(loss_op)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Graph and Variables for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"shuffle_batch:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: 100\n",
      "  }\n",
      "}\n",
      "\n",
      "name: \"add:0\"\n",
      "dtype: DT_FLOAT\n",
      "tensor_shape {\n",
      "  dim {\n",
      "    size: 100\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.saved_model import utils\n",
    "\n",
    "tensor_info_x_observed = utils.build_tensor_info(x_training_batch)\n",
    "print(tensor_info_x_observed)\n",
    "\n",
    "tensor_info_y_pred = utils.build_tensor_info(y_pred_batch)\n",
    "print(tensor_info_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/models/linear/cpu/1495531623\n"
     ]
    }
   ],
   "source": [
    "export_path = \"/root/models/linear/cpu/%s\" % version\n",
    "\n",
    "print(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'/root/models/linear/cpu/1495531623/saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'/root/models/linear/cpu/1495531623/saved_model.pb'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "  builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "prediction_signature =  signature_def_utils.build_signature_def(\n",
    "    inputs = {'inputs': tensor_info_x_observed}, \n",
    "    outputs = {'outputs': tensor_info_y_pred}, \n",
    "    method_name = signature_constants.PREDICT_METHOD_NAME)            \n",
    "\n",
    "legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    "\n",
    "builder.add_meta_graph_and_variables(sess, \n",
    "  [tag_constants.SERVING],\n",
    "  signature_def_map={\"predict_linear\":prediction_signature,\n",
    "                      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:prediction_signature}, \n",
    "                      legacy_init_op=legacy_init_op)\n",
    "\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Metagraph for Optimization and Transformation\n",
    "We will use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_metagraph_path = \"/root/models/optimize_me/linear/cpu/%s\" % version\n",
    "os.rmdir(export_metagraph_path)\n",
    "\n",
    "os.makedirs(export_metagraph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "/root/models/optimize_me/linear/cpu/1495531623.tmpa95432b921444ba68e1fb95c4ddd0fdc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9ede6d4b87bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_checkpoint_every_n_hours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_metagraph_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         clear_devices=clear_devices)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[0;34m(filename, contents)\u001b[0m\n\u001b[1;32m    364\u001b[0m   \u001b[0mtemp_pathname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tmp\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m   \u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m   \u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrename\u001b[0;34m(oldname, newname, overwrite)\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     pywrap_tensorflow.RenameFile(\n\u001b[0;32m--> 348\u001b[0;31m         compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: /root/models/optimize_me/linear/cpu/1495531623.tmpa95432b921444ba68e1fb95c4ddd0fdc"
     ]
    }
   ],
   "source": [
    "\n",
    "saver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=24)\n",
    "saver.export_meta_graph(export_metagraph_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Saved Model on Disk\n",
    "\n",
    "You must replace `[version]` with the version number from above ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/linear/cpu/[version]\n",
    "ls -l /root/models/linear/cpu/metagraph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#from tensorflow.python.framework import graph_io\n",
    "#graph_io.write_graph(sess.graph, \"/root/models/optimize_me/\", \"unoptimized_cpu.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: \n",
    "training_coord.request_stop()\n",
    "training_coord.join(enqueue_threads)\n",
    "\n",
    "#validation_coord.request_stop()\n",
    "#validation_coord.join(enqueue_threads)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:  View Accuracy of Trained Variables\n",
    "This should be relatively high after training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "validation_queue = tf.train.string_input_producer([\n",
    "#  \"hdfs://127.0.0.1:39000/linear/training.csv\",\n",
    "#  \"hdfs://127.0.0.1:39000/linear/validation.csv\",\n",
    "    \"validation.csv\",\n",
    "])\n",
    "\n",
    "validation_reader = tf.TextLineReader()\n",
    "_, validation_value = validation_reader.read(validation_queue)\n",
    "\n",
    "x_validation, y_validation = tf.decode_csv(validation_value, [[0.0],[0.0]])\n",
    "\n",
    "x_validation_batch, y_validation_batch = \\\n",
    "    tf.train.shuffle_batch([x_validation, y_validation], \n",
    "                           batch_size=100,\n",
    "                           capacity=2000,\n",
    "                           min_after_dequeue=1000)\n",
    "\n",
    "validation_coord = tf.train.Coordinator()\n",
    "\n",
    "validation_enqueue_threads = tf.train.start_queue_runners(sess=sess,\n",
    "                                                          coord=validation_coord)\n",
    "print(\"Validation Enqueue Thread Pool Size: %d\" % len(validation_enqueue_threads))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
