{
  "paragraphs": [
    {
      "title": "Retrieve dataset",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")\n  .select($\"id\", $\"title\", $\"category\", $\"description\")",
      "dateUpdated": "Jan 19, 2016 9:19:41 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451704834989_870454693",
      "id": "20160102-032034_619481341",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, title: string, category: string, description: string]\n"
      },
      "dateCreated": "Jan 2, 2016 3:20:34 AM",
      "dateStarted": "Jan 19, 2016 9:19:41 PM",
      "dateFinished": "Jan 19, 2016 9:20:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Distinct Categories",
      "text": "val distinctCategoriesDF \u003d itemsDF.select($\"title\", $\"category\").distinct()\nz.show(distinctCategoriesDF)",
      "dateUpdated": "Jan 19, 2016 9:19:41 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": true,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451771827145_-655270163",
      "id": "20160102-215707_46317998",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "title\tcategory\nSpark ML/MLlib\tLibrary\nPostgres\tDatabase\nMemcached\tDistributed Cache\nApache Pig\tData Processing Execution Engine\nKinesis\tLibrary\nSpark Streaming\tLibrary\nApache Parquet\tFile Format\nJSON\tFile Format\nTitan GraphDB\tDatabase\nNLTK\tLibrary\nKnime\tWorkflow\nCSV\tFile Format\nApache Solr\tSearch Engine\nElastic MapReduce\tData Processing Execution Engine\nApache Impala\tData Processing Execution Engine\nApache Hive\tData Processing Execution Engine\nPresto\tData Processing Execution Engine\nApache ZooKeeper\tDistributed Coordinator\nPython\tProgramming Language\nApache Kafka\tMessage Broker\nApache HUE\tUI\nTeradata\tDatabase\nVertica\tDatabase\nApache YARN\tCluster Resource Manager\nApache Nifi\tWorkflow\nRedshift\tDatabase\nMapR\tDistribution\nXML\tFile Format\nDynamoDB\tDatabase\nHortonworks\tDistribution\niPython/Jupyter\tNotebook\nRedis\tDistributed Cache\nScala\tProgramming Language\nApache Ambari\tCluster Provision\nDato GraphLab Create\tLibrary\nAmazon Web Services\tCloud Provider\nApache Flume\tLibrary\nApache Mahout\tLibrary\nStanford CoreNLP\tLibrary\nTachyon\tDistributed Cache\nMicroStrategy\tBI\nApache Cassandra\tDatabase\nS3\tFile System\nNeo4j\tLibrary\nApache Drill\tData Processing Execution Engine\nProtobuffers\tFile Format\nApache Mesos\tCluster Resource Manager\nSQL Server\tDatabase\nApache Flink\tData Processing Execution Engine\nApache Spark\tData Processing Execution Engine\nApache Storm\tStreaming\nSpark SQL\tLibrary\nMicrosft Azure\tCloud Provider\nMySQL\tDatabase\nIBM BigInsights\tDistribution\nMongoDB\tDatabase\nApache Tez\tData Processing Execution Engine\nSci-Kit Learn\tLibrary\nApache HDFS\tFile System\nR\tProgramming Language\nApache Sqoop\tData Import\nApache Lucene\tLibrary\nDeep Learning 4J\tLibrary\nApache Zeppelin\tNotebook\nTableau\tBI\nCloudera\tDistribution\nApache Oozie\tWorkflow\nGoogle Cloud Platform\tCloud Provider\nOn-Premise\tCloud Provider\nDocker\tCategory\nElasticSearch\tSearch Engine\nTensor Flow\tData Processing Execution Engine\nJava\tProgramming Language\nSpark GraphX\tLibrary\nApache Giraph\tLibrary\nApache HBase\tDatabase\nApache ORC\tFile Format\nApache MapReduce\tData Processing Execution Engine\nOracle\tDatabase\nSQL\tProgramming Language\n"
      },
      "dateCreated": "Jan 2, 2016 9:57:07 PM",
      "dateStarted": "Jan 19, 2016 9:19:43 PM",
      "dateFinished": "Jan 19, 2016 9:20:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Tokenize",
      "text": "import org.apache.spark.ml.feature.RegexTokenizer\n\n// Split each document into words\nval tokenizer \u003d new RegexTokenizer()\n  .setInputCol(\"description\")\n  .setOutputCol(\"words\")\n  .setGaps(false)\n  .setPattern(\"\\\\p{L}+\")",
      "dateUpdated": "Jan 19, 2016 9:19:41 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451704551688_-754338267",
      "id": "20160102-031551_2007021723",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.RegexTokenizer\ntokenizer: org.apache.spark.ml.feature.RegexTokenizer \u003d regexTok_bb46ca2aec71\n"
      },
      "dateCreated": "Jan 2, 2016 3:15:51 AM",
      "dateStarted": "Jan 19, 2016 9:20:02 PM",
      "dateFinished": "Jan 19, 2016 9:20:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Remove Common Stop Words",
      "text": "import org.apache.spark.ml.feature.StopWordsRemover\n\n// The following list will be used by default if we don\u0027t specify a list:  \n//   http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\nval stopWordsFilter \u003d new StopWordsRemover()\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"filteredWords\")\n  .setCaseSensitive(false)",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451413520885_1262045843",
      "id": "20151229-182520_534225248",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.StopWordsRemover\nstopWordsFilter: org.apache.spark.ml.feature.StopWordsRemover \u003d stopWords_ceab771532ff\n"
      },
      "dateCreated": "Dec 29, 2015 6:25:20 PM",
      "dateStarted": "Jan 19, 2016 9:20:05 PM",
      "dateFinished": "Jan 19, 2016 9:20:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "N-Gram Transformer",
      "text": "import org.apache.spark.ml.feature.NGram\n\nval ngram \u003d new NGram()\n  .setInputCol(stopWordsFilter.getOutputCol)\n  .setOutputCol(\"ngramFeatures\")",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452997979116_-524399973",
      "id": "20160117-023259_677917666",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.NGram\nngram: org.apache.spark.ml.feature.NGram \u003d ngram_cceae712111f\n"
      },
      "dateCreated": "Jan 17, 2016 2:32:59 AM",
      "dateStarted": "Jan 19, 2016 9:20:05 PM",
      "dateFinished": "Jan 19, 2016 9:20:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "TF transformer",
      "text": "import org.apache.spark.ml.feature.HashingTF\n\nval tf \u003d new HashingTF()\n  .setInputCol(ngram.getOutputCol)\n  .setOutputCol(\"tfFeatures\")",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451704581368_-518071794",
      "id": "20160102-031621_1939813047",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.HashingTF\ntf: org.apache.spark.ml.feature.HashingTF \u003d hashingTF_909002a30541\n"
      },
      "dateCreated": "Jan 2, 2016 3:16:21 AM",
      "dateStarted": "Jan 19, 2016 9:20:05 PM",
      "dateFinished": "Jan 19, 2016 9:20:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "IDF transformer",
      "text": "import org.apache.spark.ml.feature.IDF\n\n// Limit to top `vocabSize` most common words and convert to word count vector features\nval idf \u003d new IDF()\n  .setInputCol(tf.getOutputCol)\n  .setOutputCol(\"idfFeatures\")",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451775771985_-1247745195",
      "id": "20160102-230251_1962189552",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.IDF\nidf: org.apache.spark.ml.feature.IDF \u003d idf_0a5862d69838\n"
      },
      "dateCreated": "Jan 2, 2016 11:02:51 PM",
      "dateStarted": "Jan 19, 2016 9:20:06 PM",
      "dateFinished": "Jan 19, 2016 9:20:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "assemble TF and IDF feature vectors",
      "text": "import org.apache.spark.ml.feature.VectorAssembler\n\nval featureVectorAssembler \u003d new VectorAssembler()\n  .setInputCols(Array(ngram.getOutputCol, tf.getOutputCol, idf.getOutputCol))\n  .setOutputCol(\"allFeatures\")",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451774247109_110458938",
      "id": "20160102-223727_1555430952",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.VectorAssembler\nfeatureVectorAssembler: org.apache.spark.ml.feature.VectorAssembler \u003d vecAssembler_dacac43f0d9c\n"
      },
      "dateCreated": "Jan 2, 2016 10:37:27 PM",
      "dateStarted": "Jan 19, 2016 9:20:06 PM",
      "dateFinished": "Jan 19, 2016 9:20:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "convert the category labels into indexes",
      "text": "import org.apache.spark.ml.feature.StringIndexer\n\n// Assign an Index to Each Category \nval categoryIndexerModel \u003d new StringIndexer()\n  .setInputCol(\"category\")\n  .setOutputCol(\"indexedCategory\")\n  .fit(itemsDF)",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451768708398_-1255734024",
      "id": "20160102-210508_277987434",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.StringIndexer\ncategoryIndexerModel: org.apache.spark.ml.feature.StringIndexerModel \u003d strIdx_e4101689dc39\n"
      },
      "dateCreated": "Jan 2, 2016 9:05:08 PM",
      "dateStarted": "Jan 19, 2016 9:20:07 PM",
      "dateFinished": "Jan 19, 2016 9:20:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "create decision tree classifer",
      "text": "import org.apache.spark.ml.classification.DecisionTreeClassifier\n\nval classifier \u003d new DecisionTreeClassifier()\n  .setFeaturesCol(featureVectorAssembler.getOutputCol)\n  .setLabelCol(categoryIndexerModel.getOutputCol)\n  .setPredictionCol(\"prediction\")\n  .setRawPredictionCol(\"confidence\")\n  .setProbabilityCol(\"probability\")",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451714221599_169436069",
      "id": "20160102-055701_1764018921",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.classification.DecisionTreeClassifier\nclassifier: org.apache.spark.ml.classification.DecisionTreeClassifier \u003d dtc_cca8e7b61e07\n"
      },
      "dateCreated": "Jan 2, 2016 5:57:01 AM",
      "dateStarted": "Jan 19, 2016 9:20:07 PM",
      "dateFinished": "Jan 19, 2016 9:20:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "convert the category index back to String",
      "text": "import org.apache.spark.ml.feature.IndexToString\n\nval categoryReverseIndexer \u003d new IndexToString()\n  .setInputCol(classifier.getPredictionCol)\n  .setOutputCol(\"predictedCategory\")\n  .setLabels(categoryIndexerModel.labels)",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452219659117_711798430",
      "id": "20160108-022059_1281293442",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.IndexToString\ncategoryReverseIndexer: org.apache.spark.ml.feature.IndexToString \u003d idxToStr_6df95845d9e5\n"
      },
      "dateCreated": "Jan 8, 2016 2:20:59 AM",
      "dateStarted": "Jan 19, 2016 9:20:08 PM",
      "dateFinished": "Jan 19, 2016 9:20:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create the training pipeline",
      "text": "import org.apache.spark.ml.Pipeline\n\nval pipeline \u003d new Pipeline()\n  .setStages(Array(tokenizer, stopWordsFilter, ngram, tf, idf, featureVectorAssembler, categoryIndexerModel, classifier, categoryReverseIndexer))",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451714366805_831050936",
      "id": "20160102-055926_1532666557",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.Pipeline\npipeline: org.apache.spark.ml.Pipeline \u003d pipeline_6ac17fe0093f\n"
      },
      "dateCreated": "Jan 2, 2016 5:59:26 AM",
      "dateStarted": "Jan 19, 2016 9:20:08 PM",
      "dateFinished": "Jan 19, 2016 9:20:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create evaluator for multiclass decision tree model",
      "text": "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\nval metricName \u003d \"f1\"\n\nval modelEvaluator \u003d new MulticlassClassificationEvaluator()\n  .setLabelCol(classifier.getLabelCol)\n  .setPredictionCol(classifier.getPredictionCol)\n  .setMetricName(metricName)",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451769445587_-1118091320",
      "id": "20160102-211725_268797232",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nmetricName: String \u003d f1\nmodelEvaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \u003d mcEval_3f08664f422d\n"
      },
      "dateCreated": "Jan 2, 2016 9:17:25 PM",
      "dateStarted": "Jan 19, 2016 9:20:09 PM",
      "dateFinished": "Jan 19, 2016 9:20:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Build param grid search",
      "text": "import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval paramGrid \u003d new ParamGridBuilder()\n  .addGrid(tf.numFeatures, Array(1, 2, 3))\n  .addGrid(tf.numFeatures, Array(10, 100))\n  .addGrid(idf.minDocFreq, Array(1, 10))\n  .addGrid(classifier.maxDepth, Array(3, 5))\n  .build()\n  \nparamGrid.size",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451771035936_440379213",
      "id": "20160102-214355_638942362",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.tuning.ParamGridBuilder\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] \u003d \nArray({\n\tdtc_cca8e7b61e07-maxDepth: 3,\n\tidf_0a5862d69838-minDocFreq: 1,\n\thashingTF_909002a30541-numFeatures: 10\n}, {\n\tdtc_cca8e7b61e07-maxDepth: 3,\n\tidf_0a5862d69838-minDocFreq: 1,\n\thashingTF_909002a30541-numFeatures: 100\n}, {\n\tdtc_cca8e7b61e07-maxDepth: 3,\n\tidf_0a5862d69838-minDocFreq: 10,\n\thashingTF_909002a30541-numFeatures: 10\n}, {\n\tdtc_cca8e7b61e07-maxDepth: 3,\n\tidf_0a5862d69838-minDocFreq: 10,\n\thashingTF_909002a30541-numFeatures: 100\n}, {\n\tdtc_cca8e7b61e07-maxDepth: 5,\n\tidf_0a5862d69838-minDocFreq: 1,\n\thashingTF_909002a30541-numFeatures: 10\n}, {\n\tdtc_cca8e7b61e07-maxDepth: 5,\n\tidf_0a5862d69838-minDocFreq: 1,\n\thashingTF_909002a30541-numFeatures: 100\n}, {\n\tdtc_cca8e7b61e07-maxDepth: 5,\n\tidf_0a5862d69838-minDocFreq: 10,\n\thashingT...res33: Int \u003d 8\n"
      },
      "dateCreated": "Jan 2, 2016 9:43:55 PM",
      "dateStarted": "Jan 19, 2016 9:20:09 PM",
      "dateFinished": "Jan 19, 2016 9:20:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create cross validator",
      "text": "import org.apache.spark.ml.tuning.CrossValidator\n\n// K-Folds Cross Validation Combined With Param Grid \nval numFolds \u003d 3\n\nval modelValidator \u003d new CrossValidator()\n  .setEstimator(pipeline)\n  .setEvaluator(modelEvaluator)\n  .setEstimatorParamMaps(paramGrid)\n  .setNumFolds(numFolds) ",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451770989649_-868764763",
      "id": "20160102-214309_573238279",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.tuning.CrossValidator\nnumFolds: Int \u003d 3\nmodelValidator: org.apache.spark.ml.tuning.CrossValidator \u003d cv_7fc4574d54b2\n"
      },
      "dateCreated": "Jan 2, 2016 9:43:09 PM",
      "dateStarted": "Jan 19, 2016 9:20:10 PM",
      "dateFinished": "Jan 19, 2016 9:20:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train crossValidator model",
      "text": "val crossValidatorModel \u003d modelValidator.fit(itemsDF)",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451771239364_977982340",
      "id": "20160102-214719_422404290",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.lang.IllegalArgumentException: Data type ArrayType(StringType,false) is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:116)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:112)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:112)\n\tat org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:173)\n\tat org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:173)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108)\n\tat org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:173)\n\tat org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:129)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:68)\n\tat org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:91)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:72)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:77)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:79)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:81)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:83)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:85)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:87)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:89)\n\tat \u003cinit\u003e(\u003cconsole\u003e:91)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:95)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Jan 2, 2016 9:47:19 PM",
      "dateStarted": "Jan 19, 2016 9:20:10 PM",
      "dateFinished": "Jan 19, 2016 9:20:11 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Describe effectiveness of all param grid options",
      "text": "// Print the average metrics\nval avgMetricsParamGrid \u003d crossValidatorModel.avgMetrics\n\n// Combine with paramGrid to see how they affect the overall metrics\nval combined \u003d paramGrid.zip(avgMetricsParamGrid)",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451772919906_1239014473",
      "id": "20160102-221519_627091553",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:41: error: not found: value crossValidatorModel\n       val avgMetricsParamGrid \u003d crossValidatorModel.avgMetrics\n                                 ^\n"
      },
      "dateCreated": "Jan 2, 2016 10:15:19 PM",
      "dateStarted": "Jan 19, 2016 9:20:11 PM",
      "dateFinished": "Jan 19, 2016 9:20:11 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Describe Chosen HyperParameters",
      "text": "import org.apache.spark.ml.feature.Word2VecModel\nimport org.apache.spark.ml.feature.IDFModel\nimport org.apache.spark.ml.feature.StringIndexerModel\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.PipelineModel\n\nval bestModel \u003d crossValidatorModel.bestModel.asInstanceOf[PipelineModel]\n\n// Explain params for each stage\nval bestHashingTFNumFeatures \u003d bestModel.stages(2).asInstanceOf[HashingTF].explainParams\nval bestIDFMinDocFrequency \u003d bestModel.stages(3).asInstanceOf[IDFModel].explainParams\nval bestDecisionTreeDepth \u003d bestModel.stages(6).asInstanceOf[DecisionTreeClassificationModel].explainParams",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452289600030_-1267163701",
      "id": "20160108-214640_299879104",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.Word2VecModel\nimport org.apache.spark.ml.feature.IDFModel\nimport org.apache.spark.ml.feature.StringIndexerModel\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.PipelineModel\n\u003cconsole\u003e:46: error: not found: value crossValidatorModel\n       val bestModel \u003d crossValidatorModel.bestModel.asInstanceOf[PipelineModel]\n                       ^\n"
      },
      "dateCreated": "Jan 8, 2016 9:46:40 PM",
      "dateStarted": "Jan 19, 2016 9:20:11 PM",
      "dateFinished": "Jan 19, 2016 9:20:11 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Predict on new data",
      "text": "val predictOnDF \u003d sqlContext.createDataFrame(Seq(\n      (1, \"nosql\")\n    )).toDF(\"id\", \"description\")\n\nval predictedResultsDF \u003d bestModel.transform(predictOnDF)\n .select(classifier.getPredictionCol, categoryReverseIndexer.getOutputCol, stopWordsFilter.getOutputCol, classifier.getRawPredictionCol, classifier.getProbabilityCol)\n\nz.show(predictedResultsDF)",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "prediction",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "predictedCategory",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "prediction",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451704614990_2073874276",
      "id": "20160102-031654_499992823",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "predictOnDF: org.apache.spark.sql.DataFrame \u003d [id: int, description: string]\n\u003cconsole\u003e:68: error: not found: value bestModel\n       val predictedResultsDF \u003d bestModel.transform(predictOnDF)\n                                ^\n"
      },
      "dateCreated": "Jan 2, 2016 3:16:54 AM",
      "dateStarted": "Jan 19, 2016 9:20:11 PM",
      "dateFinished": "Jan 19, 2016 9:20:12 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Jan 19, 2016 9:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452217950823_214978392",
      "id": "20160108-015230_770789401",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 8, 2016 1:52:30 AM",
      "dateStarted": "Jan 19, 2016 9:20:12 PM",
      "dateFinished": "Jan 19, 2016 9:20:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "NLP/05: Text Classifier Pipeline (TF/IDF, N-Gram, Decision Tree)",
  "id": "2B8KKS3KC",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}