{
  "paragraphs": [
    {
      "text": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature",
      "dateUpdated": "Jan 4, 2016 3:54:53 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451318099632_1275613400",
      "id": "20151228-155459_1951209027",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature\n"
      },
      "dateCreated": "Dec 28, 2015 3:54:59 PM",
      "dateStarted": "Jan 4, 2016 3:54:53 AM",
      "dateFinished": "Jan 4, 2016 3:55:04 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load dataset including tags",
      "text": "val itemsDF \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"file:/root/pipeline/datasets/movielens/ml-latest/movies-sm.csv\").toDF(\"id\", \"title\", \"tags\")\n\nitemsDF.collect()",
      "dateUpdated": "Jan 4, 2016 3:54:56 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451323111480_-372297908",
      "id": "20151228-171831_1063248354",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [id: int, title: string, tags: string]\nres2: Array[org.apache.spark.sql.Row] \u003d Array([1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy], [2,Jumanji (1995),Adventure|Children|Fantasy], [3,Grumpier Old Men (1995),Comedy|Romance], [4,Waiting to Exhale (1995),Comedy|Drama|Romance], [5,Father of the Bride Part II (1995),Comedy], [6,Heat (1995),Action|Crime|Thriller], [7,Sabrina (1995),Comedy|Romance], [8,Tom and Huck (1995),Adventure|Children], [9,Sudden Death (1995),Action], [10,GoldenEye (1995),Action|Adventure|Thriller])\n"
      },
      "dateCreated": "Dec 28, 2015 5:18:31 PM",
      "dateStarted": "Jan 4, 2016 3:54:56 AM",
      "dateFinished": "Jan 4, 2016 3:55:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert pipe-Delimited tags into a set",
      "text": "import org.apache.spark.sql.Row\n\ncase class Item(id: Long, title: String, tags: Seq[String]) {\n  override def toString: String \u003d id + \", \" + title + \", \" + tags\n}\n\n// Convert from RDD[Row] to RDD[Item]\nval itemsRDD \u003d itemsDF.select($\"id\", $\"title\", $\"tags\").map(row \u003d\u003e {\n  val id \u003d row.getInt(0)\n  val title \u003d row.getString(1)\n  val tags \u003d row.getString(2).trim.split(\"\\\\|\")\n  Item(id, title, tags)\n})\n\nitemsRDD.collect()",
      "dateUpdated": "Jan 4, 2016 3:54:59 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451400566555_-1305258659",
      "id": "20151229-144926_1579858999",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\ndefined class Item\nitemsRDD: org.apache.spark.rdd.RDD[Item] \u003d MapPartitionsRDD[19] at map at \u003cconsole\u003e:40\nres8: Array[Item] \u003d Array(1, Toy Story (1995), WrappedArray(Adventure, Animation, Children, Comedy, Fantasy), 2, Jumanji (1995), WrappedArray(Adventure, Children, Fantasy), 3, Grumpier Old Men (1995), WrappedArray(Comedy, Romance), 4, Waiting to Exhale (1995), WrappedArray(Comedy, Drama, Romance), 5, Father of the Bride Part II (1995), WrappedArray(Comedy), 6, Heat (1995), WrappedArray(Action, Crime, Thriller), 7, Sabrina (1995), WrappedArray(Comedy, Romance), 8, Tom and Huck (1995), WrappedArray(Adventure, Children), 9, Sudden Death (1995), WrappedArray(Action), 10, GoldenEye (1995), WrappedArray(Action, Adventure, Thriller))\n"
      },
      "dateCreated": "Dec 29, 2015 2:49:26 PM",
      "dateStarted": "Jan 4, 2016 3:55:05 AM",
      "dateFinished": "Jan 4, 2016 3:55:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Number of distinct Tags found in the dataset",
      "text": "//val distinctCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").distinct()\n//distinctCounts.count()",
      "dateUpdated": "Jan 4, 2016 2:58:26 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451398205879_822632098",
      "id": "20151229-141005_354437963",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 29, 2015 2:10:05 PM",
      "dateStarted": "Jan 4, 2016 2:58:44 AM",
      "dateFinished": "Jan 4, 2016 2:58:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of tags within dataset",
      "text": "//val tagCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").groupBy($\"tag\")\n//  .agg(count($\"tag\").as(\"count\"))\n//  .orderBy($\"count\".desc)\n//  .limit(10)\n\n//z.show(tagCounts)",
      "dateUpdated": "Jan 4, 2016 2:58:26 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "pieChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451277734947_-102286185",
      "id": "20151228-044214_1740589078",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 28, 2015 4:42:14 AM",
      "dateStarted": "Jan 4, 2016 2:58:45 AM",
      "dateFinished": "Jan 4, 2016 2:58:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Exact Jaccard Similarity Algorithm",
      "text": "def getExactJaccardSimilarity(item1: Item, item2: Item): Double \u003d {\n  val intersectTagSize \u003d (item1.tags intersect item2.tags).size\n  val unionTagSize \u003d (item1.tags union item2.tags).size\n    \n  val jaccardSimilarity \u003d \n    if (unionTagSize \u003e 0) intersectTagSize.toDouble / unionTagSize.toDouble\n  \telse 0.0\n\n  jaccardSimilarity\n}",
      "dateUpdated": "Jan 4, 2016 3:55:02 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451279325043_812927468",
      "id": "20151228-050845_1911880139",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "getExactJaccardSimilarity: (item1: Item, item2: Item)Double\n"
      },
      "dateCreated": "Dec 28, 2015 5:08:45 AM",
      "dateStarted": "Jan 4, 2016 3:55:09 AM",
      "dateFinished": "Jan 4, 2016 3:55:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Approx Jaccard Similarity Algorithm",
      "text": "def getApproxLSHJaccardSimilarity(minHasher: MinHasher32, item1: Item, item2: Item): Double \u003d {\n  val minHashSignatureItem1 \u003d item1.tags.map(tag \u003d\u003e minHasher.init(tag))\n    .reduce((leftMinHashSignature, rightMinHashSignature) \u003d\u003e minHasher.plus(leftMinHashSignature, rightMinHashSignature))\n\n  val minHashSignatureItem2 \u003d item2.tags.map(tag \u003d\u003e minHasher.init(tag))\n    .reduce((leftMinHashSignature, rightMinHashSignature) \u003d\u003e minHasher.plus(leftMinHashSignature, rightMinHashSignature))\n\n  minHasher.similarity(minHashSignatureItem1, minHashSignatureItem2)\n}",
      "dateUpdated": "Jan 4, 2016 3:55:07 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451867216211_2035830206",
      "id": "20160104-002656_1737927282",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "getApproxLSHJaccardSimilarity: (minHasher: com.twitter.algebird.MinHasher32, item1: Item, item2: Item)Double\n"
      },
      "dateCreated": "Jan 4, 2016 12:26:56 AM",
      "dateStarted": "Jan 4, 2016 3:55:10 AM",
      "dateFinished": "Jan 4, 2016 3:55:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val allItemPairsRDD \u003d itemsRDD.cartesian(itemsRDD)\n\n// Filter out duplicates and preserve only the pairs with left id \u003c right id\nval distinctItemPairsRDD \u003d allItemPairsRDD\n  .filter(itemPair \u003d\u003e itemPair._1.id \u003c itemPair._2.id)",
      "dateUpdated": "Jan 4, 2016 3:55:12 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451868733983_2093329647",
      "id": "20160104-005213_454973852",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "allItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d CartesianRDD[20] at cartesian at \u003cconsole\u003e:40\ndistinctItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d MapPartitionsRDD[21] at filter at \u003cconsole\u003e:45\n"
      },
      "dateCreated": "Jan 4, 2016 12:52:13 AM",
      "dateStarted": "Jan 4, 2016 3:55:12 AM",
      "dateFinished": "Jan 4, 2016 3:55:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Exact Jaccard Similarity between all Distinct item pairs",
      "text": "val minExactJaccardSimilarityThreshold \u003d 0.01\n\n// Calculate Jaccard Similarity between all distinct item pairs\n// Only keep pairs with a Jaccard Similarity above a specific threshold\nval similarItemsAboveThresholdRDD \u003d distinctItemPairsRDD.flatMap(itemPair \u003d\u003e {\n  val jaccardSim \u003d getExactJaccardSimilarity(itemPair._1, itemPair._2)\n  if (jaccardSim \u003e\u003d minExactJaccardSimilarityThreshold)\n    Some(itemPair._1.id.toLong, itemPair._2.id.toLong, jaccardSim.toDouble)\n  else\n    None\n})\n\nval similarItemPairCount \u003d similarItemsAboveThresholdRDD.count()\nsimilarItemsAboveThresholdRDD.collect().mkString(\",\")",
      "dateUpdated": "Jan 4, 2016 3:58:19 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451317394187_1089784615",
      "id": "20151228-154314_719152611",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "minExactJaccardSimilarityThreshold: Double \u003d 0.01\nsimilarItemsAboveThresholdRDD: org.apache.spark.rdd.RDD[(Long, Long, Double)] \u003d MapPartitionsRDD[22] at flatMap at \u003cconsole\u003e:51\nsimilarItemPairCount: Long \u003d 19\nres19: String \u003d (1,2,0.375),(1,3,0.14285714285714285),(1,4,0.125),(1,5,0.16666666666666666),(3,4,0.4),(3,5,0.3333333333333333),(4,5,0.25),(1,7,0.14285714285714285),(1,8,0.2857142857142857),(1,10,0.125),(2,8,0.4),(2,10,0.16666666666666666),(3,7,0.5),(4,7,0.4),(5,7,0.3333333333333333),(6,9,0.25),(6,10,0.3333333333333333),(8,10,0.2),(9,10,0.25)\n"
      },
      "dateCreated": "Dec 28, 2015 3:43:14 PM",
      "dateStarted": "Jan 4, 2016 3:55:14 AM",
      "dateFinished": "Jan 4, 2016 3:55:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Shamelessly lifted from GraphX In Action (Great Book!)\ndef dijkstra(graph: Graph[VertexId, Double], srcId: VertexId): Graph[(VertexId, List[Any]), Double] \u003d {\n  // Dijkstra Shortest Path\n  var graph2 \u003d graph.mapVertices((vid,vd) \u003d\u003e \n    (false, if (vid \u003d\u003d srcId) 0 else Double.MaxValue, List[VertexId]()))\n\n  for (i \u003c- 1L to graph.vertices.count-1) {\n    // The fold() below simulates minBy() functionality\n    val currentVertexId \u003d graph2.vertices.filter(!_._2._1)\n      .fold((0L,(false,Double.MaxValue,List[VertexId]())))((a,b) \u003d\u003e\n        if (a._2._2 \u003c b._2._2) a else b)._1\n      \n\n    val newDistances \u003d graph2.aggregateMessages[(Double,List[VertexId])](ctx \u003d\u003e \n      if (ctx.srcId \u003d\u003d currentVertexId) \n        ctx.sendToDst((ctx.srcAttr._2 + ctx.attr, ctx.srcAttr._3 :+ ctx.srcId)),\n          (a,b) \u003d\u003e if (a._1 \u003c b._1) a else b)\n        \n    graph2 \u003d graph2.outerJoinVertices(newDistances)((vid, vd, newSum) \u003d\u003e {\n      val newSumVal \u003d newSum.getOrElse((Double.MaxValue,List[VertexId]()))\n        (vd._1 || vid \u003d\u003d currentVertexId, math.min(vd._2, newSumVal._1),\n          if (vd._2 \u003c newSumVal._1) vd._3 else newSumVal._2)})\n  }\n\n  val shortestPathGraph \u003d graph.outerJoinVertices(graph2.vertices)((vid, vd, dist) \u003d\u003e \n    (vd, dist.getOrElse((false,Double.MaxValue,List[VertexId]()))\n    .productIterator.toList.tail))\n  \n  shortestPathGraph\n}",
      "dateUpdated": "Jan 4, 2016 3:56:07 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451398384385_1393442935",
      "id": "20151229-141304_162052884",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dijkstra: (graph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Double], srcId: org.apache.spark.graphx.VertexId)org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double]\n"
      },
      "dateCreated": "Dec 29, 2015 2:13:04 PM",
      "dateStarted": "Jan 4, 2016 3:56:07 AM",
      "dateFinished": "Jan 4, 2016 3:56:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shortest Path between 2 Items Based on Exact Jaccard Similarity of Item Tags",
      "text": "val similarItemsAboveThresholdEdgeRDD \u003d similarItemsAboveThresholdRDD.map(rdd \u003d\u003e {\n  Edge(rdd._1.toLong, rdd._2.toLong, rdd._3.toDouble) \n})\n\n//similarItemsAboveThresholdEdgeRDD.collect()\n\nval graph \u003d Graph.fromEdges(similarItemsAboveThresholdEdgeRDD, 0L)\nval edgeCount \u003d graph.edges.count\n\nval src \u003d 1\nval dest \u003d 9\n// Note:  Should go through 1 -\u003e 10 -\u003e 9\n\n// Generate Shortest Path for all nodes in the graph\nval shortestPathGraph \u003d dijkstra(graph, src)\n\n// Filter out only the ones with dest as the destination vertex\nval shortestPathFromSrcToDest \u003d shortestPathGraph.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2\n\n// Example\n//\n// Shortest Path\n// 1 (Toy Story) -\u003e 10 (GoldenEye) -\u003e 9 (Sudden Death)\n\n// Tag Analysis of Shortest Path\n// 1  *Adventure*|Animation|Children|Comedy|Fantasy\n// 10 *Action*|*Adventure*|Thriller\n// 9  *Action*",
      "dateUpdated": "Jan 4, 2016 3:59:05 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451352094574_-1165080675",
      "id": "20151229-012134_1599229617",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "similarItemsAboveThresholdEdgeRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] \u003d MapPartitionsRDD[342] at map at \u003cconsole\u003e:50\ngraph: org.apache.spark.graphx.Graph[Long,Double] \u003d org.apache.spark.graphx.impl.GraphImpl@56e739f5\nedgeCount: Long \u003d 19\nsrc: Int \u003d 1\ndest: Int \u003d 9\nshortestPathGraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double] \u003d org.apache.spark.graphx.impl.GraphImpl@81a3c91\nshortestPathFromSrcToDest: List[Any] \u003d List(1.7976931348623157E308, List())\n"
      },
      "dateCreated": "Dec 29, 2015 1:21:34 AM",
      "dateStarted": "Jan 4, 2016 3:59:05 AM",
      "dateFinished": "Jan 4, 2016 3:59:07 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Approximate Jaccard Similarity Between All Distinct Item Pairs",
      "text": "val numHashes \u003d 10\nval minApproxJaccardSimilarityThreshold \u003d 0.001\n\nval numBands \u003d MinHasher.pickBands(minApproxJaccardSimilarityThreshold, numHashes)\nval minHasher \u003d new MinHasher32(numHashes, numBands)\n\nval approxAllItemPairsRDD \u003d itemsRDD.cartesian(itemsRDD)\n\n// Filter out duplicates and preserve only the pairs with left id \u003c right id\nval approxDistinctItemPairsRDD \u003d approxAllItemPairsRDD.filter(itemPair \u003d\u003e itemPair._1.id \u003c itemPair._2.id)",
      "dateUpdated": "Jan 4, 2016 3:59:47 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451868610382_946176993",
      "id": "20160104-005010_1089206854",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "numHashes: Int \u003d 10\nminApproxJaccardSimilarityThreshold: Double \u003d 0.001\nnumBands: Int \u003d 23\nminHasher: com.twitter.algebird.MinHasher32 \u003d com.twitter.algebird.MinHasher32@61ca2859\napproxAllItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d CartesianRDD[23] at cartesian at \u003cconsole\u003e:41\napproxDistinctItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d MapPartitionsRDD[24] at filter at \u003cconsole\u003e:44\n"
      },
      "dateCreated": "Jan 4, 2016 12:50:10 AM",
      "dateStarted": "Jan 4, 2016 3:55:41 AM",
      "dateFinished": "Jan 4, 2016 3:55:42 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Calculate Approx Jaccard Similarity with LSH between all distinct item pairs\n// Only keep pairs with a Jaccard Similarity above a specific threshold\nval approxSimilarItemsAboveThresholdRDD \u003d approxDistinctItemPairsRDD.flatMap(itemPair \u003d\u003e {\n  val jaccardSim \u003d getApproxLSHJaccardSimilarity(minHasher, itemPair._1, itemPair._2)\n  if (jaccardSim \u003e\u003d minApproxJaccardSimilarityThreshold)\n    Some(itemPair._1.id, itemPair._2.id, jaccardSim)\n  else\n    None\n})\n\nval approxSimilarItemPairCount \u003d approxSimilarItemsAboveThresholdRDD.count()\napproxSimilarItemsAboveThresholdRDD.collect().mkString(\",\")",
      "dateUpdated": "Jan 4, 2016 3:55:53 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451867402590_-1419940086",
      "id": "20160104-003002_401758581",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "approxSimilarItemsAboveThresholdRDD: org.apache.spark.rdd.RDD[(Long, Long, Double)] \u003d MapPartitionsRDD[25] at flatMap at \u003cconsole\u003e:56\napproxSimilarItemPairCount: Long \u003d 19\nres29: String \u003d (1,2,0.5),(1,3,0.3),(1,4,0.2),(1,5,0.3),(3,4,0.7),(3,5,0.4),(4,5,0.3),(1,7,0.3),(1,8,0.2),(1,10,0.1),(2,8,0.3),(2,10,0.1),(3,7,1.0),(4,7,0.7),(5,7,0.4),(6,9,0.1),(6,10,0.4),(8,10,0.3),(9,10,0.3)\n"
      },
      "dateCreated": "Jan 4, 2016 12:30:02 AM",
      "dateStarted": "Jan 4, 2016 3:55:53 AM",
      "dateFinished": "Jan 4, 2016 3:55:54 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shortest Path Between 2 Items Based On Approximate Jaccard Similarity Of Item Tags",
      "text": "val approxSimilarItemsAboveThresholdEdgeRDD \u003d approxSimilarItemsAboveThresholdRDD.map(rdd \u003d\u003e {\n  Edge(rdd._1.toLong, rdd._2.toLong, rdd._3.toDouble) \n})\n\n//similarItemsAboveThresholdEdgeRDD.collect()\n\nval graphApprox \u003d Graph.fromEdges(approxSimilarItemsAboveThresholdEdgeRDD, 0L)\nval edgeApproxCount \u003d graphApprox.edges.count\n\nval src \u003d 1\nval dest \u003d 9\n// Note:  Should go through 1 -\u003e 10 -\u003e 9\n\n// Generate Shortest Path for all nodes in the graph\nval shortestPathGraphApprox \u003d dijkstra(graphApprox, src)\n\n// Filter out only the ones with dest as the destination vertex\nval shortestPathFromSrcToDestApprox \u003d shortestPathGraphApprox.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2\n\n// Example\n//\n// Shortest Path\n// 1 (Toy Story) -\u003e 10 (GoldenEye) -\u003e 9 (Sudden Death)\n\n// Tag Analysis of Shortest Path\n// 1  *Adventure*|Animation|Children|Comedy|Fantasy\n// 10 *Action*|*Adventure*|Thriller\n// 9  *Action*",
      "dateUpdated": "Jan 4, 2016 4:01:15 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451868230209_379538742",
      "id": "20160104-004350_442413466",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "approxSimilarItemsAboveThresholdEdgeRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] \u003d MapPartitionsRDD[1097] at map at \u003cconsole\u003e:57\ngraphApprox: org.apache.spark.graphx.Graph[Long,Double] \u003d org.apache.spark.graphx.impl.GraphImpl@65f733ab\nedgeApproxCount: Long \u003d 19\nsrc: Int \u003d 1\ndest: Int \u003d 9\nshortestPathGraphApprox: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double] \u003d org.apache.spark.graphx.impl.GraphImpl@1dc915f8\nshortestPathFromSrcToDestApprox: List[Any] \u003d List(1.7976931348623157E308, List())\n"
      },
      "dateCreated": "Jan 4, 2016 12:43:50 AM",
      "dateStarted": "Jan 4, 2016 4:01:15 AM",
      "dateFinished": "Jan 4, 2016 4:01:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Power Iteration Clustering of Items based on Tag Jaccard Similarity",
      "text": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\n\nval clustering \u003d new PowerIterationClustering().setK(5).setMaxIterations(10)\n\nval clusteringModel \u003d clustering.run(similarItemsAboveThresholdRDD)\n\nval clusterAssignmentsRDD \u003d clusteringModel.assignments.map { assignment \u003d\u003e\n  (assignment.id, assignment.cluster)\n}",
      "dateUpdated": "Jan 4, 2016 4:00:44 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451334971967_402230190",
      "id": "20151228-203611_236636775",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\nclustering: org.apache.spark.mllib.clustering.PowerIterationClustering \u003d org.apache.spark.mllib.clustering.PowerIterationClustering@35e26a20\nclusteringModel: org.apache.spark.mllib.clustering.PowerIterationClusteringModel \u003d org.apache.spark.mllib.clustering.PowerIterationClusteringModel@766786ba\nclusterAssignmentsRDD: org.apache.spark.rdd.RDD[(Long, Int)] \u003d MapPartitionsRDD[790] at map at \u003cconsole\u003e:56\n"
      },
      "dateCreated": "Dec 28, 2015 8:36:11 PM",
      "dateStarted": "Jan 4, 2016 4:00:44 AM",
      "dateFinished": "Jan 4, 2016 4:00:46 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert the clusterAssignmentsRDD into a DataFrame",
      "text": "val schema \u003d StructType(StructField(\"itemId\", LongType, true) :: StructField(\"clusterId\", IntegerType, true) :: Nil)\n\nval clusterAssignmentsRowRDD \u003d clusterAssignmentsRDD.map(clusterAssignmentRDD \u003d\u003e \n  Row(clusterAssignmentRDD._1, clusterAssignmentRDD._2))\n\nval clusterAssignmentsDF \u003d sqlContext.createDataFrame(clusterAssignmentsRowRDD, schema)",
      "dateUpdated": "Jan 4, 2016 2:58:27 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451335712814_493669088",
      "id": "20151228-204832_49888518",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "schema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(itemId,LongType,true), StructField(clusterId,IntegerType,true))\n\u003cconsole\u003e:36: error: not found: value clusterAssignmentsRDD\n       val clusterAssignmentsRowRDD \u003d clusterAssignmentsRDD.map(clusterAssignmentRDD \u003d\u003e \n                                      ^\n"
      },
      "dateCreated": "Dec 28, 2015 8:48:32 PM",
      "dateStarted": "Jan 4, 2016 2:58:52 AM",
      "dateFinished": "Jan 4, 2016 2:58:52 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of items within a cluster",
      "text": "val joinedItemsClustersCountDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").groupBy($\"clusterId\", $\"tags\")\n  .agg(count($\"itemId\")).sort($\"clusterId\" desc)\n\nz.show(joinedItemsClustersCountDF)",
      "dateUpdated": "Jan 4, 2016 2:58:27 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count(itemId)",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451352986034_-1943585352",
      "id": "20151229-013626_1831771232",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:37: error: not found: value clusterAssignmentsDF\n       val joinedItemsClustersCountDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n                                        ^\n"
      },
      "dateCreated": "Dec 29, 2015 1:36:26 AM",
      "dateStarted": "Jan 4, 2016 2:58:52 AM",
      "dateFinished": "Jan 4, 2016 2:58:52 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cluster Details",
      "text": "// Enrich the cluster assignment tuples with itemsDF\nval joinedItemsClustersDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").select($\"itemId\", $\"clusterId\", $\"title\", $\"tags\").sort($\"clusterId\")\n  \nz.show(joinedItemsClustersDF)",
      "dateUpdated": "Jan 4, 2016 2:58:27 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 474.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451348226921_-115634507",
      "id": "20151229-001706_1772528099",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:38: error: not found: value clusterAssignmentsDF\n       val joinedItemsClustersDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n                                   ^\n"
      },
      "dateCreated": "Dec 29, 2015 12:17:06 AM",
      "dateStarted": "Jan 4, 2016 2:58:52 AM",
      "dateFinished": "Jan 4, 2016 2:58:52 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Show the intersection of tags within each cluster\nval clusterTagIntersectionDF \u003d joinedItemsClustersDF.explode(\"tags\", \"tag\"){c: List[String] \u003d\u003e c}",
      "dateUpdated": "Jan 4, 2016 2:58:28 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451350991110_1019547650",
      "id": "20151229-010311_1992313547",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:36: error: not found: value joinedItemsClustersDF\n       val clusterTagIntersectionDF \u003d joinedItemsClustersDF.explode(\"tags\", \"tag\"){c: List[String] \u003d\u003e c}\n                                      ^\n"
      },
      "dateCreated": "Dec 29, 2015 1:03:11 AM",
      "dateStarted": "Jan 4, 2016 2:58:52 AM",
      "dateFinished": "Jan 4, 2016 2:58:52 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Given an itemId (or List of itemIds), recommend more itemIds based on similar cluster \n//        and/or closest jaccard similarity ",
      "dateUpdated": "Jan 4, 2016 2:58:28 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451397463773_538617890",
      "id": "20151229-135743_947751301",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 29, 2015 1:57:43 PM",
      "dateStarted": "Jan 4, 2016 2:58:52 AM",
      "dateFinished": "Jan 4, 2016 2:58:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 4, 2016 2:58:28 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451400014910_399953924",
      "id": "20151229-144014_562495240",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Dec 29, 2015 2:40:14 PM",
      "dateStarted": "Jan 4, 2016 2:58:52 AM",
      "dateFinished": "Jan 4, 2016 2:58:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Graph/04: Item Similarity Pathways by Tag (Dijkstra)",
  "id": "2BA4YA8KU",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}