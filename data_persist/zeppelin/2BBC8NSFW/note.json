{
  "paragraphs": [
    {
      "text": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940661_1074808557",
      "id": "20160110-062220_1037017587",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load dataset including tags",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940661_1074808557",
      "id": "20160110-062220_1140620746",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [category: string, description: string, id: bigint, img: string, tags: array\u003cstring\u003e, title: string]\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Number Of Distinct Tags Found In The Dataset",
      "text": "val distinctCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").distinct()\ndistinctCounts.count()",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940661_1074808557",
      "id": "20160110-062220_1159540264",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "distinctCounts: org.apache.spark.sql.DataFrame \u003d [tag: string]\nres141: Long \u003d 108\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution Of Tags Within Dataset",
      "text": "val tagCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").groupBy($\"tag\")\n  .agg(count($\"tag\").as(\"count\"))\n  .orderBy($\"count\".desc)\n  .limit(10)\n\nz.show(tagCounts)",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "pieChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_317412322",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "tag\tcount\nJVM\t31\nJava\t31\nSQL\t24\nHadoop\t16\nLibrary\t15\nDatabase\t12\nData Procesing Execution Engine\t11\nPython\t11\nSpark\t9\nUI\t8\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Item Domain Class",
      "text": "// Note:  This class must be defined in a separate cell from where it\u0027s being used, otherwise you\u0027ll see this error:\n//   https://fluxcapacitor.zendesk.com/hc/en-us/articles/215310168\n\ncase class Item(id: Long, title: String, description: String, img: String, tags: Set[String]) {\n  override def toString: String \u003d id + \", \" + title + \", \" + tags\n}",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_1382791285",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class Item\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert Row to Item",
      "text": "import org.apache.spark.sql.Row\n\n// Convert from RDD[Row] to RDD[Item]\nval itemsRDD \u003d itemsDF.select($\"id\", $\"title\", $\"description\", $\"img\", $\"tags\").map(row \u003d\u003e\n  Item(row.getLong(0), row.getString(1), row.getString(2), row.getString(3), row.getSeq[String](4).toSet)\n)",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_1918475743",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nitemsRDD: org.apache.spark.rdd.RDD[Item] \u003d MapPartitionsRDD[1375] at map at \u003cconsole\u003e:78\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Approximate LSH Jaccard Similarity Algo",
      "text": "def getApproxLSHJaccardSimilarity(minHasher: MinHasher32, item1: Item, item2: Item): Double \u003d {\n  val minHashSignatureItem1 \u003d item1.tags.map(tag \u003d\u003e minHasher.init(tag))\n    .reduce((leftMinHashSignature, rightMinHashSignature) \u003d\u003e minHasher.plus(leftMinHashSignature, rightMinHashSignature))\n\n  val minHashSignatureItem2 \u003d item2.tags.map(tag \u003d\u003e minHasher.init(tag))\n    .reduce((leftMinHashSignature, rightMinHashSignature) \u003d\u003e minHasher.plus(leftMinHashSignature, rightMinHashSignature))\n\n  minHasher.similarity(minHashSignatureItem1, minHashSignatureItem2).toDouble\n}",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_1578729938",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "getApproxLSHJaccardSimilarity: (minHasher: com.twitter.algebird.MinHasher32, item1: Item, item2: Item)Double\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Approx Jaccard Similarity between all Distinct item pairs",
      "text": "val numHashes \u003d 100\nval minApproxJaccardSimilarityThreshold \u003d 0.1\n\nval numBands \u003d MinHasher.pickBands(minApproxJaccardSimilarityThreshold, numHashes)\nval minHasher \u003d new MinHasher32(numHashes, numBands)\n\nval allItemPairsRDD \u003d itemsRDD.cartesian(itemsRDD)\n\n// Filter out duplicates and preserve only the pairs with left id \u003c right id\n//val approxDistinctItemPairsRDD \u003d allItemPairsRDD.filter(itemPair \u003d\u003e itemPair._1.id \u003c itemPair._2.id)\n//approxDistinctItemPairsRDD.count()\n\n// Calculate Jaccard Similarity between all item pairs (cartesian, then de-duped)\n// Only keep pairs with a Jaccard Similarity above a specific threshold\nval approxSimilarItemsAboveThresholdRDD \u003d allItemPairsRDD.flatMap(itemPair \u003d\u003e {\n  val approxJaccardSim \u003d getApproxLSHJaccardSimilarity(minHasher, itemPair._1, itemPair._2)\n  if (approxJaccardSim \u003e\u003d minApproxJaccardSimilarityThreshold)\n    Some(itemPair._1.id, itemPair._2.id, approxJaccardSim)\n  else\n    None\n})\n\nval approxSimilarItemPairCount \u003d approxSimilarItemsAboveThresholdRDD.count()\napproxSimilarItemsAboveThresholdRDD.collect().mkString(\",\")",
      "dateUpdated": "Jan 16, 2016 10:54:53 PM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_379956656",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "numHashes: Int \u003d 100\nminApproxJaccardSimilarityThreshold: Double \u003d 0.1\n\u003cconsole\u003e:78: error: not found: value MinHasher\n       val numBands \u003d MinHasher.pickBands(minApproxJaccardSimilarityThreshold, numHashes)\n                      ^\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "dateStarted": "Jan 16, 2016 10:54:54 PM",
      "dateFinished": "Jan 16, 2016 10:54:55 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_427955969",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dijkstra: (graph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Double], srcId: org.apache.spark.graphx.VertexId)org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double]\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Similarity Pathway",
      "text": "val approxSimilarItemsAboveThresholdEdgsRDD \u003d approxSimilarItemsAboveThresholdRDD.map(rdd \u003d\u003e {\n  Edge(rdd._1, rdd._2, rdd._3) \n})\n\nval approxGraph \u003d Graph.fromEdges(approxSimilarItemsAboveThresholdEdgsRDD, 0L)",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_655087243",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "approxSimilarItemsAboveThresholdEdgsRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] \u003d MapPartitionsRDD[7198] at map at \u003cconsole\u003e:93\napproxGraph: org.apache.spark.graphx.Graph[Long,Double] \u003d org.apache.spark.graphx.impl.GraphImpl@7f2eeb4e\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Example 1\n//\n// Shortest Path\n// 21 (ElasticSearch) -0.15-\u003e 28 (ZooKeeper) -0.1-\u003e 56 (MicroStrategy)\n\n// Tag Analysis of Shortest Path\n//\n// 21 Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations\n// 28 Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy\n// 56 BI, UI, Visualization, SQL\n\nval src \u003d 21 // ElasticSearch\nval dest \u003d 56 // MicroStrategy\n\nval approxShortestPathGraph \u003d dijkstra(approxGraph, src)\n\n// Filter out only the ones with dest as the destination vertex\nval approxShortestPathFromSrcToDest \u003d approxShortestPathGraph.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_890160297",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "src: Int \u003d 21\ndest: Int \u003d 56\napproxShortestPathGraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double] \u003d org.apache.spark.graphx.impl.GraphImpl@4cfe7cf6\napproxShortestPathFromSrcToDest: List[Any] \u003d List(0.25, List(21, 28))\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Example 2\n//\n// Shortest Path\n// 35 (NLTK) -\u003e 37 (iPython/Jupyter) -\u003e 41 (SQL) -\u003e 42 (Scala)\n//\n// Tag Analysis of Shortest Path\n//\n// 35:  Library, NLP, Python, Text Analytics\n// 9:   Library, Java, JVM, Graph Analytics, Batch (Apache Giraph)\n// 42:  Programming Language, Functional, JVM, Static Typing, Compiled\n\n// 37:  Notebook, Python, Java, Scala, R, Visualization, SQL\n// 41:  Programming Language, SQL, RDBMS, Interpreted\n\nval src \u003d 35 // ElasticSearch\nval dest \u003d 42 // MicroStrategy\n\nval approxShortestPathGraph \u003d dijkstra(approxGraph, src)\n\n// Filter out only the ones with dest as the destination vertex\nval approxShortestPathFromSrcToDest \u003d approxShortestPathGraph.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_251475861",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "src: Int \u003d 35\ndest: Int \u003d 42\napproxShortestPathGraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double] \u003d org.apache.spark.graphx.impl.GraphImpl@4ccebe0b\napproxShortestPathFromSrcToDest: List[Any] \u003d List(0.27, List(35, 9))\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Power Iteration Clustering Of Items Based On Approx LSH Tag Jaccard Similarity",
      "text": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\n\nval clustering \u003d new PowerIterationClustering().setK(5).setMaxIterations(10)\n\nval clusteringModel \u003d clustering.run(approxSimilarItemsAboveThresholdRDD)\n\nval clusterAssignmentsRDD \u003d clusteringModel.assignments.map { assignment \u003d\u003e\n  (assignment.id, assignment.cluster)\n}",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_122553172",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\nclustering: org.apache.spark.mllib.clustering.PowerIterationClustering \u003d org.apache.spark.mllib.clustering.PowerIterationClustering@6e422193\nclusteringModel: org.apache.spark.mllib.clustering.PowerIterationClusteringModel \u003d org.apache.spark.mllib.clustering.PowerIterationClusteringModel@78dd14a4\nclusterAssignmentsRDD: org.apache.spark.rdd.RDD[(Long, Int)] \u003d MapPartitionsRDD[2641] at map at \u003cconsole\u003e:98\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert The ClusterAssignmentsRDD Into A DataFrame",
      "text": "val schema \u003d StructType(StructField(\"itemId\", LongType, true) :: StructField(\"clusterId\", IntegerType, true) :: Nil)\n\nval clusterAssignmentsRowRDD \u003d clusterAssignmentsRDD.map(clusterAssignmentRDD \u003d\u003e \n  Row(clusterAssignmentRDD._1, clusterAssignmentRDD._2))\n\nval clusterAssignmentsDF \u003d sqlContext.createDataFrame(clusterAssignmentsRowRDD, schema)",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_2114466488",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "schema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(itemId,LongType,true), StructField(clusterId,IntegerType,true))\nclusterAssignmentsRowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[2642] at map at \u003cconsole\u003e:100\nclusterAssignmentsDF: org.apache.spark.sql.DataFrame \u003d [itemId: bigint, clusterId: int]\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of items within a cluster",
      "text": "val joinedItemsClustersCountDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").groupBy($\"clusterId\", $\"tags\")\n  .agg(count($\"itemId\")).sort($\"clusterId\" desc)\n\nz.show(joinedItemsClustersCountDF)",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count(itemId)",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_1038125660",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "clusterId\ttags\tcount(itemId)\n4\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\t1\n4\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\t1\n4\tWrappedArray(Library, Streaming, AWS)\t1\n4\tWrappedArray(Library, Python, Machine Learning)\t1\n4\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\t1\n4\tWrappedArray(Library, Graph Analytics, Spark)\t1\n4\tWrappedArray(BI, UI, Visualization, SQL)\t2\n4\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\t1\n4\tWrappedArray(Database, Data Warehouse, SQL)\t2\n4\tWrappedArray(Library, Spark, Streaming)\t1\n4\tWrappedArray(Library, Spark, HiveQL, SQL)\t1\n4\tWrappedArray(Library, Deep Learning, Neural Networks)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\t1\n4\tWrappedArray(Library, Spark, Machine Learning)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\t1\n4\tWrappedArray(Database, SQL, RDBMS, Transactional)\t3\n4\tWrappedArray(Cluster Resource Manager, Docker, Container)\t1\n4\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\t1\n3\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\t1\n3\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\t1\n3\tWrappedArray(Library, Machine Learning, Java, JVM)\t1\n3\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\t1\n3\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\t1\n3\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\t1\n3\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\t1\n3\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\t1\n3\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\t1\n3\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\t1\n3\tWrappedArray(Library, NLP, Python, Text Analytics)\t1\n2\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\t1\n2\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\t4\n2\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\t1\n2\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\t1\n2\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\t1\n2\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\t1\n1\tWrappedArray(Cloud Provider, AWS)\t1\n1\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\t2\n1\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\t1\n1\tWrappedArray(Container, Linux, DevOps, Deployment)\t1\n1\tWrappedArray(Cloud Provider, Google)\t1\n1\tWrappedArray(Cloud Provider, Microsoft)\t1\n1\tWrappedArray(File Format, Key Value Store)\t2\n1\tWrappedArray(File Format)\t1\n1\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\t1\n1\tWrappedArray(File Format, Evolving Schema, Nested Schema)\t1\n1\tWrappedArray(Cloud Provider, Data Center)\t1\n0\tWrappedArray(Library, Java, JVM, Log Collection)\t1\n0\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\t1\n0\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\t1\n0\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\t1\n0\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n0\tWrappedArray(Library, Search, Java, JVM, Python)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\t1\n0\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\t1\n0\tWrappedArray(Library, Graph Analytics, Java, JVM)\t1\n0\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\t1\n0\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\t1\n0\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\t1\n0\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\t1\n0\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\t1\n0\tWrappedArray(Data Import, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\t1\n0\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\t1\n0\tWrappedArray(File System, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Streaming, Java, JVM)\t1\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cluster Details",
      "text": "// Enrich the cluster assignment tuples with itemsDF\nval joinedItemsClustersDF \u003d clusterAssignmentsDF.select($\"clusterId\", $\"itemId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").select($\"itemId\", $\"clusterId\", $\"title\", $\"tags\").sort($\"clusterId\", $\"itemId\")\n  \nz.show(joinedItemsClustersDF)",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 474.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_1522947050",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "itemId\tclusterId\ttitle\ttags\n6\t0\tApache Flink\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\n7\t0\tApache Spark\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\n8\t0\tApache Flume\tWrappedArray(Library, Java, JVM, Log Collection)\n9\t0\tApache Giraph\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\n10\t0\tApache HDFS\tWrappedArray(File System, Hadoop, Java, JVM)\n11\t0\tApache YARN\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\n12\t0\tApache HBase\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\n13\t0\tApache MapReduce\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\n16\t0\tApache HUE\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\n18\t0\tApache Kafka\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\n19\t0\tApache Lucene\tWrappedArray(Library, Search, Java, JVM, Python)\n20\t0\tApache Solr\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\n21\t0\tElasticSearch\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\n27\t0\tApache Pig\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n28\t0\tApache ZooKeeper\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n29\t0\tStanford CoreNLP\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\n30\t0\tApache Tez\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\n31\t0\tApache Storm\tWrappedArray(Streaming, Java, JVM)\n32\t0\tApache Sqoop\tWrappedArray(Data Import, Hadoop, Java, JVM)\n33\t0\tApache Oozie\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\n34\t0\tApache Nifi\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\n38\t0\tApache Zeppelin\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\n74\t0\tNeo4j\tWrappedArray(Library, Graph Analytics, Java, JVM)\n79\t0\tTitan GraphDB\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\n2\t1\tTachyon\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\n4\t1\tDocker\tWrappedArray(Container, Linux, DevOps, Deployment)\n5\t1\tMicrosft Azure\tWrappedArray(Cloud Provider, Microsoft)\n40\t1\tR\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n44\t1\tPython\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n49\t1\tAmazon Web Services\tWrappedArray(Cloud Provider, AWS)\n50\t1\tGoogle Cloud Platform\tWrappedArray(Cloud Provider, Google)\n51\t1\tRedis\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\n52\t1\tJSON\tWrappedArray(File Format, Key Value Store)\n53\t1\tXML\tWrappedArray(File Format, Key Value Store)\n55\t1\tOn-Premise\tWrappedArray(Cloud Provider, Data Center)\n64\t1\tCSV\tWrappedArray(File Format)\n76\t1\tProtobuffers\tWrappedArray(File Format, Evolving Schema, Nested Schema)\n15\t2\tHortonworks\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n26\t2\tApache ORC\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\n41\t2\tSQL\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\n46\t2\tMapR\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n47\t2\tCloudera\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n48\t2\tIBM BigInsights\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n66\t2\tRedshift\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\n68\t2\tDynamoDB\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\n77\t2\tS3\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\n1\t3\tApache Cassandra\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\n14\t3\tApache Hive\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\n22\t3\tApache Mahout\tWrappedArray(Library, Machine Learning, Java, JVM)\n25\t3\tApache Parquet\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\n35\t3\tNLTK\tWrappedArray(Library, NLP, Python, Text Analytics)\n37\t3\tiPython/Jupyter\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\n42\t3\tScala\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\n43\t3\tJava\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\n45\t3\tPresto\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\n71\t3\tElastic MapReduce\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\n73\t3\tMemcached\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\n3\t4\tApache Ambari\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\n17\t4\tApache Impala\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\n23\t4\tApache Drill\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\n24\t4\tApache Mesos\tWrappedArray(Cluster Resource Manager, Docker, Container)\n36\t4\tSci-Kit Learn\tWrappedArray(Library, Python, Machine Learning)\n39\t4\tTableau\tWrappedArray(BI, UI, Visualization, SQL)\n54\t4\tMongoDB\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\n56\t4\tMicroStrategy\tWrappedArray(BI, UI, Visualization, SQL)\n57\t4\tKnime\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\n59\t4\tOracle\tWrappedArray(Database, SQL, RDBMS, Transactional)\n60\t4\tMySQL\tWrappedArray(Database, SQL, RDBMS, Transactional)\n61\t4\tSpark ML/MLlib\tWrappedArray(Library, Spark, Machine Learning)\n62\t4\tSpark Streaming\tWrappedArray(Library, Spark, Streaming)\n63\t4\tSpark SQL\tWrappedArray(Library, Spark, HiveQL, SQL)\n65\t4\tDeep Learning 4J\tWrappedArray(Library, Deep Learning, Neural Networks)\n67\t4\tKinesis\tWrappedArray(Library, Streaming, AWS)\n69\t4\tSpark GraphX\tWrappedArray(Library, Graph Analytics, Spark)\n70\t4\tSQL Server\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\n72\t4\tDato GraphLab Create\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\n75\t4\tPostgres\tWrappedArray(Database, SQL, RDBMS, Transactional)\n78\t4\tTensor Flow\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\n80\t4\tTeradata\tWrappedArray(Database, Data Warehouse, SQL)\n81\t4\tVertica\tWrappedArray(Database, Data Warehouse, SQL)\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Show the intersection of tags within each cluster\nval clusterTagIntersectionDF \u003d joinedItemsClustersDF.explode(\"tags\", \"tag\"){c: List[String] \u003d\u003e c}",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_770027333",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "clusterTagIntersectionDF: org.apache.spark.sql.DataFrame \u003d [itemId: bigint, clusterId: int, title: string, tags: array\u003cstring\u003e, tag: string]\n"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Given an itemId (or List of itemIds), recommend more itemIds based on similar cluster \n//        and/or closest jaccard similarity ",
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940662_1075962804",
      "id": "20160110-062220_315082163",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 10, 2016 6:22:20 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406940663_1075578055",
      "id": "20160110-062220_726405085",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 10, 2016 6:22:20 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Live Recs/05: TODO LSH Similarity Item-to-Item Recs by (itemId, userId) Pairs",
  "id": "2BBC8NSFW",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}