{
  "paragraphs": [
    {
      "title": "Load dataset including tags",
      "text": "val itemsDF \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"file:/root/pipeline/datasets/movielens/ml-latest/ratings-lrg.csv\").toDF(\"userId\", \"movieId\", \"rating\", \"timestamp\")\n  \nz.show(itemsDF)",
      "dateUpdated": "Jan 15, 2016 9:11:33 PM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_163005686",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.csv. Please find packages at http://spark-packages.org\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:77)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:102)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:30)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:35)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:37)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:39)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:41)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:43)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:45)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:47)\n\tat \u003cinit\u003e(\u003cconsole\u003e:49)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:53)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.csv.DefaultSource\n\tat scala.tools.nsc.interpreter.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:83)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try.orElse(Try.scala:82)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:62)\n\t... 41 more\n\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "dateStarted": "Jan 15, 2016 9:11:33 PM",
      "dateFinished": "Jan 15, 2016 9:11:46 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert CSV-based DataFrame into TaggedItems",
      "text": "import org.apache.spark.sql.Row\nimport com.twitter.algebird.{ MinHasher, MinHasher32, MinHashSignature }\n\n//import com.advancedspark.ml.TaggedItem\n\n//val itemsRDD \u003d itemsDF.select($\"id\", $\"title\", $\"tags\").map(row \u003d\u003e {\n//  val id \u003d row.getInt(0)\n//  val title \u003d row.getString(1)\n//  val tags \u003d row.getString(2).trim.split(\"\\\\|\")\n//  TaggedItem(id, title, tags)\n//}).cache()\n\nval numHashes \u003d 5\nval numBands \u003d 1\n//val minApproxJaccardSimilarityThreshold \u003d 0.1\n//val numBands \u003d MinHasher.pickBands(minApproxJaccardSimilarityThreshold, numHashes)\n\nval minHasher \u003d new MinHasher32(numHashes, numBands)\n\ncase class MinHashTaggedItem(id: Long, title: String, combinedTagSignature: MinHashSignature) {\n  override def toString: String \u003d id + \", \" + title + \", \" + combinedTagSignature\n}\n\nval itemsRDD \u003d itemsDF.select($\"id\", $\"title\", $\"tags\").map(row \u003d\u003e {\n  val id \u003d row.getInt(0)\n  val title \u003d row.getString(1)\n  val tagSignatures \u003d row.getString(2).trim.split(\"\\\\|\")\n    .map(tag \u003d\u003e minHasher.init(tag))\n  val combinedTagSignature \u003d minHasher.sum(tagSignatures)\n  \n  MinHashTaggedItem(id, title, combinedTagSignature)\n}).cache()\n\nitemsRDD.count()\nitemsRDD.collect()\n\nval itemBucketsDF \u003d itemsRDD.flatMap(item \u003d\u003e {\n  val buckets \u003d minHasher.buckets(item.combinedTagSignature)\n  buckets.map(bucket \u003d\u003e \n    (bucket, Seq((item.id, item.title, item.combinedTagSignature))) // Using Seq so we can reduce\n  )\n}).reduceByKey(_ ++ _).toDF()\n\nz.show(itemBucketsDF)\n\n//.collect()\n//  .zipWithIndex.map((bucketId, id) \u003d\u003e println)\n//  .collect().size\n",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "_1",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "_2",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "_1",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "_2",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_571655084",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "_1\t_2\n-3120545266605136803\tWrappedArray([1,Toy Story (1995),[[B@72850f91]])\n-4277191694920311810\tWrappedArray([4,Waiting to Exhale (1995),[[B@71faf0f7]])\n5689405751688691415\tWrappedArray([2,Jumanji (1995),[[B@3fd52e69]])\n737254150484926729\tWrappedArray([9,Sudden Death (1995),[[B@5357df82]])\n908363697777081067\tWrappedArray([10,GoldenEye (1995),[[B@9f4abfe]])\n-6361927801039948658\tWrappedArray([8,Tom and Huck (1995),[[B@2f2f9f7f]])\n-3059970959338047050\tWrappedArray([5,Father of the Bride Part II (1995),[[B@399f4a95]])\n-6637841855156262341\tWrappedArray([6,Heat (1995),[[B@54d939eb]])\n-8962056707521183219\tWrappedArray([3,Grumpier Old Men (1995),[[B@78dec3d5]], [7,Sabrina (1995),[[B@2bd96fd6]])\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of tags within dataset",
      "text": "val tagsCountRDD \u003d itemsRDD.flatMap(item \u003d\u003e item.tags).map(tag \u003d\u003e (tag,1)).reduceByKey(_ + _)\nz.show(tagsCountRDD.toDF(\"tag\", \"count\"))\n\n//tagCountsRDD.collect().mkString(\"\\n\")",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "pieChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_947129605",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:115: error: value tags is not a member of MinHashTaggedItem\n         val tagsCountRDD \u003d itemsRDD.flatMap(item \u003d\u003e item.tags).map(tag \u003d\u003e (tag,1)).reduceByKey(_ + _)\n                                                          ^\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Jaccard Similarity between all Items above a threshold",
      "text": "import com.advancedspark.ml.Similarity\nimport com.twitter.algebird.{ MinHasher, MinHasher32, MinHashSignature }\n\nval numHashes \u003d 10\nval numBands \u003d 5\n//val minApproxJaccardSimilarityThreshold \u003d 0.1\n//val numBands \u003d MinHasher.pickBands(minApproxJaccardSimilarityThreshold, numHashes)\n\nval minHasher \u003d new MinHasher32(numHashes, numBands)\n\n//itemsRDD.map(item \u003d\u003e (item.id, item.tags)).flatMap()\n//  .collect()\n\n// Convert items into items \nitemsRDD.flatMap(item \u003d\u003e \n  item.tags.map(tag \u003d\u003e \n    (item.id, minHasher.init(tag))\n  )\n).reduceByKey((minHash1, minHash2) \u003d\u003e \n  minHasher.plus(minHash1, minHash2)\n)\n\n\n\n",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "item1",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "item2",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "item1",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "item2",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_1513870588",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import com.advancedspark.ml.Similarity\nimport com.twitter.algebird.{MinHasher, MinHasher32, MinHashSignature}\nnumHashes: Int \u003d 10\nnumBands: Int \u003d 5\nminHasher: com.twitter.algebird.MinHasher32 \u003d com.twitter.algebird.MinHasher32@39ca3d8d\nres219: Array[(Long, com.twitter.algebird.MinHashSignature)] \u003d Array((4,MinHashSignature([B@60c36eb9)), (6,MinHashSignature([B@496ed075)), (8,MinHashSignature([B@628e087d)), (10,MinHashSignature([B@4bd92fce)), (2,MinHashSignature([B@309436aa)), (1,MinHashSignature([B@f46b5b5)), (3,MinHashSignature([B@1d9989c0)), (7,MinHashSignature([B@1fbc62e2)), (9,MinHashSignature([B@e8c26d9)), (5,MinHashSignature([B@34ddd1a0)))\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Prepare for clustering",
      "text": "val similarItemPairIdsAboveThresholdRDD \u003d similarItemPairsAboveThresholdRDD.map(itemPair \u003d\u003e \n  (itemPair._1.id, itemPair._2.id, itemPair._3)\n)",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_1771617209",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "similarItemPairIdsAboveThresholdRDD: org.apache.spark.rdd.RDD[(Long, Long, Double)] \u003d MapPartitionsRDD[2942] at map at \u003cconsole\u003e:113\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cluster Items Using Similarity Pairs with Power Iteration Clustering",
      "text": "// http://spark.apache.org/docs/latest/mllib-clustering.html#power-iteration-clustering-pic\nimport org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\n\nval clustering \u003d new PowerIterationClustering().setK(5).setMaxIterations(10)\n\nval clusteringModel \u003d clustering.run(similarItemPairIdsAboveThresholdRDD)",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_1570970245",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\nclustering: org.apache.spark.mllib.clustering.PowerIterationClustering \u003d org.apache.spark.mllib.clustering.PowerIterationClustering@51ec1c9f\nclusteringModel: org.apache.spark.mllib.clustering.PowerIterationClusteringModel \u003d org.apache.spark.mllib.clustering.PowerIterationClusteringModel@3d012ce5\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Enrich the cluster assignments with item data",
      "text": "val clusterAssignmentsRDD \u003d clusteringModel.assignments.map { assignment \u003d\u003e\n  (assignment.id, assignment.cluster)\n}\n\nval enrichedClusterAssignmentsDF \u003d clusterAssignmentsRDD.toDF(\"itemId\", \"clusterId\")\n  .join(itemsDF, $\"itemId\" \u003d\u003d\u003d $\"id\")\n  .select($\"clusterId\", $\"itemId\", $\"title\", $\"tags\")\n  .sort($\"clusterId\" desc)\n  \nz.show(enrichedClusterAssignmentsDF)",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "itemId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "itemId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_1160194065",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "clusterId\titemId\ttitle\ttags\n4\t9\tSudden Death (1995)\tAction\n4\t6\tHeat (1995)\tAction|Crime|Thriller\n3\t2\tJumanji (1995)\tAdventure|Children|Fantasy\n3\t1\tToy Story (1995)\tAdventure|Animation|Children|Comedy|Fantasy\n2\t10\tGoldenEye (1995)\tAction|Adventure|Thriller\n1\t8\tTom and Huck (1995)\tAdventure|Children\n0\t3\tGrumpier Old Men (1995)\tComedy|Romance\n0\t4\tWaiting to Exhale (1995)\tComedy|Drama|Romance\n0\t5\tFather of the Bride Part II (1995)\tComedy\n0\t7\tSabrina (1995)\tComedy|Romance\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show cluster Distributions",
      "text": "val clusterDistributionDF \u003d joinedClusterAssignmentsDF\n  .select($\"clusterId\", $\"itemId\", $\"tags\")\n  .groupBy($\"clusterId\", $\"tags\")\n  .agg(count($\"itemId\"))\n  .sort($\"clusterId\" desc)\n\nz.show(clusterDistributionDF)",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count(itemId)",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_1094686845",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "clusterId\ttags\tcount(itemId)\n4\tAction|Adventure|Thriller\t1\n3\tComedy|Romance\t2\n2\tComedy|Drama|Romance\t1\n2\tComedy\t1\n1\tAction|Crime|Thriller\t1\n1\tAction\t1\n0\tAdventure|Animation|Children|Comedy|Fantasy\t1\n0\tAdventure|Children|Fantasy\t1\n0\tAdventure|Children\t1\n"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Given an itemId (or List of itemIds), recommend more itemIds based on similar cluster \n//        and/or closest jaccard similarity ",
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_1842783888",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 10, 2016 6:19:57 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452406797377_-2047876457",
      "id": "20160110-061957_686096029",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 10, 2016 6:19:57 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "TODO: Approx LSH (itemId, userId) Similarity",
  "id": "2B8PEZVN5",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}