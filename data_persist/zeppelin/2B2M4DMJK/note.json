{
  "paragraphs": [
    {
      "text": "%md # Join Ratings and Genders\n* Ratings (CSV, Unpartitioned) Joined with Genders (CSV, Unpartitioned)\n* Ratings (JSON, Unpartitioned) Joined with Genders (JSON, Unpartitioned)\n* Ratings (Parquet, Unpartitioned) Joined with Genders (Parquet, Unpartitioned)\n* Ratings (Parquet, Partitioned) Joined with Genders (Parquet, Partitioned) \n* Ratings (Parquet, Partitioned) Joined with Genders (JSON, Unpartitioned)\n* Ratings (JSON, Unpartitioned) Joined with Genders (Parquet, Partitioned)\n* Ratings (Cassandra, Unpartitioned) and Genders (Parquet, Partitioned)\n* Ratings (ORC, Partitioned) Joined with Genders (ORC, Partitioned)\n* Ratings (Avro, Partitioned) Joined with Genders (Avro, Partitioned)\n",
      "dateUpdated": "Dec 30, 2015 3:44:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030075933_787974901",
      "id": "20150923-174115_700622531",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eJoin Ratings and Genders\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eRatings (CSV, Unpartitioned) Joined with Genders (CSV, Unpartitioned)\u003c/li\u003e\n\u003cli\u003eRatings (JSON, Unpartitioned) Joined with Genders (JSON, Unpartitioned)\u003c/li\u003e\n\u003cli\u003eRatings (Parquet, Unpartitioned) Joined with Genders (Parquet, Unpartitioned)\u003c/li\u003e\n\u003cli\u003eRatings (Parquet, Partitioned) Joined with Genders (Parquet, Partitioned)\u003c/li\u003e\n\u003cli\u003eRatings (Parquet, Partitioned) Joined with Genders (JSON, Unpartitioned)\u003c/li\u003e\n\u003cli\u003eRatings (JSON, Unpartitioned) Joined with Genders (Parquet, Partitioned)\u003c/li\u003e\n\u003cli\u003eRatings (Cassandra, Unpartitioned) and Genders (Parquet, Partitioned)\u003c/li\u003e\n\u003cli\u003eRatings (ORC, Partitioned) Joined with Genders (ORC, Partitioned)\u003c/li\u003e\n\u003cli\u003eRatings (Avro, Partitioned) Joined with Genders (Avro, Partitioned)\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 5:41:15 PM",
      "dateStarted": "Dec 30, 2015 3:44:52 PM",
      "dateFinished": "Dec 30, 2015 3:44:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (CSV, Unpartitioned) Joined with Genders (CSV, Unpartitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030032816_26679421",
      "id": "20150923-174032_1267111885",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (CSV, Unpartitioned) Joined with Genders (CSV, Unpartitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 5:40:32 PM",
      "dateStarted": "Dec 30, 2015 3:44:53 PM",
      "dateFinished": "Dec 30, 2015 3:44:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import com.databricks.spark.csv._\n\nval ratingsCsvDF \u003d sqlContext.read.format(\"com.databricks.spark.csv\").load(\"file:/root/pipeline/datasets/dating/ratings.csv.bz2\").toDF(\"fromUserId\", \"toUserId\", \"rating\") \n\nval gendersCsvDF \u003d sqlContext.read.format(\"com.databricks.spark.csv\").load(\"file:/root/pipeline/datasets/dating/genders.csv\").toDF(\"id\", \"gender\")",
      "dateUpdated": "Dec 30, 2015 3:44:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443028720074_-1858292975",
      "id": "20150923-171840_35208043",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import com.databricks.spark.csv._\nratingsCsvDF: org.apache.spark.sql.DataFrame \u003d [fromUserId: string, toUserId: string, rating: string]\ngendersCsvDF: org.apache.spark.sql.DataFrame \u003d [id: string, gender: string]\n"
      },
      "dateCreated": "Sep 23, 2015 5:18:40 PM",
      "dateStarted": "Dec 30, 2015 3:44:52 PM",
      "dateFinished": "Dec 30, 2015 3:45:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\nval middleRatingsUnpartitionedCsvDF \u003d ratingsCsvDF.select($\"toUserId\", $\"rating\").where($\"rating\" \u003c\u003d 6).where($\"rating\" \u003e\u003d 4)\nmiddleRatingsUnpartitionedCsvDF.explain(true)\nmiddleRatingsUnpartitionedCsvDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443028849404_379188800",
      "id": "20150923-172049_1726144913",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "middleRatingsUnpartitionedCsvDF: org.apache.spark.sql.DataFrame \u003d [toUserId: string, rating: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter (\u0027rating \u003e\u003d 4)\n Filter (cast(rating#5 as double) \u003c\u003d cast(6 as double))\n  Project [toUserId#4,rating#5]\n   Project [C0#0 AS fromUserId#3,C1#1 AS toUserId#4,C2#2 AS rating#5]\n    Relation[C0#0,C1#1,C2#2] CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: string, rating: string\nFilter (cast(rating#5 as double) \u003e\u003d cast(4 as double))\n Filter (cast(rating#5 as double) \u003c\u003d cast(6 as double))\n  Project [toUserId#4,rating#5]\n   Project [C0#0 AS fromUserId#3,C1#1 AS toUserId#4,C2#2 AS rating#5]\n    Relation[C0#0,C1#1,C2#2] CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [C1#1 AS toUserId#4,C2#2 AS rating#5]\n Filter ((cast(C2#2 as double) \u003c\u003d 6.0) \u0026\u0026 (cast(C2#2 as double) \u003e\u003d 4.0))\n  Relation[C0#0,C1#1,C2#2] CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Physical Plan \u003d\u003d\nTungstenProject [C1#1 AS toUserId#4,C2#2 AS rating#5]\n Filter ((cast(C2#2 as double) \u003c\u003d 6.0) \u0026\u0026 (cast(C2#2 as double) \u003e\u003d 4.0))\n  Scan CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)[C0#0,C1#1,C2#2]\n\nCode Generation: true\nres5: Long \u003d 4693155\n"
      },
      "dateCreated": "Sep 23, 2015 5:20:49 PM",
      "dateStarted": "Dec 30, 2015 3:44:53 PM",
      "dateFinished": "Dec 30, 2015 3:45:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val unknownGendersUnpartitionedCsvDF \u003d gendersCsvDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").filter(\"gender !\u003d \u0027M\u0027\")\nunknownGendersUnpartitionedCsvDF.explain(true)\nunknownGendersUnpartitionedCsvDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443028915893_-542376761",
      "id": "20150923-172155_1443279058",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "unknownGendersUnpartitionedCsvDF: org.apache.spark.sql.DataFrame \u003d [id: string, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter NOT (\u0027gender \u003d M)\n Filter NOT (gender#9 \u003d F)\n  Project [id#8,gender#9]\n   Project [C0#6 AS id#8,C1#7 AS gender#9]\n    Relation[C0#6,C1#7] CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\nid: string, gender: string\nFilter NOT (gender#9 \u003d M)\n Filter NOT (gender#9 \u003d F)\n  Project [id#8,gender#9]\n   Project [C0#6 AS id#8,C1#7 AS gender#9]\n    Relation[C0#6,C1#7] CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [C0#6 AS id#8,C1#7 AS gender#9]\n Filter (NOT (C1#7 \u003d F) \u0026\u0026 NOT (C1#7 \u003d M))\n  Relation[C0#6,C1#7] CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Physical Plan \u003d\u003d\nTungstenProject [C0#6 AS id#8,C1#7 AS gender#9]\n Filter (NOT (C1#7 \u003d F) \u0026\u0026 NOT (C1#7 \u003d M))\n  Scan CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)[C0#6,C1#7]\n\nCode Generation: true\nres8: Long \u003d 83164\n"
      },
      "dateCreated": "Sep 23, 2015 5:21:55 PM",
      "dateStarted": "Dec 30, 2015 3:45:05 PM",
      "dateFinished": "Dec 30, 2015 3:45:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMiddleRatingsUnpartitionedCsvWithUnknownGendersUnpartitionedCsvDF \u003d\n  middleRatingsUnpartitionedCsvDF.join(unknownGendersUnpartitionedCsvDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\n\njoinMiddleRatingsUnpartitionedCsvWithUnknownGendersUnpartitionedCsvDF.explain(true)\n\njoinMiddleRatingsUnpartitionedCsvWithUnknownGendersUnpartitionedCsvDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443029006249_1313037322",
      "id": "20150923-172326_1171447928",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "joinMiddleRatingsUnpartitionedCsvWithUnknownGendersUnpartitionedCsvDF: org.apache.spark.sql.DataFrame \u003d [toUserId: string, rating: string, id: string, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#4 \u003d id#8))\n Filter (cast(rating#5 as double) \u003e\u003d cast(4 as double))\n  Filter (cast(rating#5 as double) \u003c\u003d cast(6 as double))\n   Project [toUserId#4,rating#5]\n    Project [C0#0 AS fromUserId#3,C1#1 AS toUserId#4,C2#2 AS rating#5]\n     Relation[C0#0,C1#1,C2#2] CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n Filter NOT (gender#9 \u003d M)\n  Filter NOT (gender#9 \u003d F)\n   Project [id#8,gender#9]\n    Project [C0#6 AS id#8,C1#7 AS gender#9]\n     Relation[C0#6,C1#7] CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: string, rating: string, id: string, gender: string\nJoin Inner, Some((toUserId#4 \u003d id#8))\n Filter (cast(rating#5 as double) \u003e\u003d cast(4 as double))\n  Filter (cast(rating#5 as double) \u003c\u003d cast(6 as double))\n   Project [toUserId#4,rating#5]\n    Project [C0#0 AS fromUserId#3,C1#1 AS toUserId#4,C2#2 AS rating#5]\n     Relation[C0#0,C1#1,C2#2] CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n Filter NOT (gender#9 \u003d M)\n  Filter NOT (gender#9 \u003d F)\n   Project [id#8,gender#9]\n    Project [C0#6 AS id#8,C1#7 AS gender#9]\n     Relation[C0#6,C1#7] CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#4 \u003d id#8))\n Project [C1#1 AS toUserId#4,C2#2 AS rating#5]\n  Filter ((cast(C2#2 as double) \u003c\u003d 6.0) \u0026\u0026 (cast(C2#2 as double) \u003e\u003d 4.0))\n   Relation[C0#0,C1#1,C2#2] CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n Project [C0#6 AS id#8,C1#7 AS gender#9]\n  Filter (NOT (C1#7 \u003d F) \u0026\u0026 NOT (C1#7 \u003d M))\n   Relation[C0#6,C1#7] CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)\n\n\u003d\u003d Physical Plan \u003d\u003d\nSortMergeJoin [toUserId#4], [id#8]\n TungstenSort [toUserId#4 ASC], false, 0\n  TungstenExchange hashpartitioning(toUserId#4)\n   TungstenProject [C1#1 AS toUserId#4,C2#2 AS rating#5]\n    Filter ((cast(C2#2 as double) \u003c\u003d 6.0) \u0026\u0026 (cast(C2#2 as double) \u003e\u003d 4.0))\n     Scan CsvRelation(file:/root/pipeline/datasets/dating/ratings.csv.bz2,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)[C0#0,C1#1,C2#2]\n TungstenSort [id#8 ASC], false, 0\n  TungstenExchange hashpartitioning(id#8)\n   TungstenProject [C0#6 AS id#8,C1#7 AS gender#9]\n    Filter (NOT (C1#7 \u003d F) \u0026\u0026 NOT (C1#7 \u003d M))\n     Scan CsvRelation(file:/root/pipeline/datasets/dating/genders.csv,false,,,\",null,#,PERMISSIVE,COMMONS,false,false,null,UTF-8,false)[C0#6,C1#7]\n\nCode Generation: true\nres13: Long \u003d 1123909\n"
      },
      "dateCreated": "Sep 23, 2015 5:23:26 PM",
      "dateStarted": "Dec 30, 2015 3:45:46 PM",
      "dateFinished": "Dec 30, 2015 3:46:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (JSON, Unpartitioned) Joined with Genders (JSON, Unpartitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030236681_134084017",
      "id": "20150923-174356_2076547835",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (JSON, Unpartitioned) Joined with Genders (JSON, Unpartitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 5:43:56 PM",
      "dateStarted": "Dec 30, 2015 3:44:53 PM",
      "dateFinished": "Dec 30, 2015 3:44:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val ratingsUnpartitionedJsonDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/datasets/dating/ratings.json.bz2\")\n\nval gendersUnpartitionedJsonDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/datasets/dating/genders.json.bz2\")",
      "dateUpdated": "Dec 30, 2015 3:44:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1438115408322_-2058772744",
      "id": "20150728-203008_1644505396",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "ratingsUnpartitionedJsonDF: org.apache.spark.sql.DataFrame \u003d [fromUserId: bigint, rating: bigint, toUserId: bigint]\ngendersUnpartitionedJsonDF: org.apache.spark.sql.DataFrame \u003d [gender: string, id: bigint]\n"
      },
      "dateCreated": "Jul 28, 2015 8:30:08 PM",
      "dateStarted": "Dec 30, 2015 3:45:47 PM",
      "dateFinished": "Dec 30, 2015 3:47:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val middleRatingsUnpartitionedJsonDF \u003d ratingsUnpartitionedJsonDF.select($\"toUserId\", $\"rating\")\n  .where($\"rating\" \u003c\u003d 6)\n  .where($\"rating\" \u003e\u003d 4)\n\nmiddleRatingsUnpartitionedJsonDF.explain(true)\n\nmiddleRatingsUnpartitionedJsonDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443021116136_-1680888969",
      "id": "20150923-151156_937416545",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "middleRatingsUnpartitionedJsonDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: bigint]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter (\u0027rating \u003e\u003d 4)\n Filter (rating#29L \u003c\u003d cast(6 as bigint))\n  Project [toUserId#30L,rating#29L]\n   Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: bigint\nFilter (rating#29L \u003e\u003d cast(4 as bigint))\n Filter (rating#29L \u003c\u003d cast(6 as bigint))\n  Project [toUserId#30L,rating#29L]\n   Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [toUserId#30L,rating#29L]\n Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n  Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n\n\u003d\u003d Physical Plan \u003d\u003d\nFilter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n Scan JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2][toUserId#30L,rating#29L]\n\nCode Generation: true\nres20: Long \u003d 4693155\n"
      },
      "dateCreated": "Sep 23, 2015 3:11:56 PM",
      "dateStarted": "Dec 30, 2015 3:46:29 PM",
      "dateFinished": "Dec 30, 2015 3:47:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val unknownGendersUnpartitionedJsonDF \u003d gendersUnpartitionedJsonDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").filter(\"gender !\u003d \u0027M\u0027\")\nunknownGendersUnpartitionedJsonDF.explain(true)\nunknownGendersUnpartitionedJsonDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443023485664_-1245182480",
      "id": "20150923-155125_718142519",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "unknownGendersUnpartitionedJsonDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter NOT (\u0027gender \u003d M)\n Filter NOT (gender#31 \u003d F)\n  Project [id#32L,gender#31]\n   Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\nid: bigint, gender: string\nFilter NOT (gender#31 \u003d M)\n Filter NOT (gender#31 \u003d F)\n  Project [id#32L,gender#31]\n   Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [id#32L,gender#31]\n Filter (NOT (gender#31 \u003d F) \u0026\u0026 NOT (gender#31 \u003d M))\n  Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Physical Plan \u003d\u003d\nFilter (NOT (gender#31 \u003d F) \u0026\u0026 NOT (gender#31 \u003d M))\n Scan JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2][id#32L,gender#31]\n\nCode Generation: true\nres23: Long \u003d 83164\n"
      },
      "dateCreated": "Sep 23, 2015 3:51:25 PM",
      "dateStarted": "Dec 30, 2015 3:47:03 PM",
      "dateFinished": "Dec 30, 2015 3:47:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMiddleRatingsUnpartitionedJsonWithUnknownGendersUnpartitionedJsonDF \u003d middleRatingsUnpartitionedJsonDF.join(unknownGendersUnpartitionedJsonDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\njoinMiddleRatingsUnpartitionedJsonWithUnknownGendersUnpartitionedJsonDF.explain(true)\njoinMiddleRatingsUnpartitionedJsonWithUnknownGendersUnpartitionedJsonDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443029170988_1756465625",
      "id": "20150923-172610_2122485572",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "joinMiddleRatingsUnpartitionedJsonWithUnknownGendersUnpartitionedJsonDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: bigint, id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#30L \u003d id#32L))\n Filter (rating#29L \u003e\u003d cast(4 as bigint))\n  Filter (rating#29L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#30L,rating#29L]\n    Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter NOT (gender#31 \u003d M)\n  Filter NOT (gender#31 \u003d F)\n   Project [id#32L,gender#31]\n    Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: bigint, id: bigint, gender: string\nJoin Inner, Some((toUserId#30L \u003d id#32L))\n Filter (rating#29L \u003e\u003d cast(4 as bigint))\n  Filter (rating#29L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#30L,rating#29L]\n    Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter NOT (gender#31 \u003d M)\n  Filter NOT (gender#31 \u003d F)\n   Project [id#32L,gender#31]\n    Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#30L \u003d id#32L))\n Project [toUserId#30L,rating#29L]\n  Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n   Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Project [id#32L,gender#31]\n  Filter (NOT (gender#31 \u003d F) \u0026\u0026 NOT (gender#31 \u003d M))\n   Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [toUserId#30L], [id#32L], BuildRight\n ConvertToUnsafe\n  Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n   Scan JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2][toUserId#30L,rating#29L]\n ConvertToUnsafe\n  Filter (NOT (gender#31 \u003d F) \u0026\u0026 NOT (gender#31 \u003d M))\n   Scan JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2][id#32L,gender#31]\n\nCode Generation: true\nres26: Long \u003d 1123909\n"
      },
      "dateCreated": "Sep 23, 2015 5:26:10 PM",
      "dateStarted": "Dec 30, 2015 3:47:33 PM",
      "dateFinished": "Dec 30, 2015 3:48:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write out Partitioned and unpartitioned versions of ORC",
      "text": "//ratingsJsonDF.write.format(\"orc\").partitionBy(\"rating\").save(\"file:/root/pipeline/datasets/dating/ratings-partitioned.orc\")\n//gendersJsonDF.write.format(\"orc\").partitionBy(\"gender\").save(\"file:/root/pipeline/datasets/dating/genders-partitioned.orc\")\n//ratingsJsonDF.write.format(\"orc\").save(\"file:/root/pipeline/datasets/dating/ratings-unpartitioned.orc\")\n//gendersJsonDF.write.format(\"orc\").save(\"file:/root/pipeline/datasets/dating/genders-unpartitioned.orc\")",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1438132084083_-2118710227",
      "id": "20150729-010804_695306034",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 29, 2015 1:08:04 AM",
      "dateStarted": "Dec 30, 2015 3:47:34 PM",
      "dateFinished": "Dec 30, 2015 3:48:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write Out Partitioned And Unpartitioned Versions Of Avro",
      "text": "import com.databricks.spark.avro._\n\n//ratingsJsonDF.write.format(\"com.databricks.spark.avro\").partitionBy(\"rating\").codec(\"snappy\").save(\"file:/root/pipeline/datasets/dating/ratings-partitioned.avro\")\n//gendersJsonDF.write.format(\"com.databricks.spark.avro\").partitionBy(\"gender\").codec(\"snappy\").save(\"file:/root/pipeline/datasets/dating/genders-partitioned.avro\")\n\n//ratingsJsonDF.write.format(\"com.databricks.spark.avro\").codec(\"snappy\").save(\"file:/root/pipeline/datasets/dating/ratings-unpartitioned.avro\")\n//gendersJsonDF.write.format(\"com.databricks.spark.avro\").codec(\"snappy\").save(\"file:/root/pipeline/datasets/dating/genders-unpartitioned.avro\")",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447116812453_2018130857",
      "id": "20151110-005332_1090085355",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import com.databricks.spark.avro._\n"
      },
      "dateCreated": "Nov 10, 2015 12:53:32 AM",
      "dateStarted": "Dec 30, 2015 3:48:06 PM",
      "dateFinished": "Dec 30, 2015 3:48:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (Parquet, Unpartitioned) Joined with Genders (Parquet, Unpartitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030404916_1715643370",
      "id": "20150923-174644_2094604707",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (Parquet, Unpartitioned) Joined with Genders (Parquet, Unpartitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 5:46:44 PM",
      "dateStarted": "Dec 30, 2015 3:44:54 PM",
      "dateFinished": "Dec 30, 2015 3:44:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val ratingsUnpartitionedParquetDF \u003d sqlContext.read.format(\"parquet\").load(\"file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet\")\nval gendersUnpartitionedParquetDF \u003d sqlContext.read.format(\"parquet\").load(\"file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet\")",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030425131_1177010234",
      "id": "20150923-174705_1108122618",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "ratingsUnpartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [fromUserId: bigint, rating: bigint, toUserId: bigint]\ngendersUnpartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [gender: string, id: bigint]\n"
      },
      "dateCreated": "Sep 23, 2015 5:47:05 PM",
      "dateStarted": "Dec 30, 2015 3:48:06 PM",
      "dateFinished": "Dec 30, 2015 3:48:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val middleRatingsUnpartitionedParquetDF \u003d ratingsUnpartitionedParquetDF.select($\"toUserId\", $\"rating\").where($\"rating\" \u003c\u003d 6).where($\"rating\" \u003e\u003d 4)\nmiddleRatingsUnpartitionedParquetDF.explain(true)\nmiddleRatingsUnpartitionedParquetDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030540821_1268396582",
      "id": "20150923-174900_378913387",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "middleRatingsUnpartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: bigint]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter (\u0027rating \u003e\u003d 4)\n Filter (rating#52L \u003c\u003d cast(6 as bigint))\n  Project [toUserId#53L,rating#52L]\n   Relation[fromUserId#51L,rating#52L,toUserId#53L] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: bigint\nFilter (rating#52L \u003e\u003d cast(4 as bigint))\n Filter (rating#52L \u003c\u003d cast(6 as bigint))\n  Project [toUserId#53L,rating#52L]\n   Relation[fromUserId#51L,rating#52L,toUserId#53L] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [toUserId#53L,rating#52L]\n Filter ((rating#52L \u003c\u003d 6) \u0026\u0026 (rating#52L \u003e\u003d 4))\n  Relation[fromUserId#51L,rating#52L,toUserId#53L] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nFilter ((rating#52L \u003c\u003d 6) \u0026\u0026 (rating#52L \u003e\u003d 4))\n Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet][toUserId#53L,rating#52L]\n\nCode Generation: true\nres42: Long \u003d 4688157\n"
      },
      "dateCreated": "Sep 23, 2015 5:49:00 PM",
      "dateStarted": "Dec 30, 2015 3:48:07 PM",
      "dateFinished": "Dec 30, 2015 3:48:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val unknownGendersUnpartitionedParquetDF \u003d gendersUnpartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").filter(\"gender !\u003d \u0027M\u0027\")\nunknownGendersUnpartitionedParquetDF.explain(true)\nunknownGendersUnpartitionedParquetDF.count()",
      "dateUpdated": "Dec 30, 2015 3:52:40 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030662339_204302074",
      "id": "20150923-175102_1558490429",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "unknownGendersUnpartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter NOT (\u0027gender \u003d M)\n Filter NOT (gender#54 \u003d F)\n  Project [id#55L,gender#54]\n   Relation[gender#54,id#55L] ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\nid: bigint, gender: string\nFilter NOT (gender#54 \u003d M)\n Filter NOT (gender#54 \u003d F)\n  Project [id#55L,gender#54]\n   Relation[gender#54,id#55L] ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [id#55L,gender#54]\n Filter (NOT (gender#54 \u003d F) \u0026\u0026 NOT (gender#54 \u003d M))\n  Relation[gender#54,id#55L] ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nFilter (NOT (gender#54 \u003d F) \u0026\u0026 NOT (gender#54 \u003d M))\n Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet][id#55L,gender#54]\n\nCode Generation: true\nres45: Long \u003d 79256\n"
      },
      "dateCreated": "Sep 23, 2015 5:51:02 PM",
      "dateStarted": "Dec 30, 2015 3:48:07 PM",
      "dateFinished": "Dec 30, 2015 3:48:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMiddleRatingsUnpartitionedParquetWithUnknownGendersUnpartitionedParquetDF \u003d \n    middleRatingsUnpartitionedParquetDF.join(unknownGendersUnpartitionedParquetDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\n\njoinMiddleRatingsUnpartitionedParquetWithUnknownGendersUnpartitionedParquetDF.explain(true)",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030729310_-1421723399",
      "id": "20150923-175209_818492142",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "joinMiddleRatingsUnpartitionedParquetWithUnknownGendersUnpartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: bigint, id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#53L \u003d id#55L))\n Filter (rating#52L \u003e\u003d cast(4 as bigint))\n  Filter (rating#52L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#53L,rating#52L]\n    Relation[fromUserId#51L,rating#52L,toUserId#53L] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet]\n Filter NOT (gender#54 \u003d M)\n  Filter NOT (gender#54 \u003d F)\n   Project [id#55L,gender#54]\n    Relation[gender#54,id#55L] ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: bigint, id: bigint, gender: string\nJoin Inner, Some((toUserId#53L \u003d id#55L))\n Filter (rating#52L \u003e\u003d cast(4 as bigint))\n  Filter (rating#52L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#53L,rating#52L]\n    Relation[fromUserId#51L,rating#52L,toUserId#53L] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet]\n Filter NOT (gender#54 \u003d M)\n  Filter NOT (gender#54 \u003d F)\n   Project [id#55L,gender#54]\n    Relation[gender#54,id#55L] ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#53L \u003d id#55L))\n Project [toUserId#53L,rating#52L]\n  Filter ((rating#52L \u003c\u003d 6) \u0026\u0026 (rating#52L \u003e\u003d 4))\n   Relation[fromUserId#51L,rating#52L,toUserId#53L] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet]\n Project [id#55L,gender#54]\n  Filter (NOT (gender#54 \u003d F) \u0026\u0026 NOT (gender#54 \u003d M))\n   Relation[gender#54,id#55L] ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [toUserId#53L], [id#55L], BuildRight\n ConvertToUnsafe\n  Filter ((rating#52L \u003c\u003d 6) \u0026\u0026 (rating#52L \u003e\u003d 4))\n   Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-unpartitioned.parquet][toUserId#53L,rating#52L]\n ConvertToUnsafe\n  Filter (NOT (gender#54 \u003d F) \u0026\u0026 NOT (gender#54 \u003d M))\n   Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-unpartitioned.parquet][id#55L,gender#54]\n\nCode Generation: true\n"
      },
      "dateCreated": "Sep 23, 2015 5:52:09 PM",
      "dateStarted": "Dec 30, 2015 3:48:08 PM",
      "dateFinished": "Dec 30, 2015 3:48:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ###  Ratings (Parquet, Partitioned) Joined with Genders (Parquet, Partitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443030252105_1679851856",
      "id": "20150923-174412_814152127",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (Parquet, Partitioned) Joined with Genders (Parquet, Partitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 5:44:12 PM",
      "dateStarted": "Dec 30, 2015 3:44:55 PM",
      "dateFinished": "Dec 30, 2015 3:44:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val ratingsPartitionedParquetDF \u003d sqlContext.read.format(\"parquet\").load(\"file:/root/pipeline/datasets/dating/ratings-partitioned.parquet\")\nval gendersPartitionedParquetDF \u003d sqlContext.read.format(\"parquet\").load(\"file:/root/pipeline/datasets/dating/genders-partitioned.parquet\")",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1442991228574_478294794",
      "id": "20150923-065348_1151606288",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "ratingsPartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [fromUserId: bigint, toUserId: bigint, rating: int]\ngendersPartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n"
      },
      "dateCreated": "Sep 23, 2015 6:53:48 AM",
      "dateStarted": "Dec 30, 2015 3:48:09 PM",
      "dateFinished": "Dec 30, 2015 3:48:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val middleRatingsPartitionedParquetDF \u003d ratingsPartitionedParquetDF.select($\"toUserId\", $\"rating\").where($\"rating\" \u003c\u003d 6).where($\"rating\" \u003e\u003d 4)\nmiddleRatingsPartitionedParquetDF.explain(true)\nmiddleRatingsPartitionedParquetDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443020571499_-1919587052",
      "id": "20150923-150251_575141285",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "middleRatingsPartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: int]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter (\u0027rating \u003e\u003d 4)\n Filter (rating#71 \u003c\u003d 6)\n  Project [toUserId#70L,rating#71]\n   Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: int\nFilter (rating#71 \u003e\u003d 4)\n Filter (rating#71 \u003c\u003d 6)\n  Project [toUserId#70L,rating#71]\n   Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [toUserId#70L,rating#71]\n Filter ((rating#71 \u003c\u003d 6) \u0026\u0026 (rating#71 \u003e\u003d 4))\n  Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nScan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][toUserId#70L,rating#71]\n\nCode Generation: true\nres52: Long \u003d 4693155\n"
      },
      "dateCreated": "Sep 23, 2015 3:02:51 PM",
      "dateStarted": "Dec 30, 2015 3:48:10 PM",
      "dateFinished": "Dec 30, 2015 3:48:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val unknownGendersPartitionedParquetDF \u003d gendersPartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").filter(\"gender !\u003d \u0027M\u0027\")\nunknownGendersPartitionedParquetDF.explain(true)\nunknownGendersPartitionedParquetDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443024923817_-2031376080",
      "id": "20150923-161523_1156597185",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "unknownGendersPartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter NOT (\u0027gender \u003d M)\n Filter NOT (gender#73 \u003d F)\n  Project [id#72L,gender#73]\n   Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\nid: bigint, gender: string\nFilter NOT (gender#73 \u003d M)\n Filter NOT (gender#73 \u003d F)\n  Project [id#72L,gender#73]\n   Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nFilter (NOT (gender#73 \u003d F) \u0026\u0026 NOT (gender#73 \u003d M))\n Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nScan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\n\nCode Generation: true\nres55: Long \u003d 83164\n"
      },
      "dateCreated": "Sep 23, 2015 4:15:23 PM",
      "dateStarted": "Dec 30, 2015 3:48:10 PM",
      "dateFinished": "Dec 30, 2015 3:48:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMiddleRatingsPartitionedParquetWithUnknownGendersPartitionedParquetDF \u003d middleRatingsPartitionedParquetDF.join(unknownGendersPartitionedParquetDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\njoinMiddleRatingsPartitionedParquetWithUnknownGendersPartitionedParquetDF.explain(true)\njoinMiddleRatingsPartitionedParquetWithUnknownGendersPartitionedParquetDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1442992157897_403262475",
      "id": "20150923-070917_1503747094",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "joinMiddleRatingsPartitionedParquetWithUnknownGendersPartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#70L \u003d id#72L))\n Filter (rating#71 \u003e\u003d 4)\n  Filter (rating#71 \u003c\u003d 6)\n   Project [toUserId#70L,rating#71]\n    Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n Filter NOT (gender#73 \u003d M)\n  Filter NOT (gender#73 \u003d F)\n   Project [id#72L,gender#73]\n    Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: int, id: bigint, gender: string\nJoin Inner, Some((toUserId#70L \u003d id#72L))\n Filter (rating#71 \u003e\u003d 4)\n  Filter (rating#71 \u003c\u003d 6)\n   Project [toUserId#70L,rating#71]\n    Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n Filter NOT (gender#73 \u003d M)\n  Filter NOT (gender#73 \u003d F)\n   Project [id#72L,gender#73]\n    Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#70L \u003d id#72L))\n Project [toUserId#70L,rating#71]\n  Filter ((rating#71 \u003c\u003d 6) \u0026\u0026 (rating#71 \u003e\u003d 4))\n   Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n Filter (NOT (gender#73 \u003d F) \u0026\u0026 NOT (gender#73 \u003d M))\n  Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [toUserId#70L], [id#72L], BuildRight\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][toUserId#70L,rating#71]\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\n\nCode Generation: true\nres58: Long \u003d 1123909\n"
      },
      "dateCreated": "Sep 23, 2015 7:09:17 AM",
      "dateStarted": "Dec 30, 2015 3:48:11 PM",
      "dateFinished": "Dec 30, 2015 3:48:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (Parquet, Partitioned) Joined with Genders (JSON, Unpartitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443025971612_-1297080603",
      "id": "20150923-163251_207960793",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (Parquet, Partitioned) Joined with Genders (JSON, Unpartitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 4:32:51 PM",
      "dateStarted": "Dec 30, 2015 3:44:55 PM",
      "dateFinished": "Dec 30, 2015 3:44:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMiddleRatingsPartitionedParquetWithUnknownGendersUnpartitionedJsonDF \u003d middleRatingsPartitionedParquetDF.join(unknownGendersUnpartitionedJsonDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\n\njoinMiddleRatingsPartitionedParquetWithUnknownGendersUnpartitionedJsonDF.explain(true)\n\njoinMiddleRatingsPartitionedParquetWithUnknownGendersUnpartitionedJsonDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443031250537_-1256730707",
      "id": "20150923-180050_249380911",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "joinMiddleRatingsPartitionedParquetWithUnknownGendersUnpartitionedJsonDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#70L \u003d id#32L))\n Filter (rating#71 \u003e\u003d 4)\n  Filter (rating#71 \u003c\u003d 6)\n   Project [toUserId#70L,rating#71]\n    Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n Filter NOT (gender#31 \u003d M)\n  Filter NOT (gender#31 \u003d F)\n   Project [id#32L,gender#31]\n    Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: int, id: bigint, gender: string\nJoin Inner, Some((toUserId#70L \u003d id#32L))\n Filter (rating#71 \u003e\u003d 4)\n  Filter (rating#71 \u003c\u003d 6)\n   Project [toUserId#70L,rating#71]\n    Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n Filter NOT (gender#31 \u003d M)\n  Filter NOT (gender#31 \u003d F)\n   Project [id#32L,gender#31]\n    Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#70L \u003d id#32L))\n Project [toUserId#70L,rating#71]\n  Filter ((rating#71 \u003c\u003d 6) \u0026\u0026 (rating#71 \u003e\u003d 4))\n   Relation[fromUserId#69L,toUserId#70L,rating#71] ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet]\n Project [id#32L,gender#31]\n  Filter (NOT (gender#31 \u003d F) \u0026\u0026 NOT (gender#31 \u003d M))\n   Relation[gender#31,id#32L] JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2]\n\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [toUserId#70L], [id#32L], BuildRight\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][toUserId#70L,rating#71]\n ConvertToUnsafe\n  Filter (NOT (gender#31 \u003d F) \u0026\u0026 NOT (gender#31 \u003d M))\n   Scan JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2][id#32L,gender#31]\n\nCode Generation: true\nres63: Long \u003d 1123909\n"
      },
      "dateCreated": "Sep 23, 2015 6:00:50 PM",
      "dateStarted": "Dec 30, 2015 3:48:11 PM",
      "dateFinished": "Dec 30, 2015 3:48:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (JSON, Unpartitioned) Joined with Genders (Parquet, Partitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443031351384_1827986760",
      "id": "20150923-180231_1017767255",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (JSON, Unpartitioned) Joined with Genders (Parquet, Partitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 6:02:31 PM",
      "dateStarted": "Dec 30, 2015 3:44:55 PM",
      "dateFinished": "Dec 30, 2015 3:44:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\nval joinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF \u003d middleRatingsUnpartitionedJsonDF.join(unknownGendersPartitionedParquetDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\njoinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF.explain(true)\njoinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF.count()\n",
      "dateUpdated": "Dec 30, 2015 3:44:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443031375524_1649093846",
      "id": "20150923-180255_1502407331",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "joinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: bigint, id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#30L \u003d id#72L))\n Filter (rating#29L \u003e\u003d cast(4 as bigint))\n  Filter (rating#29L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#30L,rating#29L]\n    Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter NOT (gender#73 \u003d M)\n  Filter NOT (gender#73 \u003d F)\n   Project [id#72L,gender#73]\n    Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: bigint, id: bigint, gender: string\nJoin Inner, Some((toUserId#30L \u003d id#72L))\n Filter (rating#29L \u003e\u003d cast(4 as bigint))\n  Filter (rating#29L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#30L,rating#29L]\n    Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter NOT (gender#73 \u003d M)\n  Filter NOT (gender#73 \u003d F)\n   Project [id#72L,gender#73]\n    Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#30L \u003d id#72L))\n Project [toUserId#30L,rating#29L]\n  Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n   Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter (NOT (gender#73 \u003d F) \u0026\u0026 NOT (gender#73 \u003d M))\n  Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [toUserId#30L], [id#72L], BuildRight\n ConvertToUnsafe\n  Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n   Scan JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2][toUserId#30L,rating#29L]\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\n\nCode Generation: true\nres67: Long \u003d 1123909\n"
      },
      "dateCreated": "Sep 23, 2015 6:02:55 PM",
      "dateStarted": "Dec 30, 2015 3:48:12 PM",
      "dateFinished": "Dec 30, 2015 3:48:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (JSON, Unpartitioned) Joined with Genders (Parquet, Partitioned)",
      "dateUpdated": "Dec 30, 2015 3:53:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451490656179_450215952",
      "id": "20151230-155056_106717474",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (JSON, Unpartitioned) Joined with Genders (Parquet, Partitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Dec 30, 2015 3:50:56 PM",
      "dateStarted": "Dec 30, 2015 3:53:11 PM",
      "dateFinished": "Dec 30, 2015 3:53:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF \u003d middleRatingsUnpartitionedJsonDF.join(unknownGendersPartitionedParquetDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\n\njoinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF.explain(true)\n\njoinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF.count()",
      "dateUpdated": "Dec 30, 2015 3:53:21 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124067224_540937796",
      "id": "20151110-025427_836627264",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "joinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: bigint, id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#30L \u003d id#72L))\n Filter (rating#29L \u003e\u003d cast(4 as bigint))\n  Filter (rating#29L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#30L,rating#29L]\n    Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter NOT (gender#73 \u003d M)\n  Filter NOT (gender#73 \u003d F)\n   Project [id#72L,gender#73]\n    Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: bigint, id: bigint, gender: string\nJoin Inner, Some((toUserId#30L \u003d id#72L))\n Filter (rating#29L \u003e\u003d cast(4 as bigint))\n  Filter (rating#29L \u003c\u003d cast(6 as bigint))\n   Project [toUserId#30L,rating#29L]\n    Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter NOT (gender#73 \u003d M)\n  Filter NOT (gender#73 \u003d F)\n   Project [id#72L,gender#73]\n    Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#30L \u003d id#72L))\n Project [toUserId#30L,rating#29L]\n  Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n   Relation[fromUserId#28L,rating#29L,toUserId#30L] JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2]\n Filter (NOT (gender#73 \u003d F) \u0026\u0026 NOT (gender#73 \u003d M))\n  Relation[id#72L,gender#73] ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet]\n\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [toUserId#30L], [id#72L], BuildRight\n ConvertToUnsafe\n  Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n   Scan JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2][toUserId#30L,rating#29L]\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\n\nCode Generation: true\n\u003cconsole\u003e:2: error: \u0027;\u0027 expected but \u0027with\u0027 found.\n       joinMiddleRatingsUnpartitionedJsonWithUnknownGendersPartitionedParquetDF.count()%md ### Ratings (ORC, Partitioned) Joined with Genders (ORC, Partitioned)\n                                                                                                                                 ^\n"
      },
      "dateCreated": "Nov 10, 2015 2:54:27 AM",
      "dateStarted": "Dec 30, 2015 3:48:14 PM",
      "dateFinished": "Dec 30, 2015 3:48:45 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (ORC, Partitioned) Joined with Genders (ORC, Partitioned)",
      "dateUpdated": "Dec 30, 2015 3:53:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451490825519_-855592811",
      "id": "20151230-155345_579477615",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (ORC, Partitioned) Joined with Genders (ORC, Partitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Dec 30, 2015 3:53:45 PM",
      "dateStarted": "Dec 30, 2015 3:53:57 PM",
      "dateFinished": "Dec 30, 2015 3:53:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "val ratingsPartitionedOrcDF \u003d sqlContext.read.format(\"orc\")\n  .load(\"file:/root/pipeline/datasets/dating/ratings-partitioned.orc\")\n\nval gendersPartitionedOrcDF \u003d sqlContext.read.format(\"orc\")\n  .load(\"file:/root/pipeline/datasets/dating/genders-partitioned.orc\")",
      "dateUpdated": "Dec 30, 2015 3:54:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447123866716_-1594705946",
      "id": "20151110-025106_224582510",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "ratingsPartitionedOrcDF: org.apache.spark.sql.DataFrame \u003d [fromuserid: bigint, touserid: bigint, rating: int]\ngendersPartitionedOrcDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n"
      },
      "dateCreated": "Nov 10, 2015 2:51:06 AM",
      "dateStarted": "Dec 30, 2015 3:48:45 PM",
      "dateFinished": "Dec 30, 2015 3:48:45 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val middleRatingsPartitionedOrcDF \u003d ratingsPartitionedOrcDF.select($\"toUserId\", $\"rating\").where($\"rating\" \u003c\u003d 6).where($\"rating\" \u003e\u003d 4)\nmiddleRatingsPartitionedOrcDF.explain(true)\nmiddleRatingsPartitionedOrcDF.count()",
      "dateUpdated": "Dec 30, 2015 3:54:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124018703_1723209869",
      "id": "20151110-025338_272078696",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "middleRatingsPartitionedOrcDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: int]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter (\u0027rating \u003e\u003d 4)\n Filter (rating#107 \u003c\u003d 6)\n  Project [toUserId#106L,rating#107]\n   Relation[fromuserid#105L,touserid#106L,rating#107] OrcRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.orc]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: int\nFilter (rating#107 \u003e\u003d 4)\n Filter (rating#107 \u003c\u003d 6)\n  Project [toUserId#106L,rating#107]\n   Relation[fromuserid#105L,touserid#106L,rating#107] OrcRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.orc]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [toUserId#106L,rating#107]\n Filter ((rating#107 \u003c\u003d 6) \u0026\u0026 (rating#107 \u003e\u003d 4))\n  Relation[fromuserid#105L,touserid#106L,rating#107] OrcRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.orc]\n\n\u003d\u003d Physical Plan \u003d\u003d\nScan OrcRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.orc][toUserId#106L,rating#107]\n\nCode Generation: true\nres74: Long \u003d 4693155\n"
      },
      "dateCreated": "Nov 10, 2015 2:53:38 AM",
      "dateStarted": "Dec 30, 2015 3:48:45 PM",
      "dateFinished": "Dec 30, 2015 3:48:47 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val unknownGendersPartitionedOrcDF \u003d gendersPartitionedOrcDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").filter(\"gender !\u003d \u0027M\u0027\")\nunknownGendersPartitionedOrcDF.explain(true)\nunknownGendersPartitionedOrcDF.count()\n",
      "dateUpdated": "Dec 30, 2015 3:54:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124185782_-1387638230",
      "id": "20151110-025625_992313392",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "unknownGendersPartitionedOrcDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter NOT (\u0027gender \u003d M)\n Filter NOT (gender#109 \u003d F)\n  Project [id#108L,gender#109]\n   Relation[id#108L,gender#109] OrcRelation[file:/root/pipeline/datasets/dating/genders-partitioned.orc]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\nid: bigint, gender: string\nFilter NOT (gender#109 \u003d M)\n Filter NOT (gender#109 \u003d F)\n  Project [id#108L,gender#109]\n   Relation[id#108L,gender#109] OrcRelation[file:/root/pipeline/datasets/dating/genders-partitioned.orc]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nFilter (NOT (gender#109 \u003d F) \u0026\u0026 NOT (gender#109 \u003d M))\n Relation[id#108L,gender#109] OrcRelation[file:/root/pipeline/datasets/dating/genders-partitioned.orc]\n\n\u003d\u003d Physical Plan \u003d\u003d\nScan OrcRelation[file:/root/pipeline/datasets/dating/genders-partitioned.orc][id#108L,gender#109]\n\nCode Generation: true\nres77: Long \u003d 83164\n"
      },
      "dateCreated": "Nov 10, 2015 2:56:25 AM",
      "dateStarted": "Dec 30, 2015 3:48:46 PM",
      "dateFinished": "Dec 30, 2015 3:48:48 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMediumHottieRatingsPartitionedOrcWithUnknownGendersPartitionedOrcDF \u003d mediumHottieRatingsPartitionedOrcDF.join(unknownGendersPartitionedOrcDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\njoinMiddleRatingsPartitionedOrcWithUnknownGendersPartitionedOrcDF.explain(true)\njoinMiddleRatingsPartitionedOrcWithUnknownGendersPartitionedOrcDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124197656_-421118344",
      "id": "20151110-025637_2116491568",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:31: error: not found: value mediumHottieRatingsPartitionedOrcDF\n       val joinMediumHottieRatingsPartitionedOrcWithUnknownGendersPartitionedOrcDF \u003d mediumHottieRatingsPartitionedOrcDF.join(unknownGendersPartitionedOrcDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\n                                                                                     ^\n"
      },
      "dateCreated": "Nov 10, 2015 2:56:37 AM",
      "dateStarted": "Dec 30, 2015 3:48:48 PM",
      "dateFinished": "Dec 30, 2015 3:48:48 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (Avro, Partitioned) Joined with Genders (Avro, Partitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124379148_-629992669",
      "id": "20151110-025939_1317589464",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (Avro, Partitioned) Joined with Genders (Avro, Partitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Nov 10, 2015 2:59:39 AM",
      "dateStarted": "Dec 30, 2015 3:44:56 PM",
      "dateFinished": "Dec 30, 2015 3:44:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val ratingsPartitionedAvroDF \u003d sqlContext.read.format(\"com.databricks.spark.avro\").load(\"file:/root/pipeline/datasets/dating/ratings-partitioned.avro\")\nval gendersPartitionedAvroDF \u003d sqlContext.read.format(\"com.databricks.spark.avro\").load(\"file:/root/pipeline/datasets/dating/genders-partitioned.avro\")",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124460587_1287919301",
      "id": "20151110-030100_724955488",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "ratingsPartitionedAvroDF: org.apache.spark.sql.DataFrame \u003d [fromUserId: bigint, toUserId: bigint, rating: int]\ngendersPartitionedAvroDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n"
      },
      "dateCreated": "Nov 10, 2015 3:01:00 AM",
      "dateStarted": "Dec 30, 2015 3:48:48 PM",
      "dateFinished": "Dec 30, 2015 3:48:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val middleRatingsPartitionedAvroDF \u003d ratingsPartitionedAvroDF.select($\"toUserId\", $\"rating\").where($\"rating\" \u003c\u003d 6).where($\"rating\" \u003e\u003d 4)\nmiddleRatingsPartitionedAvroDF.explain(true)\nmiddleRatingsPartitionedAvroDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124441144_622257732",
      "id": "20151110-030041_567122448",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "middleRatingsPartitionedAvroDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: int]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter (\u0027rating \u003e\u003d 4)\n Filter (rating#128 \u003c\u003d 6)\n  Project [toUserId#127L,rating#128]\n   Relation[fromUserId#126L,toUserId#127L,rating#128] AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: int\nFilter (rating#128 \u003e\u003d 4)\n Filter (rating#128 \u003c\u003d 6)\n  Project [toUserId#127L,rating#128]\n   Relation[fromUserId#126L,toUserId#127L,rating#128] AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nProject [toUserId#127L,rating#128]\n Filter ((rating#128 \u003c\u003d 6) \u0026\u0026 (rating#128 \u003e\u003d 4))\n  Relation[fromUserId#126L,toUserId#127L,rating#128] AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro]\n\n\u003d\u003d Physical Plan \u003d\u003d\nScan AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro][toUserId#127L,rating#128]\n\nCode Generation: true\nres81: Long \u003d 4693155\n"
      },
      "dateCreated": "Nov 10, 2015 3:00:41 AM",
      "dateStarted": "Dec 30, 2015 3:48:48 PM",
      "dateFinished": "Dec 30, 2015 3:48:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val unknownGendersPartitionedAvroDF \u003d gendersPartitionedAvroDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").filter(\"gender !\u003d \u0027M\u0027\")\nunknownGendersPartitionedAvroDF.explain(true)\nunknownGendersPartitionedAvroDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124609248_-1586941449",
      "id": "20151110-030329_1040791001",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "unknownGendersPartitionedAvroDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\n\u0027Filter NOT (\u0027gender \u003d M)\n Filter NOT (gender#130 \u003d F)\n  Project [id#129L,gender#130]\n   Relation[id#129L,gender#130] AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\nid: bigint, gender: string\nFilter NOT (gender#130 \u003d M)\n Filter NOT (gender#130 \u003d F)\n  Project [id#129L,gender#130]\n   Relation[id#129L,gender#130] AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nFilter (NOT (gender#130 \u003d F) \u0026\u0026 NOT (gender#130 \u003d M))\n Relation[id#129L,gender#130] AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro]\n\n\u003d\u003d Physical Plan \u003d\u003d\nScan AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro][id#129L,gender#130]\n\nCode Generation: true\nres84: Long \u003d 83164\n"
      },
      "dateCreated": "Nov 10, 2015 3:03:29 AM",
      "dateStarted": "Dec 30, 2015 3:48:49 PM",
      "dateFinished": "Dec 30, 2015 3:48:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinMiddleRatingsPartitionedAvroWithUnknownGendersPartitionedAvroDF \u003d middleRatingsPartitionedAvroDF.join(unknownGendersPartitionedAvroDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\njoinMiddleRatingsPartitionedAvroWithUnknownGendersPartitionedAvroDF.explain(true)\njoinMiddleRatingsPartitionedAvroWithUnknownGendersPartitionedAvroDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1447124676187_1528678523",
      "id": "20151110-030436_1794076859",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "joinMiddleRatingsPartitionedAvroWithUnknownGendersPartitionedAvroDF: org.apache.spark.sql.DataFrame \u003d [toUserId: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Parsed Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#127L \u003d id#129L))\n Filter (rating#128 \u003e\u003d 4)\n  Filter (rating#128 \u003c\u003d 6)\n   Project [toUserId#127L,rating#128]\n    Relation[fromUserId#126L,toUserId#127L,rating#128] AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro]\n Filter NOT (gender#130 \u003d M)\n  Filter NOT (gender#130 \u003d F)\n   Project [id#129L,gender#130]\n    Relation[id#129L,gender#130] AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro]\n\n\u003d\u003d Analyzed Logical Plan \u003d\u003d\ntoUserId: bigint, rating: int, id: bigint, gender: string\nJoin Inner, Some((toUserId#127L \u003d id#129L))\n Filter (rating#128 \u003e\u003d 4)\n  Filter (rating#128 \u003c\u003d 6)\n   Project [toUserId#127L,rating#128]\n    Relation[fromUserId#126L,toUserId#127L,rating#128] AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro]\n Filter NOT (gender#130 \u003d M)\n  Filter NOT (gender#130 \u003d F)\n   Project [id#129L,gender#130]\n    Relation[id#129L,gender#130] AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro]\n\n\u003d\u003d Optimized Logical Plan \u003d\u003d\nJoin Inner, Some((toUserId#127L \u003d id#129L))\n Project [toUserId#127L,rating#128]\n  Filter ((rating#128 \u003c\u003d 6) \u0026\u0026 (rating#128 \u003e\u003d 4))\n   Relation[fromUserId#126L,toUserId#127L,rating#128] AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro]\n Filter (NOT (gender#130 \u003d F) \u0026\u0026 NOT (gender#130 \u003d M))\n  Relation[id#129L,gender#130] AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro]\n\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [toUserId#127L], [id#129L], BuildRight\n ConvertToUnsafe\n  Scan AvroRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.avro][toUserId#127L,rating#128]\n ConvertToUnsafe\n  Scan AvroRelation[file:/root/pipeline/datasets/dating/genders-partitioned.avro][id#129L,gender#130]\n\nCode Generation: true\nres87: Long \u003d 1123909\n"
      },
      "dateCreated": "Nov 10, 2015 3:04:36 AM",
      "dateStarted": "Dec 30, 2015 3:48:50 PM",
      "dateFinished": "Dec 30, 2015 3:48:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (Cassandra, Partitioned) and Genders (Parquet, Unpartitioned)",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443031159774_723633179",
      "id": "20150923-175919_702207845",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (Cassandra, Partitioned) and Genders (Parquet, Unpartitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Sep 23, 2015 5:59:19 PM",
      "dateStarted": "Dec 30, 2015 3:44:56 PM",
      "dateFinished": "Dec 30, 2015 3:44:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val middleRatingsPartitionedCassandraDF \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(Map(\"pushdown\" -\u003e \"true\", \"keyspace\" -\u003e \"fluxcapacitor\", \"table\" -\u003e \"ratings\")).load()\nmiddleRatingsPartitionedCassandraDF.explain(true)\nmiddleRatingsPartitionedCassandraDF.count()\n\nval joinMiddleRatingsPartitionedCassandraWithUnknownGendersUnpartitionedJsonDF \u003d middleRatingsPartitionedCassandraDF.join(unknownGendersUnpartitionedJsonDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\njoinMiddleRatingsPartitionedCassandraWithUnknownGendersUnpartitionedJsonDF.explain(true)\njoinMiddleRatingsPartitionedCassandraWithUnknownGendersUnpartitionedJsonDF.count()",
      "dateUpdated": "Dec 30, 2015 3:55:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448342793421_288487303",
      "id": "20151124-052633_1200375561",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.io.IOException: Couldn\u0027t find fluxcapacitor.ratings or any similarly named keyspace and table pairs\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.\u003cinit\u003e(CassandraSourceRelation.scala:53)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:184)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:57)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:125)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:27)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:32)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:34)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:36)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:38)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:40)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:42)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:44)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:46)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:48)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:50)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:52)\n\tat \u003cinit\u003e(\u003cconsole\u003e:54)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Nov 24, 2015 5:26:33 AM",
      "dateStarted": "Dec 30, 2015 3:48:51 PM",
      "dateFinished": "Dec 30, 2015 3:48:54 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Ratings (Cassandra, Partitioned) and Genders (Parquet, Partitioned)\n",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1446486069397_692796763",
      "id": "20151102-174109_1542846656",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRatings (Cassandra, Partitioned) and Genders (Parquet, Partitioned)\u003c/h3\u003e\n"
      },
      "dateCreated": "Nov 2, 2015 5:41:09 PM",
      "dateStarted": "Dec 30, 2015 3:44:56 PM",
      "dateFinished": "Dec 30, 2015 3:44:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val middleRatingsPartitionedCassandraDF \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(Map(\"pushdown\" -\u003e \"true\", \"keyspace\" -\u003e \"fluxcapacitor\", \"table\" -\u003e \"ratings\")).load()\nmiddleRatingsPartitionedCassandraDF.explain(true)\nmiddleRatingsPartitionedCassandraDF.count()\n\nval joinMiddleRatingsPartitionedCassandraWithUnknownGendersPartitionedParquetDF \u003d middleRatingsPartitionedCassandraDF.join(unknownGendersPartitionedParquetDF, $\"toUserId\" \u003d\u003d\u003d $\"id\")\njoinMiddleRatingsPartitionedCassandraWithUnknownGendersPartitionedParquetDF.explain(true)\njoinMiddleRatingsPartitionedCassandraWithUnknownGendersPartitionedParquetDF.count()",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1443031227537_689298582",
      "id": "20150923-180027_260903546",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.io.IOException: Couldn\u0027t find fluxcapacitor.ratings or any similarly named keyspace and table pairs\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.\u003cinit\u003e(CassandraSourceRelation.scala:53)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:184)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:57)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:125)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:27)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:32)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:34)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:36)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:38)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:40)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:42)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:44)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:46)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:48)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:50)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:52)\n\tat \u003cinit\u003e(\u003cconsole\u003e:54)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Sep 23, 2015 6:00:27 PM",
      "dateStarted": "Dec 30, 2015 3:48:53 PM",
      "dateFinished": "Dec 30, 2015 3:48:54 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Partitioned vs. Unpartitioned Join",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448357811615_103737870",
      "id": "20151124-093651_1532456479",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003ePartitioned vs. Unpartitioned Join\u003c/h3\u003e\n"
      },
      "dateCreated": "Nov 24, 2015 9:36:51 AM",
      "dateStarted": "Dec 30, 2015 3:44:57 PM",
      "dateFinished": "Dec 30, 2015 3:44:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Ratings Partitioned Parquet, Genders Unpartitioned JSON Join",
      "text": "val df \u003d ratingsPartitionedParquetDF.select($\"touserid\", $\"rating\").where($\"rating\" \u003c\u003d 6).filter($\"rating\" \u003e\u003d 4)\n  .join(gendersUnpartitionedJsonDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").where(\"gender !\u003d \u0027M\u0027\"),\n    $\"touserid\" \u003d\u003d\u003d $\"id\")\ndf.explain()\ndf.count()",
      "dateUpdated": "Dec 30, 2015 3:44:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448343958042_477736897",
      "id": "20151124-054558_66892236",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [touserid: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [touserid#70L], [id#32L], BuildRight\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][touserid#70L,rating#71]\n ConvertToUnsafe\n  Filter (NOT (gender#31 \u003d F) \u0026\u0026 NOT (gender#31 \u003d M))\n   Scan JSONRelation[file:/root/pipeline/datasets/dating/genders.json.bz2][id#32L,gender#31]\nres90: Long \u003d 1123909\n"
      },
      "dateCreated": "Nov 24, 2015 5:45:58 AM",
      "dateStarted": "Dec 30, 2015 3:48:54 PM",
      "dateFinished": "Dec 30, 2015 3:48:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Ratings Unpartitioned JSON, Genders Partitioned Parquet Join",
      "text": "val df \u003d ratingsUnpartitionedJsonDF.select($\"touserid\", $\"rating\").where($\"rating\" \u003c\u003d 6).filter($\"rating\" \u003e\u003d 4)\n  .join(gendersPartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").where(\"gender !\u003d \u0027M\u0027\"), \n    $\"touserid\" \u003d\u003d\u003d $\"id\")\ndf.explain()\ndf.count()",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448344097984_-630741653",
      "id": "20151124-054817_1197086108",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [touserid: bigint, rating: bigint, id: bigint, gender: string]\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [touserid#30L], [id#72L], BuildRight\n ConvertToUnsafe\n  Filter ((rating#29L \u003c\u003d 6) \u0026\u0026 (rating#29L \u003e\u003d 4))\n   Scan JSONRelation[file:/root/pipeline/datasets/dating/ratings.json.bz2][touserid#30L,rating#29L]\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\nres93: Long \u003d 1123909\n"
      },
      "dateCreated": "Nov 24, 2015 5:48:17 AM",
      "dateStarted": "Dec 30, 2015 3:48:54 PM",
      "dateFinished": "Dec 30, 2015 3:49:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Ratings Partitioned Parquet, Genders Partitioned Parquet Join",
      "text": "val df \u003d ratingsPartitionedParquetDF.select($\"touserid\", $\"rating\").where($\"rating\" \u003c\u003d 6).where($\"rating\" \u003e\u003d 4)\n  .join(gendersPartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").filter(\"gender !\u003d \u0027M\u0027\"), \n    $\"touserid\" \u003d\u003d\u003d $\"id\")\ndf.explain()\ndf.count()",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448357692009_1930740186",
      "id": "20151124-093452_1230251969",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [touserid: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [touserid#70L], [id#72L], BuildRight\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][touserid#70L,rating#71]\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\nres96: Long \u003d 1123909\n"
      },
      "dateCreated": "Nov 24, 2015 9:34:52 AM",
      "dateStarted": "Dec 30, 2015 3:48:55 PM",
      "dateFinished": "Dec 30, 2015 3:49:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Cartesian vs. Inner Join",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448343695877_-1747633772",
      "id": "20151124-054135_192544129",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eCartesian vs. Inner Join\u003c/h3\u003e\n"
      },
      "dateCreated": "Nov 24, 2015 5:41:35 AM",
      "dateStarted": "Dec 30, 2015 3:44:57 PM",
      "dateFinished": "Dec 30, 2015 3:44:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "cartesian Join",
      "text": "val df \u003d ratingsPartitionedParquetDF.select($\"touserid\", $\"rating\").where($\"rating\" \u003c\u003d 6).filter($\"rating\" \u003e\u003d 4)\n  .join(gendersPartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").where(\"gender !\u003d \u0027M\u0027\"))\ndf.explain()\ndf.count()",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448342768118_-1540471051",
      "id": "20151124-052608_1973013568",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [touserid: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Physical Plan \u003d\u003d\nCartesianProduct\n Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][touserid#70L,rating#71]\n Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 60.0 failed 4 times, most recent failure: Lost task 1.3 in stage 60.0 (TID 352, 127.0.0.1): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1$$anonfun$apply$2.apply(CartesianRDD.scala:76)\n\tat org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1$$anonfun$apply$2.apply(CartesianRDD.scala:76)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.next(Iterator.scala:372)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:366)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.start(TungstenAggregationIterator.scala:622)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1.org$apache$spark$sql$execution$aggregate$TungstenAggregate$$anonfun$$executePartition$1(TungstenAggregate.scala:110)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)\n\tat org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:64)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:177)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collect$1.apply(DataFrame.scala:1385)\n\tat org.apache.spark.sql.DataFrame$$anonfun$collect$1.apply(DataFrame.scala:1385)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:1903)\n\tat org.apache.spark.sql.DataFrame.collect(DataFrame.scala:1384)\n\tat org.apache.spark.sql.DataFrame.count(DataFrame.scala:1402)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:34)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:39)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:41)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:43)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:45)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:47)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:49)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:51)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:53)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:55)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:57)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:59)\n\tat \u003cinit\u003e(\u003cconsole\u003e:61)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:65)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1$$anonfun$apply$2.apply(CartesianRDD.scala:76)\n\tat org.apache.spark.rdd.CartesianRDD$$anonfun$compute$1$$anonfun$apply$2.apply(CartesianRDD.scala:76)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.next(Iterator.scala:372)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:366)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.start(TungstenAggregationIterator.scala:622)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1.org$apache$spark$sql$execution$aggregate$TungstenAggregate$$anonfun$$executePartition$1(TungstenAggregate.scala:110)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)\n\tat org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:64)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\t... 3 more\n\n"
      },
      "dateCreated": "Nov 24, 2015 5:26:08 AM",
      "dateStarted": "Dec 30, 2015 3:49:28 PM",
      "dateFinished": "Dec 30, 2015 3:33:07 PM",
      "status": "RUNNING",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Inner Join",
      "text": "val df \u003d ratingsPartitionedParquetDF.select($\"touserid\", $\"rating\").where($\"rating\" \u003c\u003d 6).filter($\"rating\" \u003e\u003d 4)\n  .join(gendersPartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").where(\"gender !\u003d \u0027M\u0027\"),\n    $\"touserid\" \u003d\u003d\u003d $\"id\")\ndf.explain()\ndf.count()",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448343825895_708385378",
      "id": "20151124-054345_2012690459",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "df: org.apache.spark.sql.DataFrame \u003d [touserid: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [touserid#70L], [id#72L], BuildRight\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][touserid#70L,rating#71]\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\nres101: Long \u003d 1123909\n"
      },
      "dateCreated": "Nov 24, 2015 5:43:45 AM",
      "dateStarted": "Dec 30, 2015 3:49:29 PM",
      "dateFinished": "Dec 30, 2015 3:33:21 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md ### Broadcast vs. Normal Shuffle Join",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448357873202_1915828568",
      "id": "20151124-093753_1370424687",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eBroadcast vs. Normal Shuffle Join\u003c/h3\u003e\n"
      },
      "dateCreated": "Nov 24, 2015 9:37:53 AM",
      "dateStarted": "Dec 30, 2015 3:44:57 PM",
      "dateFinished": "Dec 30, 2015 3:44:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Broadcast Join",
      "text": "sqlContext.sql(\"set spark.sql.autoBroadcastJoinThreshold\u003d10485760\") // default \u003d 10 MB\n\nval df \u003d ratingsPartitionedParquetDF.select($\"touserid\", $\"rating\").where($\"rating\" \u003c\u003d 6).filter($\"rating\" \u003e\u003d 4)\n  .join(gendersPartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").where(\"gender !\u003d \u0027M\u0027\"), \n    $\"touserid\" \u003d\u003d\u003d $\"id\")\n\ndf.explain()\ndf.count()",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448358155289_-387295317",
      "id": "20151124-094235_1404171614",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res103: org.apache.spark.sql.DataFrame \u003d [key: string, value: string]\ndf: org.apache.spark.sql.DataFrame \u003d [touserid: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [touserid#70L], [id#72L], BuildRight\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][touserid#70L,rating#71]\n ConvertToUnsafe\n  Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\nres107: Long \u003d 1123909\n"
      },
      "dateCreated": "Nov 24, 2015 9:42:35 AM",
      "dateStarted": "Dec 30, 2015 3:33:08 PM",
      "dateFinished": "Dec 30, 2015 3:33:22 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Normal Shuffle Join",
      "text": "sqlContext.sql(\"set spark.sql.autoBroadcastJoinThreshold\u003d-1\")\n\nval df \u003d ratingsPartitionedParquetDF.select($\"touserid\", $\"rating\").where($\"rating\" \u003c\u003d 6).filter($\"rating\" \u003e\u003d 4)\n  .join(gendersPartitionedParquetDF.select($\"id\", $\"gender\").filter(\"gender !\u003d \u0027F\u0027\").where(\"gender !\u003d \u0027M\u0027\"), \n    $\"touserid\" \u003d\u003d\u003d $\"id\")\n\ndf.explain()\ndf.count()\n  \nsqlContext.sql(\"set spark.sql.autoBroadcastJoinThreshold\u003d10485760\") // default \u003d 10 MB\n",
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448357979252_-1199193761",
      "id": "20151124-093939_690159599",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res109: org.apache.spark.sql.DataFrame \u003d [key: string, value: string]\ndf: org.apache.spark.sql.DataFrame \u003d [touserid: bigint, rating: int, id: bigint, gender: string]\n\u003d\u003d Physical Plan \u003d\u003d\nSortMergeJoin [touserid#70L], [id#72L]\n TungstenSort [touserid#70L ASC], false, 0\n  TungstenExchange hashpartitioning(touserid#70L)\n   ConvertToUnsafe\n    Scan ParquetRelation[file:/root/pipeline/datasets/dating/ratings-partitioned.parquet][touserid#70L,rating#71]\n TungstenSort [id#72L ASC], false, 0\n  TungstenExchange hashpartitioning(id#72L)\n   ConvertToUnsafe\n    Scan ParquetRelation[file:/root/pipeline/datasets/dating/genders-partitioned.parquet][id#72L,gender#73]\nres113: Long \u003d 1123909\nres115: org.apache.spark.sql.DataFrame \u003d [key: string, value: string]\n"
      },
      "dateCreated": "Nov 24, 2015 9:39:39 AM",
      "dateStarted": "Dec 30, 2015 3:33:22 PM",
      "dateFinished": "Dec 30, 2015 3:33:26 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448437351207_1586195198",
      "id": "20151125-074231_389285762",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Nov 25, 2015 7:42:31 AM",
      "dateStarted": "Dec 30, 2015 3:33:23 PM",
      "dateFinished": "Dec 30, 2015 3:33:26 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Dec 30, 2015 3:44:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1448437344326_-2012158762",
      "id": "20151125-074224_319525179",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Nov 25, 2015 7:42:24 AM",
      "dateStarted": "Dec 30, 2015 3:33:26 PM",
      "dateFinished": "Dec 30, 2015 3:33:26 PM",
      "status": "PENDING",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "SQL/01: Compare Query Plans (Formats, Partitions, Joins)",
  "id": "2B2M4DMJK",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "simple"
  },
  "info": {}
}