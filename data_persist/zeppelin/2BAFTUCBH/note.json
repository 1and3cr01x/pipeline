{
  "paragraphs": [
    {
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")\n  .select($\"id\", $\"title\", $\"category\", $\"description\")\n  .limit(1)",
      "dateUpdated": "Feb 4, 2016 1:16:10 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452407914910_1583997153",
      "id": "20160110-063834_50383130",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, title: string, category: string, description: string]\n"
      },
      "dateCreated": "Jan 10, 2016 6:38:34 AM",
      "dateStarted": "Feb 4, 2016 1:16:10 AM",
      "dateFinished": "Feb 4, 2016 1:16:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import com.databricks.spark.corenlp.CoreNLP\n\n// https://github.com/databricks/spark-corenlp\n\nval coreNLP \u003d new CoreNLP()\n  .setInputCol(\"description\")\n  .setOutputCol(\"corenlp\")\n  .setAnnotators(Array(\"tokenize\", \"ssplit\", \"pos\", \"lemma\"))\n  .setFlattenNestedFields(Array(\"sentence_token_lemma\"))\n  \nz.show(coreNLP.transform(itemsDF))",
      "dateUpdated": "Feb 4, 2016 1:16:49 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452407914910_1583997153",
      "id": "20160110-063834_1389822814",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:40: error: object corenlp is not a member of package com.databricks.spark\n         import com.databricks.spark.corenlp.CoreNLP\n                                     ^\n"
      },
      "dateCreated": "Jan 10, 2016 6:38:34 AM",
      "dateStarted": "Feb 4, 2016 1:16:49 AM",
      "dateFinished": "Feb 4, 2016 1:16:49 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.feature.StopWordsRemover\n\n// The following list will be used by default if we don\u0027t specify a list:  \n//   http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\nval stopWordsFilter \u003d new StopWordsRemover()\n  .setInputCol(coreNLP.getOutputCol)\n  .setOutputCol(\"filteredWords\")\n  .setCaseSensitive(false)\n\n//val filteredTokens \u003d stopWordsFilter.transform(itemsDF)",
      "dateUpdated": "Jan 28, 2016 8:39:27 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452407914910_1583997153",
      "id": "20160110-063834_1306770047",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.StopWordsRemover\nstopWordsFilter: org.apache.spark.ml.feature.StopWordsRemover \u003d stopWords_52b11fafcc5e\n"
      },
      "dateCreated": "Jan 10, 2016 6:38:34 AM",
      "dateStarted": "Jan 28, 2016 8:40:02 PM",
      "dateFinished": "Jan 28, 2016 8:40:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val coreNLPLemmatizer \u003d new CoreNLP()\n  .setInputCol(stopWordsFilter.getOutputCol)\n  .setOutputCol(\"lemmaWords\")\n  .setAnnotators(Array(\"lemma\"))\n  .setFlattenNestedFields(Array(\"sentence_token_lemma\"))\n  \n//z.show(coreNLPLemmatizer.transform(itemsDF))\n",
      "dateUpdated": "Jan 28, 2016 8:39:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452470958952_-1650818947",
      "id": "20160111-000918_1206065802",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "coreNLPLemmatizer: com.databricks.spark.corenlp.CoreNLP \u003d corenlp_ed73f9cebe0e\n"
      },
      "dateCreated": "Jan 11, 2016 12:09:18 AM",
      "dateStarted": "Jan 28, 2016 8:40:08 PM",
      "dateFinished": "Jan 28, 2016 8:40:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.feature.VectorAssembler\n\n// Combine TF and IDF \nval featureVectorAssembler \u003d new VectorAssembler()\n  .setInputCols(Array(tf.getOutputCol, idf.getOutputCol))\n  .setOutputCol(\"allFeatures\")",
      "dateUpdated": "Jan 28, 2016 8:39:27 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452407914911_1583612404",
      "id": "20160110-063834_108615110",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.VectorAssembler\n\u003cconsole\u003e:33: error: not found: value tf\n         .setInputCols(Array(tf.getOutputCol, idf.getOutputCol))\n                             ^\n"
      },
      "dateCreated": "Jan 10, 2016 6:38:34 AM",
      "dateStarted": "Jan 28, 2016 8:40:08 PM",
      "dateFinished": "Jan 28, 2016 8:40:09 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.Pipeline\n\nval pipeline \u003d new Pipeline()\n  .setStages(Array(coreNLPTokenizer, stopWordsFilter, coreNLPLemmatizer))",
      "dateUpdated": "Jan 28, 2016 8:39:27 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452407914911_1583612404",
      "id": "20160110-063834_1791197538",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.Pipeline\n\u003cconsole\u003e:39: error: not found: value coreNLPTokenizer\n         .setStages(Array(coreNLPTokenizer, stopWordsFilter, coreNLPLemmatizer))\n                          ^\n"
      },
      "dateCreated": "Jan 10, 2016 6:38:34 AM",
      "dateStarted": "Jan 28, 2016 8:40:09 PM",
      "dateFinished": "Jan 28, 2016 8:40:09 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val model \u003d pipeline.fit(itemsDF)",
      "dateUpdated": "Jan 28, 2016 8:39:27 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452407914911_1583612404",
      "id": "20160110-063834_393936262",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:33: error: not found: value pipeline\n         val model \u003d pipeline.fit(itemsDF)\n                     ^\n"
      },
      "dateCreated": "Jan 10, 2016 6:38:34 AM",
      "dateStarted": "Jan 28, 2016 8:40:09 PM",
      "dateFinished": "Jan 28, 2016 8:40:10 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 28, 2016 8:39:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452407914912_1569376694",
      "id": "20160110-063834_178903318",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 10, 2016 6:38:34 AM",
      "dateStarted": "Jan 28, 2016 8:40:10 PM",
      "dateFinished": "Jan 28, 2016 8:40:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "NLP/07: TODO: Spark ML + Stanford CoreNLP (Sentiment)",
  "id": "2BAFTUCBH",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}