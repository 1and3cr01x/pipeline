{
  "paragraphs": [
    {
      "text": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451318099632_1275613400",
      "id": "20151228-155459_1951209027",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature\n"
      },
      "dateCreated": "Dec 28, 2015 3:54:59 PM",
      "dateStarted": "Jan 4, 2016 3:37:45 AM",
      "dateFinished": "Jan 4, 2016 3:37:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load dataset including tags",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451323111480_-372297908",
      "id": "20151228-171831_1063248354",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [category: string, description: string, id: bigint, img: string, tags: array\u003cstring\u003e, title: string]\n"
      },
      "dateCreated": "Dec 28, 2015 5:18:31 PM",
      "dateStarted": "Jan 4, 2016 3:37:45 AM",
      "dateFinished": "Jan 4, 2016 3:37:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Item Domain Class",
      "text": "// Note:  This class must be defined in a separate cell from where it\u0027s being used, otherwise you\u0027ll see this error:\n//   https://fluxcapacitor.zendesk.com/hc/en-us/articles/215310168\n\ncase class Item(id: Long, title: String, description: String, img: String, tags: Set[String]) {\n  override def toString: String \u003d id + \", \" + title + \", \" + tags\n}",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451279325043_812927468",
      "id": "20151228-050845_1911880139",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class Item\n"
      },
      "dateCreated": "Dec 28, 2015 5:08:45 AM",
      "dateStarted": "Jan 4, 2016 3:37:57 AM",
      "dateFinished": "Jan 4, 2016 3:38:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert Row to Item",
      "text": "import org.apache.spark.sql.Row\n\n// Convert from RDD[Row] to RDD[Item]\nval itemsRDD \u003d itemsDF.select($\"id\", $\"title\", $\"description\", $\"img\", $\"tags\").map(row \u003d\u003e\n  Item(row.getLong(0), row.getString(1), row.getString(2), row.getString(3), row.getSeq[String](4).toSet)\n)",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451864080583_-182003292",
      "id": "20160103-233440_515380012",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nitemsRDD: org.apache.spark.rdd.RDD[Item] \u003d MapPartitionsRDD[8] at map at \u003cconsole\u003e:40\n"
      },
      "dateCreated": "Jan 3, 2016 11:34:40 PM",
      "dateStarted": "Jan 4, 2016 3:38:00 AM",
      "dateFinished": "Jan 4, 2016 3:38:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Approximate LSH Jaccard Similarity Algo",
      "text": "def getApproxLSHJaccardSimilarity(minHasher: MinHasher32, item1: Item, item2: Item): Double \u003d {\n  val minHashSignatureItem1 \u003d item1.tags.map(tag \u003d\u003e minHasher.init(tag))\n    .reduce((leftMinHashSignature, rightMinHashSignature) \u003d\u003e minHasher.plus(leftMinHashSignature, rightMinHashSignature))\n\n  val minHashSignatureItem2 \u003d item2.tags.map(tag \u003d\u003e minHasher.init(tag))\n    .reduce((leftMinHashSignature, rightMinHashSignature) \u003d\u003e minHasher.plus(leftMinHashSignature, rightMinHashSignature))\n\n  minHasher.similarity(minHashSignatureItem1, minHashSignatureItem2).toDouble\n}",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451864113520_-825875381",
      "id": "20160103-233513_99791713",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "getApproxLSHJaccardSimilarity: (minHasher: com.twitter.algebird.MinHasher32, item1: Item, item2: Item)Double\n"
      },
      "dateCreated": "Jan 3, 2016 11:35:13 PM",
      "dateStarted": "Jan 4, 2016 3:38:01 AM",
      "dateFinished": "Jan 4, 2016 3:38:02 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Approx Jaccard Similarity between all Distinct item pairs",
      "text": "val numHashes \u003d 100\nval minApproxJaccardSimilarityThreshold \u003d 0.1\n\nval numBands \u003d MinHasher.pickBands(minApproxJaccardSimilarityThreshold, numHashes)\nval minHasher \u003d new MinHasher32(numHashes, numBands)\n\nval allItemPairsRDD \u003d itemsRDD.cartesian(itemsRDD)\n\n// Filter out duplicates and preserve only the pairs with left id \u003c right id\nval approxDistinctItemPairsRDD \u003d allItemPairsRDD.filter(itemPair \u003d\u003e itemPair._1.id \u003c itemPair._2.id)\napproxDistinctItemPairsRDD.count()\n\n// Calculate Jaccard Similarity between all item pairs (cartesian, then de-duped)\n// Only keep pairs with a Jaccard Similarity above a specific threshold\nval approxSimilarItemsAboveThresholdRDD \u003d approxDistinctItemPairsRDD.flatMap(itemPair \u003d\u003e {\n  val jaccardSim \u003d getApproxLSHJaccardSimilarity(minHasher, itemPair._1, itemPair._2)\n  if (jaccardSim \u003e\u003d minApproxJaccardSimilarityThreshold)\n    Some(itemPair._1.id, itemPair._2.id, jaccardSim)\n  else\n    None\n})\n\nval approxSimilarItemPairCount \u003d approxSimilarItemsAboveThresholdRDD.count()\napproxSimilarItemsAboveThresholdRDD.collect().mkString(\",\")",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451317394187_1089784615",
      "id": "20151228-154314_719152611",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "numHashes: Int \u003d 100\nminApproxJaccardSimilarityThreshold: Double \u003d 0.1\nnumBands: Int \u003d 57\nminHasher: com.twitter.algebird.MinHasher32 \u003d com.twitter.algebird.MinHasher32@75e20736\nallItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d CartesianRDD[9] at cartesian at \u003cconsole\u003e:41\napproxDistinctItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d MapPartitionsRDD[10] at filter at \u003cconsole\u003e:44\nres14: Long \u003d 3160\napproxSimilarItemsAboveThresholdRDD: org.apache.spark.rdd.RDD[(Long, Long, Double)] \u003d MapPartitionsRDD[11] at flatMap at \u003cconsole\u003e:57\napproxSimilarItemPairCount: Long \u003d 923\nres19: String \u003d (1,6,0.16),(1,7,0.14),(1,8,0.3),(1,9,0.28),(1,10,0.31),(1,11,0.29),(1,12,0.66),(1,13,0.29),(1,14,0.24),(1,16,0.17),(1,18,0.23),(1,19,0.28),(1,20,0.23),(1,21,0.2),(1,22,0.29),(1,25,0.22),(1,27,0.2),(1,28,0.16),(1,29,0.27),(1,30,0.18),(1,31,0.35),(1,32,0.32),(1,33,0.27),(1,34,0.24),(1,37,0.1),(1,38,0.2),(1,42,0.14),(1,43,0.13),(1,45,0.24),(1,54,0.28),(1,59,0.24),(1,60,0.24),(1,68,0.28),(1,70,0.21),(1,73,0.14),(1,74,0.3),(1,75,0.24),(1,77,0.13),(1,79,0.5),(1,80,0.11),(1,81,0.11),(2,51,0.13),(2,73,0.12),(3,20,0.11),(3,21,0.11),(4,24,0.14),(5,49,0.47),(5,50,0.38),(5,55,0.39),(5,70,0.13),(6,7,0.79),(6,8,0.17),(6,9,0.24),(6,10,0.17),(6,11,0.16),(6,12,0.17),(6,13,0.23),(6,14,0.27),(6,16,0.2),(6,17,0.17),(6,18,0.14),(6,19,0.16),(6,20,0.12),(6,21,0.12),(6,22,0.23),(6,23,0.16),(6,2..."
      },
      "dateCreated": "Dec 28, 2015 3:43:14 PM",
      "dateStarted": "Jan 4, 2016 3:38:02 AM",
      "dateFinished": "Jan 4, 2016 3:38:06 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Shamelessly lifted from GraphX In Action (Great Book!)\ndef dijkstra(graph: Graph[VertexId, Double], srcId: VertexId): Graph[(VertexId, List[Any]), Double] \u003d {\n  // Dijkstra Shortest Path\n  var graph2 \u003d graph.mapVertices((vid,vd) \u003d\u003e \n    (false, if (vid \u003d\u003d srcId) 0 else Double.MaxValue, List[VertexId]()))\n\n  for (i \u003c- 1L to graph.vertices.count-1) {\n    // The fold() below simulates minBy() functionality\n    val currentVertexId \u003d graph2.vertices.filter(!_._2._1)\n      .fold((0L,(false,Double.MaxValue,List[VertexId]())))((a,b) \u003d\u003e\n        if (a._2._2 \u003c b._2._2) a else b)._1\n      \n\n    val newDistances \u003d graph2.aggregateMessages[(Double,List[VertexId])](ctx \u003d\u003e \n      if (ctx.srcId \u003d\u003d currentVertexId) \n        ctx.sendToDst((ctx.srcAttr._2 + ctx.attr, ctx.srcAttr._3 :+ ctx.srcId)),\n          (a,b) \u003d\u003e if (a._1 \u003c b._1) a else b)\n        \n    graph2 \u003d graph2.outerJoinVertices(newDistances)((vid, vd, newSum) \u003d\u003e {\n      val newSumVal \u003d newSum.getOrElse((Double.MaxValue,List[VertexId]()))\n        (vd._1 || vid \u003d\u003d currentVertexId, math.min(vd._2, newSumVal._1),\n          if (vd._2 \u003c newSumVal._1) vd._3 else newSumVal._2)})\n  }\n\n  val shortestPathGraph \u003d graph.outerJoinVertices(graph2.vertices)((vid, vd, dist) \u003d\u003e \n    (vd, dist.getOrElse((false,Double.MaxValue,List[VertexId]()))\n    .productIterator.toList.tail))\n  \n  shortestPathGraph\n}",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451876838156_-367956757",
      "id": "20160104-030718_1026185433",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dijkstra: (graph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Double], srcId: org.apache.spark.graphx.VertexId)org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double]\n"
      },
      "dateCreated": "Jan 4, 2016 3:07:18 AM",
      "dateStarted": "Jan 4, 2016 3:38:02 AM",
      "dateFinished": "Jan 4, 2016 3:38:06 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Similarity Pathway",
      "text": "val approxSimilarItemsAboveThresholdEdgsRDD \u003d approxSimilarItemsAboveThresholdRDD.map(rdd \u003d\u003e {\n  Edge(rdd._1, rdd._2, rdd._3) \n})\n\nval approxGraph \u003d Graph.fromEdges(approxSimilarItemsAboveThresholdEdgsRDD, 0L)\n\n//val src \u003d 21 // ElasticSearch\n//val dest \u003d 56 // MicroStrategy\n\nval src \u003d 21\nval dest \u003d 56\n\nval approxShortestPathGraph \u003d dijkstra(approxGraph, src)\n\n// Filter out only the ones with dest as the destination vertex\nval approxShortestPathFromSrcToDest \u003d approxShortestPathGraph.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2\n\n//approxShortestPathFromSrcToDest.collect().mkString(\",\")\n\n// Example 1\n//\n// Shortest Path\n// 21 (ElasticSearch) -\u003e 28 (ZooKeeper) -\u003e 56 (MicroStrategy)\n\n// Tag Analysis of Shortest Path\n//\n// 21 Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations\n// 28 Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy\n// 56 BI, UI, Visualization, SQL\n\n// Example 2\n//\n// Shortest Path\n// 35 (NLTK) -\u003e 37 (iPython/Jupyter) -\u003e 41 (SQL) -\u003e 42 (Scala)\n//\n// Tag Analysis of Shortest Path\n//\n// 35:  Library, NLP, Python, Text Analytics\n// 37:  Notebook, Python, Java, Scala, R, Visualization, SQL\n// 41:  Programming Language, SQL, RDBMS, Interpreted\n// 42:  Programming Language, Functional, JVM, Static Typing, Compiled",
      "dateUpdated": "Jan 4, 2016 3:44:32 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451876564095_671251667",
      "id": "20160104-030244_1592701688",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "approxSimilarItemsAboveThresholdEdgsRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] \u003d MapPartitionsRDD[12] at map at \u003cconsole\u003e:56\napproxGraph: org.apache.spark.graphx.Graph[Long,Double] \u003d org.apache.spark.graphx.impl.GraphImpl@5c604c9e\nsrc: Int \u003d 21\ndest: Int \u003d 56\napproxShortestPathGraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double] \u003d org.apache.spark.graphx.impl.GraphImpl@729e4d7d\napproxShortestPathFromSrcToDest: List[Any] \u003d List(0.25, List(21, 28))\n"
      },
      "dateCreated": "Jan 4, 2016 3:02:44 AM",
      "dateStarted": "Jan 4, 2016 3:38:06 AM",
      "dateFinished": "Jan 4, 2016 3:38:28 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Power Iteration Clustering Of Items Based On Approx LSH Tag Jaccard Similarity",
      "text": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\n\nval clustering \u003d new PowerIterationClustering().setK(5).setMaxIterations(10)\n\nval clusteringModel \u003d clustering.run(approxSimilarItemsAboveThresholdRDD)\n\nval clusterAssignmentsRDD \u003d clusteringModel.assignments.map { assignment \u003d\u003e\n  (assignment.id, assignment.cluster)\n}",
      "dateUpdated": "Jan 4, 2016 3:44:10 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451865415503_1747751574",
      "id": "20160103-235655_580851070",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\nclustering: org.apache.spark.mllib.clustering.PowerIterationClustering \u003d org.apache.spark.mllib.clustering.PowerIterationClustering@34509c66\nclusteringModel: org.apache.spark.mllib.clustering.PowerIterationClusteringModel \u003d org.apache.spark.mllib.clustering.PowerIterationClusteringModel@4646093b\nclusterAssignmentsRDD: org.apache.spark.rdd.RDD[(Long, Int)] \u003d MapPartitionsRDD[2611] at map at \u003cconsole\u003e:77\n"
      },
      "dateCreated": "Jan 3, 2016 11:56:55 PM",
      "dateStarted": "Jan 4, 2016 3:44:10 AM",
      "dateFinished": "Jan 4, 2016 3:44:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert The ClusterAssignmentsRDD Into A DataFrame",
      "text": "val schema \u003d StructType(StructField(\"itemId\", LongType, true) :: StructField(\"clusterId\", IntegerType, true) :: Nil)\n\nval clusterAssignmentsRowRDD \u003d clusterAssignmentsRDD.map(clusterAssignmentRDD \u003d\u003e \n  Row(clusterAssignmentRDD._1, clusterAssignmentRDD._2))\n\nval clusterAssignmentsDF \u003d sqlContext.createDataFrame(clusterAssignmentsRowRDD, schema)",
      "dateUpdated": "Jan 4, 2016 3:45:02 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451865467022_150350762",
      "id": "20160103-235747_159734173",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "schema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(itemId,LongType,true), StructField(clusterId,IntegerType,true))\nclusterAssignmentsRowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[2612] at map at \u003cconsole\u003e:79\nclusterAssignmentsDF: org.apache.spark.sql.DataFrame \u003d [itemId: bigint, clusterId: int]\n"
      },
      "dateCreated": "Jan 3, 2016 11:57:47 PM",
      "dateStarted": "Jan 4, 2016 3:45:02 AM",
      "dateFinished": "Jan 4, 2016 3:45:03 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of items within a cluster",
      "text": "val joinedItemsClustersCountDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").groupBy($\"clusterId\", $\"tags\")\n  .agg(count($\"itemId\")).sort($\"clusterId\" desc)\n\nz.show(joinedItemsClustersCountDF)",
      "dateUpdated": "Jan 4, 2016 3:45:05 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count(itemId)",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451352986034_-1943585352",
      "id": "20151229-013626_1831771232",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "clusterId\ttags\tcount(itemId)\n4\tWrappedArray(Library, Spark, Machine Learning)\t1\n4\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\t1\n4\tWrappedArray(BI, UI, Visualization, SQL)\t2\n4\tWrappedArray(Library, Spark, Streaming)\t1\n4\tWrappedArray(Library, Streaming, AWS)\t1\n4\tWrappedArray(Library, Python, Machine Learning)\t1\n4\tWrappedArray(Database, SQL, RDBMS, Transactional)\t3\n4\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\t1\n4\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\t1\n4\tWrappedArray(Library, Deep Learning, Neural Networks)\t1\n4\tWrappedArray(Database, Data Warehouse, SQL)\t2\n4\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\t1\n4\tWrappedArray(Library, Spark, HiveQL, SQL)\t1\n4\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\t1\n4\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\t1\n4\tWrappedArray(Library, Graph Analytics, Spark)\t1\n4\tWrappedArray(Cluster Resource Manager, Docker, Container)\t1\n3\tWrappedArray(File Format, Evolving Schema, Nested Schema)\t1\n3\tWrappedArray(File Format, Key Value Store)\t2\n3\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\t1\n3\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\t1\n3\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\t2\n3\tWrappedArray(File Format)\t1\n2\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\t1\n2\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\t1\n2\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\t4\n2\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\t1\n2\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\t1\n2\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\t1\n1\tWrappedArray(Cloud Provider, Google)\t1\n1\tWrappedArray(Cloud Provider, AWS)\t1\n1\tWrappedArray(Container, Linux, DevOps, Deployment)\t1\n1\tWrappedArray(Cloud Provider, Data Center)\t1\n1\tWrappedArray(Cloud Provider, Microsoft)\t1\n0\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\t1\n0\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\t1\n0\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n0\tWrappedArray(Library, Machine Learning, Java, JVM)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\t1\n0\tWrappedArray(File System, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\t1\n0\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\t1\n0\tWrappedArray(Library, Search, Java, JVM, Python)\t1\n0\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\t1\n0\tWrappedArray(Library, Graph Analytics, Java, JVM)\t1\n0\tWrappedArray(Library, NLP, Python, Text Analytics)\t1\n0\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\t1\n0\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\t1\n0\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\t1\n0\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\t1\n0\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\t1\n0\tWrappedArray(Data Import, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\t1\n0\tWrappedArray(Library, Java, JVM, Log Collection)\t1\n0\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\t1\n0\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\t1\n0\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\t1\n0\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n0\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\t1\n0\tWrappedArray(Streaming, Java, JVM)\t1\n0\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\t1\n0\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\t1\n"
      },
      "dateCreated": "Dec 29, 2015 1:36:26 AM",
      "dateStarted": "Jan 4, 2016 3:45:05 AM",
      "dateFinished": "Jan 4, 2016 3:45:06 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cluster Details",
      "text": "// Enrich the cluster assignment tuples with itemsDF\nval joinedItemsClustersDF \u003d clusterAssignmentsDF.select($\"clusterId\", $\"itemId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").select($\"itemId\", $\"clusterId\", $\"title\", $\"tags\").sort($\"clusterId\", $\"itemId\")\n  \nz.show(joinedItemsClustersDF)",
      "dateUpdated": "Jan 4, 2016 3:45:16 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 474.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451348226921_-115634507",
      "id": "20151229-001706_1772528099",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "itemId\tclusterId\ttitle\ttags\n1\t0\tApache Cassandra\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\n6\t0\tApache Flink\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\n7\t0\tApache Spark\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\n8\t0\tApache Flume\tWrappedArray(Library, Java, JVM, Log Collection)\n9\t0\tApache Giraph\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\n10\t0\tApache HDFS\tWrappedArray(File System, Hadoop, Java, JVM)\n11\t0\tApache YARN\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\n12\t0\tApache HBase\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\n13\t0\tApache MapReduce\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\n14\t0\tApache Hive\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\n16\t0\tApache HUE\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\n18\t0\tApache Kafka\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\n19\t0\tApache Lucene\tWrappedArray(Library, Search, Java, JVM, Python)\n20\t0\tApache Solr\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\n21\t0\tElasticSearch\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\n22\t0\tApache Mahout\tWrappedArray(Library, Machine Learning, Java, JVM)\n25\t0\tApache Parquet\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\n27\t0\tApache Pig\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n28\t0\tApache ZooKeeper\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n29\t0\tStanford CoreNLP\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\n30\t0\tApache Tez\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\n31\t0\tApache Storm\tWrappedArray(Streaming, Java, JVM)\n32\t0\tApache Sqoop\tWrappedArray(Data Import, Hadoop, Java, JVM)\n33\t0\tApache Oozie\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\n34\t0\tApache Nifi\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\n35\t0\tNLTK\tWrappedArray(Library, NLP, Python, Text Analytics)\n37\t0\tiPython/Jupyter\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\n38\t0\tApache Zeppelin\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\n42\t0\tScala\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\n43\t0\tJava\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\n45\t0\tPresto\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\n71\t0\tElastic MapReduce\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\n73\t0\tMemcached\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\n74\t0\tNeo4j\tWrappedArray(Library, Graph Analytics, Java, JVM)\n79\t0\tTitan GraphDB\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\n4\t1\tDocker\tWrappedArray(Container, Linux, DevOps, Deployment)\n5\t1\tMicrosft Azure\tWrappedArray(Cloud Provider, Microsoft)\n49\t1\tAmazon Web Services\tWrappedArray(Cloud Provider, AWS)\n50\t1\tGoogle Cloud Platform\tWrappedArray(Cloud Provider, Google)\n55\t1\tOn-Premise\tWrappedArray(Cloud Provider, Data Center)\n15\t2\tHortonworks\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n26\t2\tApache ORC\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\n41\t2\tSQL\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\n46\t2\tMapR\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n47\t2\tCloudera\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n48\t2\tIBM BigInsights\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n66\t2\tRedshift\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\n68\t2\tDynamoDB\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\n77\t2\tS3\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\n2\t3\tTachyon\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\n40\t3\tR\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n44\t3\tPython\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n51\t3\tRedis\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\n52\t3\tJSON\tWrappedArray(File Format, Key Value Store)\n53\t3\tXML\tWrappedArray(File Format, Key Value Store)\n64\t3\tCSV\tWrappedArray(File Format)\n76\t3\tProtobuffers\tWrappedArray(File Format, Evolving Schema, Nested Schema)\n3\t4\tApache Ambari\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\n17\t4\tApache Impala\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\n23\t4\tApache Drill\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\n24\t4\tApache Mesos\tWrappedArray(Cluster Resource Manager, Docker, Container)\n36\t4\tSci-Kit Learn\tWrappedArray(Library, Python, Machine Learning)\n39\t4\tTableau\tWrappedArray(BI, UI, Visualization, SQL)\n54\t4\tMongoDB\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\n56\t4\tMicroStrategy\tWrappedArray(BI, UI, Visualization, SQL)\n57\t4\tKnime\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\n59\t4\tOracle\tWrappedArray(Database, SQL, RDBMS, Transactional)\n60\t4\tMySQL\tWrappedArray(Database, SQL, RDBMS, Transactional)\n61\t4\tSpark ML/MLlib\tWrappedArray(Library, Spark, Machine Learning)\n62\t4\tSpark Streaming\tWrappedArray(Library, Spark, Streaming)\n63\t4\tSpark SQL\tWrappedArray(Library, Spark, HiveQL, SQL)\n65\t4\tDeep Learning 4J\tWrappedArray(Library, Deep Learning, Neural Networks)\n67\t4\tKinesis\tWrappedArray(Library, Streaming, AWS)\n69\t4\tSpark GraphX\tWrappedArray(Library, Graph Analytics, Spark)\n70\t4\tSQL Server\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\n72\t4\tDato GraphLab Create\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\n75\t4\tPostgres\tWrappedArray(Database, SQL, RDBMS, Transactional)\n78\t4\tTensor Flow\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\n80\t4\tTeradata\tWrappedArray(Database, Data Warehouse, SQL)\n81\t4\tVertica\tWrappedArray(Database, Data Warehouse, SQL)\n"
      },
      "dateCreated": "Dec 29, 2015 12:17:06 AM",
      "dateStarted": "Jan 4, 2016 3:45:16 AM",
      "dateFinished": "Jan 4, 2016 3:45:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Show the intersection of tags within each cluster\nval clusterTagIntersectionDF \u003d joinedItemsClustersDF.explode(\"tags\", \"tag\"){c: List[String] \u003d\u003e c}",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451350991110_1019547650",
      "id": "20151229-010311_1992313547",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:36: error: not found: value joinedItemsClustersDF\n       val clusterTagIntersectionDF \u003d joinedItemsClustersDF.explode(\"tags\", \"tag\"){c: List[String] \u003d\u003e c}\n                                      ^\n"
      },
      "dateCreated": "Dec 29, 2015 1:03:11 AM",
      "dateStarted": "Jan 4, 2016 3:38:28 AM",
      "dateFinished": "Jan 4, 2016 3:38:28 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Given an itemId (or List of itemIds), recommend more itemIds based on similar cluster \n//        and/or closest jaccard similarity ",
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451397463773_538617890",
      "id": "20151229-135743_947751301",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 29, 2015 1:57:43 PM",
      "dateStarted": "Jan 4, 2016 3:38:28 AM",
      "dateFinished": "Jan 4, 2016 3:38:28 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 4, 2016 3:37:45 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451400014910_399953924",
      "id": "20151229-144014_562495240",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Dec 29, 2015 2:40:14 PM",
      "dateStarted": "Jan 4, 2016 3:38:28 AM",
      "dateFinished": "Jan 4, 2016 3:38:28 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Live Recs/05: Generate Recs (Similar Items by Tag - Approx LSH)",
  "id": "2B9YAWU6M",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}