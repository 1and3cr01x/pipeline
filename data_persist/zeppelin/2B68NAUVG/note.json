{
  "paragraphs": [
    {
      "text": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._",
      "dateUpdated": "Jan 7, 2016 11:00:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451318099632_1275613400",
      "id": "20151228-155459_1951209027",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n"
      },
      "dateCreated": "Dec 28, 2015 3:54:59 PM",
      "dateStarted": "Jan 7, 2016 11:00:46 PM",
      "dateFinished": "Jan 7, 2016 11:00:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load dataset including tags",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451323111480_-372297908",
      "id": "20151228-171831_1063248354",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [category: string, description: string, id: bigint, img: string, tags: array\u003cstring\u003e, title: string]\n"
      },
      "dateCreated": "Dec 28, 2015 5:18:31 PM",
      "dateStarted": "Jan 7, 2016 11:00:46 PM",
      "dateFinished": "Jan 7, 2016 11:00:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Number of distinct Tags found in the dataset",
      "text": "val distinctCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").distinct()\ndistinctCounts.count()",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451398205879_822632098",
      "id": "20151229-141005_354437963",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "distinctCounts: org.apache.spark.sql.DataFrame \u003d [tag: string]\nres85: Long \u003d 108\n"
      },
      "dateCreated": "Dec 29, 2015 2:10:05 PM",
      "dateStarted": "Jan 7, 2016 11:00:47 PM",
      "dateFinished": "Jan 7, 2016 11:00:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of tags within dataset",
      "text": "val tagCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").groupBy($\"tag\")\n  .agg(count($\"tag\").as(\"count\"))\n  .orderBy($\"count\".desc)\n  .limit(10)\n\nz.show(tagCounts)",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "pieChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451277734947_-102286185",
      "id": "20151228-044214_1740589078",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "tag\tcount\nJVM\t31\nJava\t31\nSQL\t24\nHadoop\t16\nLibrary\t15\nDatabase\t12\nData Procesing Execution Engine\t11\nPython\t11\nSpark\t9\nQuery Processing\t8\n"
      },
      "dateCreated": "Dec 28, 2015 4:42:14 AM",
      "dateStarted": "Jan 7, 2016 11:00:47 PM",
      "dateFinished": "Jan 7, 2016 11:00:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Domain-specific Case Class and Jaccard Similarity Algorithm",
      "text": "// Note:  This class must be defined in a separate cell from where it\u0027s being used, otherwise you\u0027ll see this error:\n//   https://fluxcapacitor.zendesk.com/hc/en-us/articles/215310168\n\ncase class Item(id: Long, title: String, description: String, img: String, tags: Set[String]) {\n  override def toString: String \u003d id + \", \" + title + \", \" + tags\n}",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451279325043_812927468",
      "id": "20151228-050845_1911880139",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class Item\n"
      },
      "dateCreated": "Dec 28, 2015 5:08:45 AM",
      "dateStarted": "Jan 7, 2016 11:00:48 PM",
      "dateFinished": "Jan 7, 2016 11:00:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert Row into Item",
      "text": "import org.apache.spark.sql.Row\n\n// Convert from RDD[Row] to RDD[Item]\nval itemsRDD \u003d itemsDF.select($\"id\", $\"title\", $\"description\", $\"img\", $\"tags\").map(row \u003d\u003e\n  Item(row.getLong(0), row.getString(1), row.getString(2), row.getString(3), row.getSeq[String](4).toSet)\n)\n",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451861953575_-679332547",
      "id": "20160103-225913_708349349",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nitemsRDD: org.apache.spark.rdd.RDD[Item] \u003d MapPartitionsRDD[904] at map at \u003cconsole\u003e:73\n"
      },
      "dateCreated": "Jan 3, 2016 10:59:13 PM",
      "dateStarted": "Jan 7, 2016 11:00:49 PM",
      "dateFinished": "Jan 7, 2016 11:00:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Exact Jaccard Similarity Algo",
      "text": "def getJaccardSimilarity(item1: Item, item2: Item): Double \u003d {\n  val intersectTagSize \u003d (item1.tags \u0026 item2.tags).size\n  val unionTagSize \u003d (item1.tags ++ item2.tags).size\n    \n  val jaccardSimilarity \u003d \n    if (unionTagSize \u003e 0) intersectTagSize.toDouble / unionTagSize.toDouble\n  \telse 0.0\n\n  jaccardSimilarity\n}",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451861976887_1869674889",
      "id": "20160103-225936_549512917",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "getJaccardSimilarity: (item1: Item, item2: Item)Double\n"
      },
      "dateCreated": "Jan 3, 2016 10:59:36 PM",
      "dateStarted": "Jan 7, 2016 11:00:49 PM",
      "dateFinished": "Jan 7, 2016 11:00:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Exact Jaccard Similarity between all Distinct item pairs",
      "text": "\nval allItemPairsRDD \u003d itemsRDD.cartesian(itemsRDD)\n\n// Filter out duplicates and preserve only the pairs with left id \u003c right id\nval distinctItemPairsRDD \u003d allItemPairsRDD.filter(itemPair \u003d\u003e itemPair._1.id \u003c itemPair._2.id)\ndistinctItemPairsRDD.count()\n\nval minJaccardSimilarityThreshold \u003d 0.1\n\n// Calculate Jaccard Similarity between all item pairs (cartesian, then de-duped)\n// Only keep pairs with a Jaccard Similarity above a specific threshold\nval similarItemsAboveThresholdRDD \u003d distinctItemPairsRDD.flatMap(itemPair \u003d\u003e {\n  val jaccardSim \u003d getJaccardSimilarity(itemPair._1, itemPair._2)\n  if (jaccardSim \u003e\u003d minJaccardSimilarityThreshold)\n    Some(itemPair._1.id, itemPair._2.id, jaccardSim)\n  else\n    None\n})\n\nval similarItemPairCount \u003d similarItemsAboveThresholdRDD.count()\nsimilarItemsAboveThresholdRDD.collect().mkString(\",\")",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451317394187_1089784615",
      "id": "20151228-154314_719152611",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "allItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d CartesianRDD[905] at cartesian at \u003cconsole\u003e:74\ndistinctItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d MapPartitionsRDD[906] at filter at \u003cconsole\u003e:77\nres101: Long \u003d 3160\nminJaccardSimilarityThreshold: Double \u003d 0.1\nsimilarItemsAboveThresholdRDD: org.apache.spark.rdd.RDD[(Long, Long, Double)] \u003d MapPartitionsRDD[907] at flatMap at \u003cconsole\u003e:84\nsimilarItemPairCount: Long \u003d 913\nres107: String \u003d (1,6,0.11764705882352941),(1,8,0.25),(1,9,0.2222222222222222),(1,10,0.25),(1,11,0.25),(1,12,0.7142857142857143),(1,13,0.2222222222222222),(1,14,0.16666666666666666),(1,16,0.15384615384615385),(1,18,0.16666666666666666),(1,19,0.2222222222222222),(1,20,0.15384615384615385),(1,21,0.15384615384615385),(1,22,0.25),(1,25,0.15384615384615385),(1,27,0.16666666666666666),(1,28,0.14285714285714285),(1,29,0.2222222222222222),(1,30,0.14285714285714285),(1,31,0.2857142857142857),(1,32,0.25),(1,33,0.2222222222222222),(1,34,0.2),(1,38,0.14285714285714285),(1,42,0.1),(1,43,0.1),(1,45,0.2),(1,54,0.3333333333333333),(1,59,0.25),(1,60,0.25),(1,66,0.1),(1,68,0.3333333333333333),(1,70,0.2222222222222222),(1,73,0.1),(1,74,0.25),(1,75,0.25),(1,77,0.1111111111111111),(1,79,0.5),(1,80,0.125),(1..."
      },
      "dateCreated": "Dec 28, 2015 3:43:14 PM",
      "dateStarted": "Jan 7, 2016 11:00:50 PM",
      "dateFinished": "Jan 7, 2016 11:00:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Shamelessly lifted from GraphX In Action (Great Book!)\ndef dijkstra(graph: Graph[VertexId, Double], srcId: VertexId): Graph[(VertexId, List[Any]), Double] \u003d {\n  // Dijkstra Shortest Path\n  var graph2 \u003d graph.mapVertices((vid,vd) \u003d\u003e \n    (false, if (vid \u003d\u003d srcId) 0 else Double.MaxValue, List[VertexId]()))\n\n  for (i \u003c- 1L to graph.vertices.count-1) {\n    // The fold() below simulates minBy() functionality\n    val currentVertexId \u003d graph2.vertices.filter(!_._2._1)\n      .fold((0L,(false,Double.MaxValue,List[VertexId]())))((a,b) \u003d\u003e\n        if (a._2._2 \u003c b._2._2) a else b)._1\n      \n\n    val newDistances \u003d graph2.aggregateMessages[(Double,List[VertexId])](ctx \u003d\u003e \n      if (ctx.srcId \u003d\u003d currentVertexId) \n        ctx.sendToDst((ctx.srcAttr._2 + ctx.attr, ctx.srcAttr._3 :+ ctx.srcId)),\n          (a,b) \u003d\u003e if (a._1 \u003c b._1) a else b)\n        \n    graph2 \u003d graph2.outerJoinVertices(newDistances)((vid, vd, newSum) \u003d\u003e {\n      val newSumVal \u003d newSum.getOrElse((Double.MaxValue,List[VertexId]()))\n        (vd._1 || vid \u003d\u003d currentVertexId, math.min(vd._2, newSumVal._1),\n          if (vd._2 \u003c newSumVal._1) vd._3 else newSumVal._2)})\n  }\n\n  val shortestPathGraph \u003d graph.outerJoinVertices(graph2.vertices)((vid, vd, dist) \u003d\u003e \n    (vd, dist.getOrElse((false,Double.MaxValue,List[VertexId]()))\n    .productIterator.toList.tail))\n  \n  shortestPathGraph\n}",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451398384385_1393442935",
      "id": "20151229-141304_162052884",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dijkstra: (graph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Double], srcId: org.apache.spark.graphx.VertexId)org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double]\n"
      },
      "dateCreated": "Dec 29, 2015 2:13:04 PM",
      "dateStarted": "Jan 7, 2016 11:00:50 PM",
      "dateFinished": "Jan 7, 2016 11:00:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Similarity Pathway",
      "text": "val similarItemsAboveThresholdEdgeRDD \u003d similarItemsAboveThresholdRDD.map(rdd \u003d\u003e {\n  Edge(rdd._1, rdd._2, rdd._3) \n})\n\nval graph \u003d Graph.fromEdges(similarItemsAboveThresholdEdgeRDD, 0L)\n\nval src \u003d 21 // ElasticSearch\nval dest \u003d 56 // MicroStrategy\n\n//val src \u003d 35\n//val dest \u003d 42\n\nval shortestPathGraph \u003d dijkstra(graph, src)\n\n// Filter out only the ones with dest as the destination vertex\nval shortestPathFromSrcToDest \u003d shortestPathGraph.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2\n\n// Example 1\n//\n// Shortest Path\n// 21 (ElasticSearch) -0.1538-\u003e 45 (Presto) -0.1111-\u003e 56 (MicroStrategy)\n// 21 (ElasticSearch) -0.1538-\u003e 34 (NiFi) -0.1111-\u003e 56 (MicroStrategy)\n\n// Tag Analysis of Shortest Path\n//\n// 21 Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations\n// 34 Workflow, Streaming, Message Broker, Java, JVM, UI\n// 45 Data Processing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning\n// 56 BI, UI, Visualization, SQL\n\n// Example 2\n//\n// Shortest Path\n// 35 (NLTK) -\u003e 37 (iPython/Jupyter) -\u003e 41 (SQL) -\u003e 42 (Scala)\n//\n// Tag Analysis of Shortest Path\n//\n// 35:  Library, NLP, Python, Text Analytics\n// 37:  Notebook, Python, Java, Scala, R, Visualization, SQL\n// 41:  Programming Language, SQL, RDBMS, Interpreted\n// 42:  Programming Language, Functional, JVM, Static Typing, Compiled",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451352094574_-1165080675",
      "id": "20151229-012134_1599229617",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "similarItemsAboveThresholdEdgeRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] \u003d MapPartitionsRDD[908] at map at \u003cconsole\u003e:83\ngraph: org.apache.spark.graphx.Graph[Long,Double] \u003d org.apache.spark.graphx.impl.GraphImpl@5ffa6067\nsrc: Int \u003d 21\ndest: Int \u003d 56\nshortestPathGraph: org.apache.spark.graphx.Graph[(org.apache.spark.graphx.VertexId, List[Any]),Double] \u003d org.apache.spark.graphx.impl.GraphImpl@58cd5570\nshortestPathFromSrcToDest: List[Any] \u003d List(0.26495726495726496, List(21, 45))\n"
      },
      "dateCreated": "Dec 29, 2015 1:21:34 AM",
      "dateStarted": "Jan 7, 2016 11:00:53 PM",
      "dateFinished": "Jan 7, 2016 11:01:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Power Iteration Clustering of Items based on Tag Jaccard Similarity",
      "text": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\n\nval clustering \u003d new PowerIterationClustering().setK(5).setMaxIterations(10)\n\nval clusteringModel \u003d clustering.run(similarItemsAboveThresholdRDD)\n\nval clusterAssignmentsRDD \u003d clusteringModel.assignments.map { assignment \u003d\u003e\n  (assignment.id, assignment.cluster)\n}",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451334971967_402230190",
      "id": "20151228-203611_236636775",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\nclustering: org.apache.spark.mllib.clustering.PowerIterationClustering \u003d org.apache.spark.mllib.clustering.PowerIterationClustering@51d4fed4\nclusteringModel: org.apache.spark.mllib.clustering.PowerIterationClusteringModel \u003d org.apache.spark.mllib.clustering.PowerIterationClusteringModel@3c11d50\nclusterAssignmentsRDD: org.apache.spark.rdd.RDD[(Long, Int)] \u003d MapPartitionsRDD[2177] at map at \u003cconsole\u003e:89\n"
      },
      "dateCreated": "Dec 28, 2015 8:36:11 PM",
      "dateStarted": "Jan 7, 2016 11:00:53 PM",
      "dateFinished": "Jan 7, 2016 11:01:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert the clusterAssignmentsRDD into a DataFrame",
      "text": "val schema \u003d StructType(StructField(\"itemId\", LongType, true) :: StructField(\"clusterId\", IntegerType, true) :: Nil)\n\nval clusterAssignmentsRowRDD \u003d clusterAssignmentsRDD.map(clusterAssignmentRDD \u003d\u003e \n  Row(clusterAssignmentRDD._1, clusterAssignmentRDD._2))\n\nval clusterAssignmentsDF \u003d sqlContext.createDataFrame(clusterAssignmentsRowRDD, schema)",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451335712814_493669088",
      "id": "20151228-204832_49888518",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "schema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(itemId,LongType,true), StructField(clusterId,IntegerType,true))\nclusterAssignmentsRowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[2178] at map at \u003cconsole\u003e:91\nclusterAssignmentsDF: org.apache.spark.sql.DataFrame \u003d [itemId: bigint, clusterId: int]\n"
      },
      "dateCreated": "Dec 28, 2015 8:48:32 PM",
      "dateStarted": "Jan 7, 2016 11:01:08 PM",
      "dateFinished": "Jan 7, 2016 11:01:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of items within a cluster",
      "text": "val joinedItemsClustersCountDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").groupBy($\"clusterId\", $\"tags\")\n  .agg(count($\"itemId\")).sort($\"clusterId\" desc)\n\nz.show(joinedItemsClustersCountDF)",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count(itemId)",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451352986034_-1943585352",
      "id": "20151229-013626_1831771232",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "clusterId\ttags\tcount(itemId)\n4\tWrappedArray(Library, Graph Analytics, Java, JVM)\t1\n4\tWrappedArray(Streaming, Java, JVM)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\t1\n4\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\t1\n4\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\t1\n4\tWrappedArray(Data Import, Hadoop, Java, JVM)\t1\n4\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\t1\n4\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\t1\n4\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\t1\n4\tWrappedArray(File System, Hadoop, Java, JVM)\t1\n4\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\t1\n4\tWrappedArray(Library, Java, JVM, Log Collection)\t1\n4\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\t1\n4\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\t1\n4\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\t1\n4\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\t1\n4\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\t1\n4\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n4\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\t1\n4\tWrappedArray(Library, Machine Learning, Java, JVM)\t1\n4\tWrappedArray(Library, Search, Java, JVM, Python)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\t1\n4\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\t1\n4\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\t1\n3\tWrappedArray(Cloud Provider, AWS)\t1\n3\tWrappedArray(File Format)\t1\n3\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\t1\n3\tWrappedArray(File Format, Evolving Schema, Nested Schema)\t1\n3\tWrappedArray(File Format, Key Value Store)\t2\n3\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\t1\n3\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\t2\n3\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\t1\n2\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\t1\n2\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\t4\n2\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\t1\n2\tWrappedArray(Database, Data Warehouse, SQL)\t2\n2\tWrappedArray(Cluster Resource Manager, Docker, Container)\t1\n2\tWrappedArray(Library, Spark, Machine Learning)\t1\n2\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\t1\n2\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\t1\n2\tWrappedArray(Library, Streaming, AWS)\t1\n2\tWrappedArray(Database, SQL, RDBMS, Transactional)\t3\n2\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\t1\n2\tWrappedArray(BI, UI, Visualization, SQL)\t2\n2\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\t1\n2\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\t1\n2\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\t1\n2\tWrappedArray(Library, Spark, HiveQL, SQL)\t1\n2\tWrappedArray(Library, Spark, Streaming)\t1\n2\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\t1\n2\tWrappedArray(Library, Deep Learning, Neural Networks)\t1\n1\tWrappedArray(Container, Linux, DevOps, Deployment)\t1\n1\tWrappedArray(Cloud Provider, Google)\t1\n1\tWrappedArray(Cloud Provider, Data Center)\t1\n1\tWrappedArray(Cloud Provider, Microsoft)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\t1\n0\tWrappedArray(Library, NLP, Python, Text Analytics)\t1\n0\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\t1\n0\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\t1\n0\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\t1\n0\tWrappedArray(Library, Graph Analytics, Spark)\t1\n0\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\t1\n0\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\t1\n0\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\t1\n0\tWrappedArray(Library, Python, Machine Learning)\t1\n"
      },
      "dateCreated": "Dec 29, 2015 1:36:26 AM",
      "dateStarted": "Jan 7, 2016 11:01:10 PM",
      "dateFinished": "Jan 7, 2016 11:01:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cluster Details",
      "text": "// Enrich the cluster assignment tuples with itemsDF\nval joinedItemsClustersDF \u003d clusterAssignmentsDF.select($\"clusterId\", $\"itemId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").select($\"itemId\", $\"clusterId\", $\"title\", $\"tags\").sort($\"clusterId\", $\"itemId\")\n  \nz.show(joinedItemsClustersDF)",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 474.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451348226921_-115634507",
      "id": "20151229-001706_1772528099",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "itemId\tclusterId\ttitle\ttags\n17\t0\tApache Impala\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\n23\t0\tApache Drill\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\n25\t0\tApache Parquet\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\n35\t0\tNLTK\tWrappedArray(Library, NLP, Python, Text Analytics)\n36\t0\tSci-Kit Learn\tWrappedArray(Library, Python, Machine Learning)\n37\t0\tiPython/Jupyter\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\n42\t0\tScala\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\n43\t0\tJava\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\n45\t0\tPresto\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\n69\t0\tSpark GraphX\tWrappedArray(Library, Graph Analytics, Spark)\n72\t0\tDato GraphLab Create\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\n73\t0\tMemcached\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\n4\t1\tDocker\tWrappedArray(Container, Linux, DevOps, Deployment)\n5\t1\tMicrosft Azure\tWrappedArray(Cloud Provider, Microsoft)\n50\t1\tGoogle Cloud Platform\tWrappedArray(Cloud Provider, Google)\n55\t1\tOn-Premise\tWrappedArray(Cloud Provider, Data Center)\n15\t2\tHortonworks\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n24\t2\tApache Mesos\tWrappedArray(Cluster Resource Manager, Docker, Container)\n39\t2\tTableau\tWrappedArray(BI, UI, Visualization, SQL)\n41\t2\tSQL\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\n46\t2\tMapR\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n47\t2\tCloudera\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n48\t2\tIBM BigInsights\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n54\t2\tMongoDB\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\n56\t2\tMicroStrategy\tWrappedArray(BI, UI, Visualization, SQL)\n57\t2\tKnime\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\n59\t2\tOracle\tWrappedArray(Database, SQL, RDBMS, Transactional)\n60\t2\tMySQL\tWrappedArray(Database, SQL, RDBMS, Transactional)\n61\t2\tSpark ML/MLlib\tWrappedArray(Library, Spark, Machine Learning)\n62\t2\tSpark Streaming\tWrappedArray(Library, Spark, Streaming)\n63\t2\tSpark SQL\tWrappedArray(Library, Spark, HiveQL, SQL)\n65\t2\tDeep Learning 4J\tWrappedArray(Library, Deep Learning, Neural Networks)\n66\t2\tRedshift\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\n67\t2\tKinesis\tWrappedArray(Library, Streaming, AWS)\n68\t2\tDynamoDB\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\n70\t2\tSQL Server\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\n71\t2\tElastic MapReduce\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\n75\t2\tPostgres\tWrappedArray(Database, SQL, RDBMS, Transactional)\n77\t2\tS3\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\n78\t2\tTensor Flow\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\n80\t2\tTeradata\tWrappedArray(Database, Data Warehouse, SQL)\n81\t2\tVertica\tWrappedArray(Database, Data Warehouse, SQL)\n2\t3\tTachyon\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\n26\t3\tApache ORC\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\n40\t3\tR\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n44\t3\tPython\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n49\t3\tAmazon Web Services\tWrappedArray(Cloud Provider, AWS)\n51\t3\tRedis\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\n52\t3\tJSON\tWrappedArray(File Format, Key Value Store)\n53\t3\tXML\tWrappedArray(File Format, Key Value Store)\n64\t3\tCSV\tWrappedArray(File Format)\n76\t3\tProtobuffers\tWrappedArray(File Format, Evolving Schema, Nested Schema)\n1\t4\tApache Cassandra\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\n3\t4\tApache Ambari\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\n6\t4\tApache Flink\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\n7\t4\tApache Spark\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\n8\t4\tApache Flume\tWrappedArray(Library, Java, JVM, Log Collection)\n9\t4\tApache Giraph\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\n10\t4\tApache HDFS\tWrappedArray(File System, Hadoop, Java, JVM)\n11\t4\tApache YARN\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\n12\t4\tApache HBase\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\n13\t4\tApache MapReduce\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\n14\t4\tApache Hive\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\n16\t4\tApache HUE\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\n18\t4\tApache Kafka\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\n19\t4\tApache Lucene\tWrappedArray(Library, Search, Java, JVM, Python)\n20\t4\tApache Solr\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\n21\t4\tElasticSearch\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\n22\t4\tApache Mahout\tWrappedArray(Library, Machine Learning, Java, JVM)\n27\t4\tApache Pig\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n28\t4\tApache ZooKeeper\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n29\t4\tStanford CoreNLP\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\n30\t4\tApache Tez\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\n31\t4\tApache Storm\tWrappedArray(Streaming, Java, JVM)\n32\t4\tApache Sqoop\tWrappedArray(Data Import, Hadoop, Java, JVM)\n33\t4\tApache Oozie\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\n34\t4\tApache Nifi\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\n38\t4\tApache Zeppelin\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\n74\t4\tNeo4j\tWrappedArray(Library, Graph Analytics, Java, JVM)\n79\t4\tTitan GraphDB\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\n"
      },
      "dateCreated": "Dec 29, 2015 12:17:06 AM",
      "dateStarted": "Jan 7, 2016 11:01:10 PM",
      "dateFinished": "Jan 7, 2016 11:01:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Show the intersection of tags within each cluster\nval clusterTagIntersectionDF \u003d joinedItemsClustersDF.explode(\"tags\", \"tag\"){c: List[String] \u003d\u003e c}",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451350991110_1019547650",
      "id": "20151229-010311_1992313547",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "clusterTagIntersectionDF: org.apache.spark.sql.DataFrame \u003d [itemId: bigint, clusterId: int, title: string, tags: array\u003cstring\u003e, tag: string]\n"
      },
      "dateCreated": "Dec 29, 2015 1:03:11 AM",
      "dateStarted": "Jan 7, 2016 11:01:11 PM",
      "dateFinished": "Jan 7, 2016 11:01:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Given an itemId (or List of itemIds), recommend more itemIds based on similar cluster \n//        and/or closest jaccard similarity ",
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451397463773_538617890",
      "id": "20151229-135743_947751301",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 29, 2015 1:57:43 PM",
      "dateStarted": "Jan 7, 2016 11:01:12 PM",
      "dateFinished": "Jan 7, 2016 11:01:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 7, 2016 11:00:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1451400014910_399953924",
      "id": "20151229-144014_562495240",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Dec 29, 2015 2:40:14 PM",
      "dateStarted": "Jan 7, 2016 11:01:12 PM",
      "dateFinished": "Jan 7, 2016 11:01:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Live Recs/04: Generate Recs (Similar Items by Tag - Similarity Graph)",
  "id": "2B68NAUVG",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}