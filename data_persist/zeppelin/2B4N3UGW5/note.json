{
  "paragraphs": [
    {
      "text": "// Databricks notebook source exported at Tue, 27 Oct 2015 09:41:12 UTC\n// MAGIC %md\n// MAGIC # Regression\n// MAGIC  \n// MAGIC This lab covers building regression models using linear regression and decision trees.  Also covered, are regression metrics, bootstrapping, and some traditional model evaluation methods.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Read in and prepare the data\n// MAGIC  \n// MAGIC First, we\u0027ll load the data from our parquet file.\n\n// COMMAND ----------\n\nval baseDir \u003d \"/mnt/ml-amsterdam/\"\nval irisDense \u003d sqlContext.read.parquet(baseDir + \"irisDense.parquet\")\n \ndisplay(irisDense)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC View the dataset.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Prepare the data so that we have the sepal width as our target and a dense vector containing sepal length as our features.\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.functions.{udf, lit}\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\n \nval getElement \u003d udf { (v:Vector, i: Int) \u003d\u003e v(i) }\nval getElementAsVector \u003d udf { (v:Vector, i: Int) \u003d\u003e Vectors.dense(Array(v(i))) }\n \nval irisSepal \u003d irisDense.select(getElement($\"features\", lit(1)).as(\"sepalWidth\"),\n                                 getElementAsVector($\"features\", lit(0)).as(\"features\"))\nirisSepal.cache()\n \ndisplay(irisSepal)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Build a linear regression model\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC First, we\u0027ll sample from our dataset to obtain a [bootstrap sample](https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29) of our data.\n// MAGIC  \n// MAGIC When using a `DataFrame` we can call `.sample` to return a random sample with or without replacement.  `sample` takes in a boolean for whether to sample with replacement and a fraction for what percentage of the dataset to sample.  Note that if replacement is true we can sample more than 100% of the data.  For example, to choose approximately twice as much data as the original dataset we can set fraction equal to 2.0.\n// MAGIC  \n// MAGIC An explanation of `sample` can be found under `DataFrame` in both the [Python](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) and [Scala](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.DataFrame) APIs.\n\n// COMMAND ----------\n\nval irisSepalSample \u003d irisSepal.sample(true, 1.0, 1)\ndisplay(irisSepalSample)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, let\u0027s create our linear regression object.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.regression.LinearRegression\n \nval lr \u003d new LinearRegression()\n  .setLabelCol(\"sepalWidth\")\n  .setMaxIter(1000)\nprintln(lr.explainParams)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll create a `Pipeline` that only contains one stage for the linear regression.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.Pipeline\n \nval pipeline \u003d new Pipeline().setStages(Array(lr))\n \nval pipelineModel \u003d pipeline.fit(irisSepalSample)\nval sepalPredictions \u003d pipelineModel.transform(irisSepalSample)\n \ndisplay(sepalPredictions)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What does our resulting model look like?\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.regression.LinearRegressionModel\n \nval lrModel \u003d pipelineModel.stages.last.asInstanceOf[LinearRegressionModel]\nprintln(lrModel.getClass)\n \nprintln(s\"\\n${lrModel.intercept} ${lrModel.weights}\")\n \nprintln(f\"\\nsepalWidth \u003d ${lrModel.intercept}%.3f + (${lrModel.weights(0)}%.3f * sepalLength)\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Boostrap sampling 100 models\n// MAGIC  \n// MAGIC In order to reason about how stable or models are and whether or not the coefficients are significantly different from zero, we\u0027ll draw 100 samples with replacement and generate a linear model for each of those samples.\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.DataFrame\n \ndef generateModels(df: DataFrame, pipeline: Pipeline, numModels: Int \u003d 100) \u003d {\n  (0 until numModels).map(i \u003d\u003e {\n    val sample \u003d df.sample(true, 1.0, i)\n    val pipelineModel \u003d pipeline.fit(sample)\n    pipelineModel.stages.last.asInstanceOf[LinearRegressionModel]\n  })\n}\n \nval sepalModels \u003d generateModels(irisSepal, pipeline)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll convert our models to a `DataFrame` so we can analyze the different values we obtained for intercept and weight.\n\n// COMMAND ----------\n\nval sepalModelsTuple \u003d sepalModels.map(m \u003d\u003e (m.intercept, m.weights(0)))\nval sepalModelResults \u003d sqlContext.createDataFrame(sepalModelsTuple)\n  .toDF(\"intercept\", \"weight\")\ndisplay(sepalModelResults)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Then we can use `describe` to see the count, mean, and standard deviation of our intercept and weight.  Based on these results it is pretty clear that there isn\u0027t a significant relationship between sepal length and sepal width.\n\n// COMMAND ----------\n\ndisplay(sepalModelResults.describe())\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Petal width vs petal length\n// MAGIC  \n// MAGIC We saw that there wasn\u0027t a significant relationship between sepal width and sepal length.  Let\u0027s repeat the analysis for the petal attributes.\n\n// COMMAND ----------\n\nval irisPetal \u003d irisDense.select(getElement($\"features\", lit(3)).as(\"petalWidth\"),\n                                 getElementAsVector($\"features\", lit(2)).as(\"features\"))\nirisPetal.cache()\ndisplay(irisPetal)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Create the linear regression estimator and pipeline estimator.\n\n// COMMAND ----------\n\nval lrPetal \u003d new LinearRegression()\n  .setLabelCol(\"petalWidth\")\n \nval petalPipeline \u003d new Pipeline().setStages(Array(lrPetal))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Generate the models.\n\n// COMMAND ----------\n\nval petalModels \u003d generateModels(irisPetal, petalPipeline)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We\u0027ll repeat the conversion of the model data to a `DataFrame` and then view the statistics on the `DataFrame`.\n\n// COMMAND ----------\n\nval petalModelsTuple \u003d petalModels.map(m \u003d\u003e (m.intercept, m.weights(0)))\nval petalModelResults \u003d sqlContext.createDataFrame(petalModelsTuple)\n  .toDF(\"intercept\", \"weight\")\ndisplay(petalModelResults)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC From these results, we can clearly see that this weight is significantly different from zero.\n\n// COMMAND ----------\n\ndisplay(petalModelResults.describe())\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### View and evaluate predictions\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC To start, we\u0027ll generate the predictions by using the first model in `petalModels`.\n\n// COMMAND ----------\n\nval petalPredictions \u003d petalModels(0).transform(irisPetal)\ndisplay(petalPredictions)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Next, we\u0027ll evaluate the model using the `RegressionEvaluator`.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nval regEval \u003d new RegressionEvaluator().setLabelCol(\"petalWidth\")\n \nprintln(regEval.explainParams)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC The default value for `RegressionEvaluator` is root mean square error (RMSE).  Let\u0027s view that first.\n\n// COMMAND ----------\n\nprintln(regEval.evaluate(petalPredictions))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC `RegressionEvaluator` also supports mean square error (MSE), \\\\( r^2 \\\\), and mean absolute error (MAE).  We\u0027ll view the \\\\( r^2 \\\\) metric next.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.param.ParamMap\n \nprintln(regEval.evaluate(petalPredictions, ParamMap(regEval.metricName -\u003e \"r2\")))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s evaluate our model on the sepal data as well.\n\n// COMMAND ----------\n\nval sepalPredictions \u003d sepalModels(0).transform(irisSepal)\nprintln(regEval.evaluate(sepalPredictions,\n                         ParamMap(regEval.metricName -\u003e \"r2\", regEval.labelCol -\u003e \"sepalWidth\")))\nprintln(regEval.evaluate(sepalPredictions,\n                         ParamMap(regEval.metricName -\u003e \"rmse\", regEval.labelCol -\u003e \"sepalWidth\")))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Regression with decision trees\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\n \nval dtr \u003d new DecisionTreeRegressor().setLabelCol(\"petalWidth\")\nprintln(dtr.explainParams)\n\n// COMMAND ----------\n\nval dtrModel \u003d dtr.fit(irisPetal)\nval dtrPredictions \u003d dtrModel.transform(irisPetal)\nprintln(regEval.evaluate(dtrPredictions, ParamMap(regEval.metricName -\u003e \"r2\")))\nprintln(regEval.evaluate(dtrPredictions, ParamMap(regEval.metricName -\u003e \"rmse\")))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s also build a gradient boosted tree.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.regression.GBTRegressor\nval gbt \u003d new GBTRegressor().setLabelCol(\"petalWidth\")\nprintln(gbt.explainParams)\n\n// COMMAND ----------\n\nval gbtModel \u003d gbt.fit(irisPetal)\nval gbtPredictions \u003d gbtModel.transform(irisPetal)\nprintln(regEval.evaluate(gbtPredictions, ParamMap(regEval.metricName -\u003e \"r2\")))\nprintln(regEval.evaluate(gbtPredictions, ParamMap(regEval.metricName -\u003e \"rmse\")))\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We should really test our gradient boosted tree out-of-sample as it is easy to overfit with a GBT model.\n",
      "dateUpdated": "Oct 27, 2015 12:48:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445943007836_-1967706229",
      "id": "20151027-105007_172965288",
      "dateCreated": "Oct 27, 2015 10:50:07 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "MLlib/03:  DecisionTree Classifier",
  "id": "2B4N3UGW5",
  "angularObjects": {
    "2AR33ZMZJ": [],
    "2AS9P7JSA": [],
    "2ARR8UZDJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}