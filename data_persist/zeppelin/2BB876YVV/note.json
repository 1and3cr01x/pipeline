{
  "paragraphs": [
    {
      "text": "import org.apache.spark.mllib.clustering.{LDA, OnlineLDAOptimizer}\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.sql.Row\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.feature.CountVectorizerModel\n\nimport sqlContext.implicits._",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706035_-1694361595",
      "id": "20160115-142146_788832848",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.clustering.{LDA, OnlineLDAOptimizer}\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.sql.Row\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.feature.CountVectorizerModel\nimport sqlContext.implicits._\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/datasets/nlp/country-lyrics.json\")\n  .select($\"id\", $\"title\", $\"url\", $\"lyrics\")",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706035_-1694361595",
      "id": "20160115-142146_933665854",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, title: string, url: string, lyrics: string]\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.feature.RegexTokenizer\n\n// Split each document into words\nval tokenizer \u003d new RegexTokenizer()\n  .setInputCol(\"lyrics\")\n  .setOutputCol(\"words\")\n  .setGaps(false)\n  .setPattern(\"\\\\p{L}+\")",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706035_-1694361595",
      "id": "20160115-142146_913946491",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.RegexTokenizer\ntokenizer: org.apache.spark.ml.feature.RegexTokenizer \u003d regexTok_76131fdad30c\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.feature.StopWordsRemover\n\n// Filter out stopwords\n// The following list will be used by default if we don\u0027t specify a list:  \n//   http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\nval stopWordsFilter \u003d new StopWordsRemover()\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"filteredWords\")\n  .setCaseSensitive(false)\n  \nval stopWords \u003d stopWordsFilter.getStopWords\nval newStopWords \u003d Array(\"did\", \"s\", \"t\", \"m\", \"n\", \"uh\", \"ll\", \"ha\", \"makes\", \"make\", \"yeah\", \"goes\", \"gettin\", \"v\", \"went\", \"aint\", \"let\", \"d\", \"yer\", \"don\", \"got\", \"just\", \"ain\", \"ve\", \"come\", \"gonna\", \"said\", \"says\", \"oh\") ++ stopWords \n\nstopWordsFilter.setStopWords(newStopWords)",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706035_-1694361595",
      "id": "20160115-142146_1874651183",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.StopWordsRemover\nstopWordsFilter: org.apache.spark.ml.feature.StopWordsRemover \u003d stopWords_4dc483b9b109\nstopWords: Array[String] \u003d Array(a, about, above, across, after, afterwards, again, against, all, almost, alone, along, already, also, although, always, am, among, amongst, amoungst, amount, an, and, another, any, anyhow, anyone, anything, anyway, anywhere, are, around, as, at, back, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, below, beside, besides, between, beyond, bill, both, bottom, but, by, call, can, cannot, cant, co, con, could, couldnt, cry, de, describe, detail, do, done, down, due, during, each, eg, eight, either, eleven, else, elsewhere, empty, enough, etc, even, ever, every, everyone, everything, everywhere, except, few, fifteen, fify, fill, find, fire, first, five, for, former, formerly, forty, found, four, from, front, full, fur...newStopWords: Array[String] \u003d Array(did, s, t, m, n, uh, ll, ha, makes, make, yeah, goes, gettin, v, went, aint, let, d, yer, don, got, just, ain, ve, come, gonna, said, says, oh, a, about, above, across, after, afterwards, again, against, all, almost, alone, along, already, also, although, always, am, among, amongst, amoungst, amount, an, and, another, any, anyhow, anyone, anything, anyway, anywhere, are, around, as, at, back, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, below, beside, besides, between, beyond, bill, both, bottom, but, by, call, can, cannot, cant, co, con, could, couldnt, cry, de, describe, detail, do, done, down, due, during, each, eg, eight, either, eleven, else, elsewhere, empty, enough, etc, even, ever, every, everyone, e...res220: stopWordsFilter.type \u003d stopWords_4dc483b9b109\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.feature.CountVectorizer\n\n// Limit to top `vocabSize` most common words and convert to word count vector features\nval vocabSize: Int \u003d 500\n\nval countVectorizer \u003d new CountVectorizer()\n  .setInputCol(stopWordsFilter.getOutputCol)\n  .setOutputCol(\"countFeatures\")\n  .setVocabSize(vocabSize)",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_1705034154",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.CountVectorizer\nvocabSize: Int \u003d 500\ncountVectorizer: org.apache.spark.ml.feature.CountVectorizer \u003d cntVec_63aa7cacf152\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val pipeline \u003d new Pipeline()\n  .setStages(Array(tokenizer, stopWordsFilter, countVectorizer))",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_1717892684",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "pipeline: org.apache.spark.ml.Pipeline \u003d pipeline_7e8b08cbc7ac\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val model \u003d pipeline.fit(itemsDF)",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_501045140",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "model: org.apache.spark.ml.PipelineModel \u003d pipeline_7e8b08cbc7ac\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val countVectors \u003d model.transform(itemsDF)\n  .select(\"id\", countVectorizer.getOutputCol)\n  .map { case Row(id: Long, countVector: Vector) \u003d\u003e (id, countVector) }\n  .cache()",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_354957712",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "countVectors: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.linalg.Vector)] \u003d MapPartitionsRDD[2086] at map at \u003cconsole\u003e:123\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Run LDA\nval maxIterations: Int \u003d 100\n\n// Note:  We\u0027re adding (1.0 / actualCorpusSize) to MiniBatchFraction \n//        to be more robust on tiny datasets.\nval miniBatchFraction \u003d math.min(1.0, 2.0 / maxIterations + 1.0 / countVectors.count())\n\nval numTopics: Int \u003d 10\n\nval lda \u003d new LDA()\n  .setOptimizer(new OnlineLDAOptimizer().setMiniBatchFraction(miniBatchFraction))\n  .setK(numTopics)\n  .setMaxIterations(maxIterations)\n\nval ldaModel \u003d lda.run(countVectors)",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_1986397966",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "maxIterations: Int \u003d 100\nminiBatchFraction: Double \u003d 0.03\nnumTopics: Int \u003d 10\nlda: org.apache.spark.mllib.clustering.LDA \u003d org.apache.spark.mllib.clustering.LDA@77b7d112\nldaModel: org.apache.spark.mllib.clustering.LDAModel \u003d org.apache.spark.mllib.clustering.LocalLDAModel@5cbefb6e\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Print topics and top terms per topic.\nval vocabArray \u003d model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary\n\nval topicIndices \u003d ldaModel.describeTopics(maxTermsPerTopic \u003d 10)\n\nval topics \u003d topicIndices.map { case (terms, termWeights) \u003d\u003e\n  terms.map(vocabArray(_)).zip(termWeights)\n}",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_938563864",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "vocabArray: Array[String] \u003d Array(like, love, know, chorus, baby, life, gone, way, good, time, long, right, cause, little, say, day, feel, night, eyes, old, need, heart, man, home, left, tell, hope, think, wanna, leave, believe, god, away, fall, girl, world, rain, big, dance, remember, thing, tonight, vroom, smile, won, road, going, came, morning, better, hey, live, hear, chance, head, want, miss, things, hard, hell, town, lost, someday, look, didn, face, xl, double, coming, train, years, people, boondocks, pretty, ride, dreams, stay, sure, mary, blue, round, bad, touch, nothin, hands, proud, change, new, hair, probably, girls, strong, hold, friend, pain, light, wouldn, sky, gotta, white, song, true, giving, wind, ooh, jesus, door, best, really, mom, lord, days, making, loving, born, lo...topicIndices: Array[(Array[Int], Array[Double])] \u003d Array((Array(42, 74, 270, 256, 286, 133, 201, 347, 8, 406),Array(0.13070937154581783, 0.06542637340974748, 0.05831192536085249, 0.039072684103205996, 0.03783465228567194, 0.036272090542846705, 0.026571551999761144, 0.02430993643297398, 0.022832845571115305, 0.019505374673930386)), (Array(88, 491, 109, 324, 410, 293, 335, 374, 219, 232),Array(0.053128625526011296, 0.04038126409473224, 0.028354213215777253, 0.028116255246598363, 0.026218905385537526, 0.023749596692639294, 0.022617639956304145, 0.020617272537129964, 0.019924435088609044, 0.019695890245759056)), (Array(66, 67, 181, 4, 104, 136, 272, 313, 1, 92),Array(0.06077647291358887, 0.060680446380727435, 0.04526710467836925, 0.03990919614719823, 0.035073359832807766, 0.0320918625166213...topics: Array[Array[(String, Double)]] \u003d Array(Array((vroom,0.13070937154581783), (ride,0.06542637340974748), (cowboy,0.05831192536085249), (hat,0.039072684103205996), (nascar,0.03783465228567194), (sang,0.036272090542846705), (songs,0.026571551999761144), (pull,0.02430993643297398), (good,0.022832845571115305), (men,0.019505374673930386)), Array((hair,0.053128625526011296), (mr,0.04038126409473224), (mom,0.028354213215777253), (read,0.028116255246598363), (closes,0.026218905385537526), (scared,0.023749596692639294), (soldier,0.022617639956304145), (bow,0.020617272537129964), (told,0.019924435088609044), (beth,0.019695890245759056)), Array((xl,0.06077647291358887), (double,0.060680446380727435), (pickin,0.04526710467836925), (baby,0.03990919614719823), (ooh,0.035073359832807766), (ring,..."
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "println(s\"\\n\\n$numTopics topics:\\n\\n\")\ntopics.zipWithIndex.foreach { case (topic, i) \u003d\u003e\n  println(s\"TOPIC $i\")\n  topic.foreach { case (term, weight) \u003d\u003e println(s\"$term\\t$weight\") }\n  println(s\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n}\n",
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_1080547717",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\n\n10 topics:\n\n\nTOPIC 0\nvroom\t0.13070937154581783\nride\t0.06542637340974748\ncowboy\t0.05831192536085249\nhat\t0.039072684103205996\nnascar\t0.03783465228567194\nsang\t0.036272090542846705\nsongs\t0.026571551999761144\npull\t0.02430993643297398\ngood\t0.022832845571115305\nmen\t0.019505374673930386\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 1\nhair\t0.053128625526011296\nmr\t0.04038126409473224\nmom\t0.028354213215777253\nread\t0.028116255246598363\ncloses\t0.026218905385537526\nscared\t0.023749596692639294\nsoldier\t0.022617639956304145\nbow\t0.020617272537129964\ntold\t0.019924435088609044\nbeth\t0.019695890245759056\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 2\nxl\t0.06077647291358887\ndouble\t0.060680446380727435\npickin\t0.04526710467836925\nbaby\t0.03990919614719823\nooh\t0.035073359832807766\nring\t0.03209186251662135\nwildflowers\t0.021774810688948\nbrrr\t0.02064050107036206\nlove\t0.017039157592155924\nhold\t0.01679456025899226\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 3\nclothes\t0.09355633697751467\nfall\t0.09349055548844819\ntequila\t0.09214613249371247\nyea\t0.03990325515023931\nshoes\t0.027251613408650652\nstart\t0.026551265995833802\ndrink\t0.02654400506636401\nlose\t0.026192712904232626\nleave\t0.02488380936821524\nchild\t0.024297353479400183\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 4\nlove\t0.02610679656126622\nknow\t0.025986878087979497\nchorus\t0.02471602150278065\nlike\t0.023538177940648136\ngood\t0.022779170908140692\nbaby\t0.0196272502217844\nlife\t0.018314092396633373\ncause\t0.01253015747475087\ntell\t0.012282732299556235\nway\t0.012261284220338634\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 5\nlike\t0.07993105339871694\ngone\t0.07335086518454362\nwanna\t0.02613177577153475\nbig\t0.02146686503421839\nremember\t0.018131736509128183\nlittle\t0.01607080066453644\nwake\t0.014058804112835655\nhell\t0.013996180595393874\nright\t0.013122783267311556\nmaking\t0.012367302128911326\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 6\nmary\t0.05052575218057095\nheart\t0.04289968311189699\nmonday\t0.04128012958432608\nheaven\t0.03631673157754517\nchurch\t0.03436347075153784\nmorning\t0.03229491089771154\ndrawer\t0.030811662088339707\nleft\t0.022544019232464053\nhooked\t0.0215869209116271\nmerry\t0.021420788489013746\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 7\nboondocks\t0.06428696641446863\nproud\t0.03993596112275287\nfeel\t0.03977201515883925\nsoul\t0.03311207284470323\nlearned\t0.02992501458318297\nraised\t0.02726553126864565\nthing\t0.024342693708041076\nmatter\t0.024258695461589822\nshame\t0.024178871425925123\ntrain\t0.023725707510639314\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 8\nglad\t0.0686586772177256\ngirls\t0.05034611753903244\nspend\t0.048804150342295925\ncalifornia\t0.04750794318814721\nafraid\t0.023914805182207028\nskin\t0.021149734009574487\ndirty\t0.021038116189905066\neat\t0.02028945887046246\nrock\t0.020070116047736578\ntouch\t0.01525408933734602\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTOPIC 9\nmoment\t0.050988609657812003\nlord\t0.050912744643322516\ngeorgia\t0.044220267868381444\ndirt\t0.030572510764595866\nnothin\t0.029981854127106244\nrain\t0.028709974845201037\nwash\t0.024925007678919663\nol\t0.022662832461627574\naway\t0.021978217446233867\nverse\t0.01782562627865046\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 15, 2016 2:21:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452867706036_-1696285339",
      "id": "20160115-142146_349670560",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 15, 2016 2:21:46 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "NLP/05: Word2Vec",
  "id": "2BB876YVV",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {},
  "info": {}
}