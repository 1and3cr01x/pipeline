{
  "paragraphs": [
    {
      "title": "Retrieve dataset",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")\n  .select($\"id\", $\"title\", $\"category\", $\"description\")",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999346_-234095678",
      "id": "20160117-021639_522798722",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, title: string, category: string, description: string]\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:12 PM",
      "dateFinished": "Jan 19, 2016 9:20:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Distinct Categories",
      "text": "val distinctCategoriesDF \u003d itemsDF.select($\"title\", $\"category\").distinct()\nz.show(distinctCategoriesDF)",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": true,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999347_-234480427",
      "id": "20160117-021639_2060882215",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "title\tcategory\nSpark ML/MLlib\tLibrary\nPostgres\tDatabase\nMemcached\tDistributed Cache\nApache Pig\tData Processing Execution Engine\nKinesis\tLibrary\nSpark Streaming\tLibrary\nApache Parquet\tFile Format\nJSON\tFile Format\nTitan GraphDB\tDatabase\nNLTK\tLibrary\nKnime\tWorkflow\nCSV\tFile Format\nApache Solr\tSearch Engine\nElastic MapReduce\tData Processing Execution Engine\nApache Impala\tData Processing Execution Engine\nApache Hive\tData Processing Execution Engine\nPresto\tData Processing Execution Engine\nApache ZooKeeper\tDistributed Coordinator\nPython\tProgramming Language\nApache Kafka\tMessage Broker\nApache HUE\tUI\nTeradata\tDatabase\nVertica\tDatabase\nApache YARN\tCluster Resource Manager\nApache Nifi\tWorkflow\nRedshift\tDatabase\nMapR\tDistribution\nXML\tFile Format\nDynamoDB\tDatabase\nHortonworks\tDistribution\niPython/Jupyter\tNotebook\nRedis\tDistributed Cache\nScala\tProgramming Language\nApache Ambari\tCluster Provision\nDato GraphLab Create\tLibrary\nAmazon Web Services\tCloud Provider\nApache Flume\tLibrary\nApache Mahout\tLibrary\nStanford CoreNLP\tLibrary\nTachyon\tDistributed Cache\nMicroStrategy\tBI\nApache Cassandra\tDatabase\nS3\tFile System\nNeo4j\tLibrary\nApache Drill\tData Processing Execution Engine\nProtobuffers\tFile Format\nApache Mesos\tCluster Resource Manager\nSQL Server\tDatabase\nApache Flink\tData Processing Execution Engine\nApache Spark\tData Processing Execution Engine\nApache Storm\tStreaming\nSpark SQL\tLibrary\nMicrosft Azure\tCloud Provider\nMySQL\tDatabase\nIBM BigInsights\tDistribution\nMongoDB\tDatabase\nApache Tez\tData Processing Execution Engine\nSci-Kit Learn\tLibrary\nApache HDFS\tFile System\nR\tProgramming Language\nApache Sqoop\tData Import\nApache Lucene\tLibrary\nDeep Learning 4J\tLibrary\nApache Zeppelin\tNotebook\nTableau\tBI\nCloudera\tDistribution\nApache Oozie\tWorkflow\nGoogle Cloud Platform\tCloud Provider\nOn-Premise\tCloud Provider\nDocker\tCategory\nElasticSearch\tSearch Engine\nTensor Flow\tData Processing Execution Engine\nJava\tProgramming Language\nSpark GraphX\tLibrary\nApache Giraph\tLibrary\nApache HBase\tDatabase\nApache ORC\tFile Format\nApache MapReduce\tData Processing Execution Engine\nOracle\tDatabase\nSQL\tProgramming Language\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:12 PM",
      "dateFinished": "Jan 19, 2016 9:20:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Tokenize",
      "text": "import org.apache.spark.ml.feature.RegexTokenizer\n\n// Split each document into words\nval tokenizer \u003d new RegexTokenizer()\n  .setInputCol(\"description\")\n  .setOutputCol(\"words\")\n  .setGaps(false)\n  .setPattern(\"\\\\p{L}+\")",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999347_-234480427",
      "id": "20160117-021639_1778875532",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.RegexTokenizer\ntokenizer: org.apache.spark.ml.feature.RegexTokenizer \u003d regexTok_475d3b3ec71b\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:13 PM",
      "dateFinished": "Jan 19, 2016 9:20:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Remove Common Stop Words",
      "text": "import org.apache.spark.ml.feature.StopWordsRemover\n\n// The following list will be used by default if we don\u0027t specify a list:  \n//   http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\nval stopWordsFilter \u003d new StopWordsRemover()\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"filteredWords\")\n  .setCaseSensitive(false)",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999347_-234480427",
      "id": "20160117-021639_1342104069",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.StopWordsRemover\nstopWordsFilter: org.apache.spark.ml.feature.StopWordsRemover \u003d stopWords_637a2e64e957\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:14 PM",
      "dateFinished": "Jan 19, 2016 9:20:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "N-Gram Featurizer",
      "text": "import org.apache.spark.ml.feature.NGram\n\nval ngram \u003d new NGram()\n  .setInputCol(stopWordsFilter.getOutputCol)\n  .setOutputCol(\"ngramFeatures\")\n  \nngram.fit(itemsDF)",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452997167396_-1847131109",
      "id": "20160117-021927_1560901546",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.NGram\nngram: org.apache.spark.ml.feature.NGram \u003d ngram_110b0f6931a9\n\u003cconsole\u003e:59: error: value fit is not a member of org.apache.spark.ml.feature.NGram\n              ngram.fit(itemsDF)\n                    ^\n"
      },
      "dateCreated": "Jan 17, 2016 2:19:27 AM",
      "dateStarted": "Jan 19, 2016 9:20:15 PM",
      "dateFinished": "Jan 19, 2016 9:20:15 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.feature.VectorAssembler\n\nval featureVectorAssembler \u003d new VectorAssembler()\n  .setInputCols(Array(ngram.getOutputCol))\n  .setOutputCol(\"allFeatures\")",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452997504006_481275645",
      "id": "20160117-022504_1104441934",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.VectorAssembler\nfeatureVectorAssembler: org.apache.spark.ml.feature.VectorAssembler \u003d vecAssembler_4de545221328\n"
      },
      "dateCreated": "Jan 17, 2016 2:25:04 AM",
      "dateStarted": "Jan 19, 2016 9:20:15 PM",
      "dateFinished": "Jan 19, 2016 9:20:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "convert the category labels into indexes",
      "text": "import org.apache.spark.ml.feature.StringIndexer\n\n// Assign an Index to Each Category \nval categoryIndexerModel \u003d new StringIndexer()\n  .setInputCol(\"category\")\n  .setOutputCol(\"indexedCategory\")\n  .fit(itemsDF)",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999348_-236404171",
      "id": "20160117-021639_1036227727",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.StringIndexer\ncategoryIndexerModel: org.apache.spark.ml.feature.StringIndexerModel \u003d strIdx_4e27c48017c0\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:15 PM",
      "dateFinished": "Jan 19, 2016 9:20:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "create decision tree classifer",
      "text": "import org.apache.spark.ml.classification.DecisionTreeClassifier\n\nval classifier \u003d new DecisionTreeClassifier()\n  .setFeaturesCol(ngram.getOutputCol)\n  .setLabelCol(categoryIndexerModel.getOutputCol)\n  .setPredictionCol(\"prediction\")\n  .setRawPredictionCol(\"confidence\")\n  .setProbabilityCol(\"probability\")",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999349_-236788920",
      "id": "20160117-021639_316372185",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.classification.DecisionTreeClassifier\nclassifier: org.apache.spark.ml.classification.DecisionTreeClassifier \u003d dtc_42e7a0879988\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:16 PM",
      "dateFinished": "Jan 19, 2016 9:20:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "convert the category index back to String",
      "text": "import org.apache.spark.ml.feature.IndexToString\n\nval categoryReverseIndexer \u003d new IndexToString()\n  .setInputCol(classifier.getPredictionCol)\n  .setOutputCol(\"predictedCategory\")\n  .setLabels(categoryIndexerModel.labels)",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999349_-236788920",
      "id": "20160117-021639_2084375441",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.IndexToString\ncategoryReverseIndexer: org.apache.spark.ml.feature.IndexToString \u003d idxToStr_15112a700c95\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:16 PM",
      "dateFinished": "Jan 19, 2016 9:20:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create the training pipeline",
      "text": "import org.apache.spark.ml.Pipeline\n\nval pipeline \u003d new Pipeline()\n  .setStages(Array(tokenizer, stopWordsFilter, ngram, featureVectorAssembler, categoryIndexerModel, classifier, categoryReverseIndexer))",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999349_-236788920",
      "id": "20160117-021639_1409791556",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.Pipeline\npipeline: org.apache.spark.ml.Pipeline \u003d pipeline_0cf564e229ad\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:16 PM",
      "dateFinished": "Jan 19, 2016 9:20:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create evaluator for multiclass decision tree model",
      "text": "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\nval metricName \u003d \"f1\"\n\nval modelEvaluator \u003d new MulticlassClassificationEvaluator()\n  .setLabelCol(classifier.getLabelCol)\n  .setPredictionCol(classifier.getPredictionCol)\n  .setMetricName(metricName)",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999349_-236788920",
      "id": "20160117-021639_1975592871",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nmetricName: String \u003d f1\nmodelEvaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \u003d mcEval_7a6b1cb08ed0\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:17 PM",
      "dateFinished": "Jan 19, 2016 9:20:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Build param grid search",
      "text": "import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval paramGrid \u003d new ParamGridBuilder()\n  .addGrid(ngram.n, Array(2, 3))\n  .addGrid(classifier.maxDepth, Array(3, 5))\n  .build()\n  \nparamGrid.size",
      "dateUpdated": "Jan 19, 2016 9:19:57 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999350_-235634673",
      "id": "20160117-021639_1022748794",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.tuning.ParamGridBuilder\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] \u003d \nArray({\n\tdtc_42e7a0879988-maxDepth: 3,\n\tngram_110b0f6931a9-n: 2\n}, {\n\tdtc_42e7a0879988-maxDepth: 3,\n\tngram_110b0f6931a9-n: 3\n}, {\n\tdtc_42e7a0879988-maxDepth: 5,\n\tngram_110b0f6931a9-n: 2\n}, {\n\tdtc_42e7a0879988-maxDepth: 5,\n\tngram_110b0f6931a9-n: 3\n})\nres71: Int \u003d 4\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:17 PM",
      "dateFinished": "Jan 19, 2016 9:20:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create cross validator",
      "text": "import org.apache.spark.ml.tuning.CrossValidator\n\n// K-Folds Cross Validation Combined With Param Grid \nval numFolds \u003d 3\n\nval modelValidator \u003d new CrossValidator()\n  .setEstimator(pipeline)\n  .setEvaluator(modelEvaluator)\n  .setEstimatorParamMaps(paramGrid)\n  .setNumFolds(numFolds) ",
      "dateUpdated": "Jan 19, 2016 9:19:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999350_-235634673",
      "id": "20160117-021639_679480822",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.tuning.CrossValidator\nnumFolds: Int \u003d 3\nmodelValidator: org.apache.spark.ml.tuning.CrossValidator \u003d cv_757dd43c6269\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:17 PM",
      "dateFinished": "Jan 19, 2016 9:20:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train crossValidator model",
      "text": "val crossValidatorModel \u003d modelValidator.fit(itemsDF)",
      "dateUpdated": "Jan 19, 2016 9:19:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999350_-235634673",
      "id": "20160117-021639_1104948020",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.lang.IllegalArgumentException: Data type ArrayType(StringType,false) is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:116)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:112)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:112)\n\tat org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:173)\n\tat org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:173)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108)\n\tat org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:173)\n\tat org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:129)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:68)\n\tat org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:91)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:85)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:90)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:92)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:94)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:96)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:98)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:100)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:102)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:104)\n\tat \u003cinit\u003e(\u003cconsole\u003e:106)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:110)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:18 PM",
      "dateFinished": "Jan 19, 2016 9:20:18 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Describe effectiveness of all param grid options",
      "text": "// Print the average metrics\nval avgMetricsParamGrid \u003d crossValidatorModel.avgMetrics\n\n// Combine with paramGrid to see how they affect the overall metrics\nval combined \u003d paramGrid.zip(avgMetricsParamGrid)",
      "dateUpdated": "Jan 19, 2016 9:19:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999350_-235634673",
      "id": "20160117-021639_1029492919",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:58: error: not found: value crossValidatorModel\n       val avgMetricsParamGrid \u003d crossValidatorModel.avgMetrics\n                                 ^\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:18 PM",
      "dateFinished": "Jan 19, 2016 9:20:18 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Describe Chosen HyperParameters",
      "text": "import org.apache.spark.ml.feature.Word2VecModel\nimport org.apache.spark.ml.feature.IDFModel\nimport org.apache.spark.ml.feature.StringIndexerModel\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.PipelineModel\n\nval bestModel \u003d crossValidatorModel.bestModel.asInstanceOf[PipelineModel]\n\n// Explain params for each stage\nval bestHashingTFNumFeatures \u003d bestModel.stages(2).asInstanceOf[HashingTF].explainParams\nval bestIDFMinDocFrequency \u003d bestModel.stages(3).asInstanceOf[IDFModel].explainParams\nval bestDecisionTreeDepth \u003d bestModel.stages(6).asInstanceOf[DecisionTreeClassificationModel].explainParams",
      "dateUpdated": "Jan 19, 2016 9:19:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999351_-236019422",
      "id": "20160117-021639_779700574",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.Word2VecModel\nimport org.apache.spark.ml.feature.IDFModel\nimport org.apache.spark.ml.feature.StringIndexerModel\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.PipelineModel\n\u003cconsole\u003e:63: error: not found: value crossValidatorModel\n       val bestModel \u003d crossValidatorModel.bestModel.asInstanceOf[PipelineModel]\n                       ^\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:18 PM",
      "dateFinished": "Jan 19, 2016 9:20:18 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Predict on new data",
      "text": "val predictOnDF \u003d sqlContext.createDataFrame(Seq(\n      (1, \"nosql\")\n    )).toDF(\"id\", \"description\")\n\nval predictedResultsDF \u003d bestModel.transform(predictOnDF)\n .select(classifier.getPredictionCol, categoryReverseIndexer.getOutputCol, stopWordsFilter.getOutputCol, classifier.getRawPredictionCol, classifier.getProbabilityCol)\n\nz.show(predictedResultsDF)",
      "dateUpdated": "Jan 19, 2016 9:19:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "prediction",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "predictedCategory",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "prediction",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999351_-236019422",
      "id": "20160117-021639_113123034",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "predictOnDF: org.apache.spark.sql.DataFrame \u003d [id: int, description: string]\n\u003cconsole\u003e:79: error: not found: value bestModel\n       val predictedResultsDF \u003d bestModel.transform(predictOnDF)\n                                ^\n"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:19 PM",
      "dateFinished": "Jan 19, 2016 9:20:19 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Jan 19, 2016 9:19:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452996999351_-236019422",
      "id": "20160117-021639_639307954",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 17, 2016 2:16:39 AM",
      "dateStarted": "Jan 19, 2016 9:20:19 PM",
      "dateFinished": "Jan 19, 2016 9:20:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "NLP/06: Text Classifier Pipeline (N-Gram + Decision Tree)",
  "id": "2BC4DHMA8",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}