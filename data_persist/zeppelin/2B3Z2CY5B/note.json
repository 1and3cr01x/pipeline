{
  "paragraphs": [
    {
      "text": "// Databricks notebook source exported at Tue, 27 Oct 2015 09:45:49 UTC\n// MAGIC %md\n// MAGIC # Wikipedia: TF-IDF with Normalization for K-Means\n// MAGIC  \n// MAGIC In this lab, we explore generating a k-means model to cluster Wikipedia articles.  This clustering could be used as part of an exploratory data analysis (EDA) process or as a way to build features for a supervised learning technique.\n// MAGIC  \n// MAGIC We\u0027ll create a `Pipeline` that can be used to make the cluster predictions.  This lab will make use of `RegexTokenizer`, `HashingTF`, `IDF`, `Normalizer`, `Pipeline`, and `KMeans`.  You\u0027ll also see how to perform a stratified random sample.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Load in the data.\n\n// COMMAND ----------\n\nval dfSmall \u003d sqlContext.read.load(\"/mnt/ml-amsterdam/smallwiki.parquet\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Filter out non-relevant data.\n\n// COMMAND ----------\n\nval parsed \u003d dfSmall.filter(($\"title\" !\u003d\u003d \"\u003cPARSE ERROR\u003e\") \u0026\u0026\n                              $\"redirect_title\".isNull \u0026\u0026\n                              $\"text\".isNotNull)\nparsed.take(1)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Use a regular expression to tokenize (split into words).  Pattern defaults to matching the separator, but can be set to match tokens instead.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.RegexTokenizer\n \nval tokenizer \u003d new RegexTokenizer()\n  .setInputCol(\"text\")\n  .setOutputCol(\"words\")\n  .setPattern(\"\\\\W+\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Create a `HashingTF` transformer to hash words to buckets with counts, then use an `IDF` estimator to compute inverse-document frequency for buckets based on how frequently words have hashed to those buckets in the given documents.  Next, normalize the tf-idf values so that the \\\\( l^2 \\\\) norm is one for each row.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.{IDF, HashingTF, Normalizer}\n \nval hashingTF \u003d new HashingTF()\n  .setNumFeatures(10000)\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"hashingTF\")\n \nval idf \u003d new IDF()\n  .setMinDocFreq(10)\n  .setInputCol(hashingTF.getOutputCol)\n  .setOutputCol(\"idf\")\n \nval normalizer \u003d new Normalizer()\n  .setInputCol(idf.getOutputCol)\n  .setOutputCol(\"features\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Now, let\u0027s build the `KMeans` estimator and a `Pipeline` that will contain all of the stages.  We\u0027ll then call fit on the `Pipeline` which will give us back a `PipelineModel`.  This will take about a minute to run.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.clustering.KMeans\n \nval kmeans \u003d new KMeans()\n  .setFeaturesCol(\"features\")\n  .setPredictionCol(\"prediction\")\n  .setK(5)\n  .setSeed(0)\n \nval pipeline \u003d new Pipeline()\n  .setStages(Array(tokenizer, hashingTF, idf, normalizer, kmeans))\n \nval model \u003d pipeline.fit(parsed)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s take a look at a sample of the data to see if we can see a pattern between predicted clusters and titles.  We\u0027ll use a stratified sample to over-weight the less frequent predictions for inspection purposes.\n\n// COMMAND ----------\n\nval predictions \u003d model.transform(parsed)\nval stratifiedMap \u003d Map(0 -\u003e .03, 1 -\u003e .04, 2 -\u003e .06, 3 -\u003e .4, 4 -\u003e .005)\nval sampleDF \u003d predictions.stat.sampleBy(\"prediction\", stratifiedMap, 0)\ndisplay(sampleDF.select(\"title\", \"prediction\").orderBy(\"prediction\"))\n\n// COMMAND ----------\n\npredictions.columns\n\n// COMMAND ----------\n\ndisplay(predictions.select(\"features\"))\n",
      "dateUpdated": "Oct 27, 2015 11:00:37 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445943585695_-1062385489",
      "id": "20151027-105945_873890120",
      "dateCreated": "Oct 27, 2015 10:59:45 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Wiki NLP/03: TFIDF",
  "id": "2B3Z2CY5B",
  "angularObjects": {
    "2AR33ZMZJ": [],
    "2AS9P7JSA": [],
    "2ARR8UZDJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}